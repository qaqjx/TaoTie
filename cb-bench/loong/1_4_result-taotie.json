{"id": "13a4a371-6339-4c9d-82cf-fc9ab2bb017d", "pred": ["\n\nIn this paper, we present a new approach for building large language models using a mixture of experts. Our approach is based on the following principles: (1) Expert merging, (2) Efficient training, and (3) Scalability. We first describe the details of our approach, then evaluate its performance on various NLP tasks.\n\n### 1.1 Expert Merging\n\nOur approach merges experts by taking a weighted average of their parameters."], "TTFT": 11.706799030303955, "answer": "{\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"], \"Citation\": [\"Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training\"]}", "f1 socre": 0.11111111111111112}
{"id": "13a4a371-6339-4c9d-82cf-fc9ab2bb017d", "pred": ["\n\nIn this paper, we present a new approach for building large language models using a mixture of experts. Our approach is based on the following principles: (1) Expert merging, (2) Efficient training, and (3) Scalability. We first describe the details of our approach, then evaluate its performance on various NLP tasks.\n\n### 1.1 Expert Merging\n\nOur approach merges experts by taking a weighted average of their parameters."], "TTFT": 11.706799030303955, "answer": "{\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"], \"Citation\": [\"Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training\"]}", "f1 socre": 0.11111111111111112}
{"id": "c4a4e642-fa00-4377-a756-40f2f7ea7a74", "pred": ["\n\nThe paper proposes a Chinese Benchmark for Role-Playing Conversational Agent Evaluation, which aims to evaluate the performance of a Chinese LLM in a conversational setting. The evaluation is based on the following metrics:\n\n- Fluency: Measures the grammatical correctness of the generated text.\n- Coherency: Evaluates the relevance between the generated text and the context.\n- Consistency: Evaluates the stability"], "TTFT": 9.802894353866577, "answer": "{\"Reference\": [\"A Survey of Hallucination in \\\"Large\\\" Foundation Models\"], \"Citation\": [\"From Role-Play to Drama-Interaction: An LLM Solution\"]}", "f1 socre": 0.16470588235294115}
