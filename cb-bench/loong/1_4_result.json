{"id": "13a4a371-6339-4c9d-82cf-fc9ab2bb017d", "pred": ["{\n\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"],\n\"Citation\": []\n}\n\nThe paper \"OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models\" does not mention \"TinyLlama: An Open-Source Small Language Model\" in its text, so there is no citation relationship between the two papers. However, \"OpenMoE\" references \"Tiny"], "TTFT": 13.604034900665283, "answer": "{\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"], \"Citation\": [\"Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training\"]}", "f1 socre": 0.4047619047619047}
{"id": "13a4a371-6339-4c9d-82cf-fc9ab2bb017d", "pred": ["{\n\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"],\n\"Citation\": []\n}\n\nThe paper \"OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models\" does not mention \"TinyLlama: An Open-Source Small Language Model\" in its text, so there is no citation relationship between the two papers. However, \"OpenMoE\" references \"Tiny"], "TTFT": 13.604034900665283, "answer": "{\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"], \"Citation\": [\"Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training\"]}", "f1 socre": 0.4047619047619047}
{"id": "c4a4e642-fa00-4377-a756-40f2f7ea7a74", "pred": ["{{\"Reference\": [\"A Survey of Hallucination in Large Foundation Models by Vipula Rawte, Amit Sheth, and Amitava Das\"], \"Citation\": [\"CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation by Quan Tu, Shilong Fan, Zihang Tian, Rui Yan\"]}}\n\nThis paper on CharacterEval references the survey paper on hallucination in large foundation models and"], "TTFT": 11.539960622787476, "answer": "{\"Reference\": [\"A Survey of Hallucination in \\\"Large\\\" Foundation Models\"], \"Citation\": [\"From Role-Play to Drama-Interaction: An LLM Solution\"]}", "f1 socre": 0.2745098039215686}
{"id": "13a4a371-6339-4c9d-82cf-fc9ab2bb017d", "pred": ["{\n\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"],\n\"Citation\": []\n}\n\nThe paper \"OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models\" does not mention \"TinyLlama: An Open-Source Small Language Model\" in its text, so there is no citation relationship between the two papers. However, \"OpenMoE\" references \"Tiny"], "TTFT": 13.604034900665283, "answer": "{\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"], \"Citation\": [\"Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training\"]}", "f1 socre": 0.4047619047619047}
{"id": "c4a4e642-fa00-4377-a756-40f2f7ea7a74", "pred": ["{{\"Reference\": [\"A Survey of Hallucination in Large Foundation Models by Vipula Rawte, Amit Sheth, and Amitava Das\"], \"Citation\": [\"CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation by Quan Tu, Shilong Fan, Zihang Tian, Rui Yan\"]}}\n\nThis paper on CharacterEval references the survey paper on hallucination in large foundation models and"], "TTFT": 11.539960622787476, "answer": "{\"Reference\": [\"A Survey of Hallucination in \\\"Large\\\" Foundation Models\"], \"Citation\": [\"From Role-Play to Drama-Interaction: An LLM Solution\"]}", "f1 socre": 0.2745098039215686}
{"id": "835a4005-8c32-4083-b52d-4e014d1d35f5", "pred": ["Based on the provided instructions and papers, here is the longest citation chain:\n\n[\"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\" by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed, \"SUPERB: Speech processing Universal"], "TTFT": 11.292267322540283, "answer": "[\"CONTRASTIVE SEMI-SUPERVISED LEARNING FOR ASR\", \"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\", \"SUPERB: Speech processing Universal PERformance Benchmark\"]", "f1 socre": 0.38461538461538464}
{"id": "13a4a371-6339-4c9d-82cf-fc9ab2bb017d", "pred": ["{\n\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"],\n\"Citation\": []\n}\n\nThe paper \"OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models\" does not mention \"TinyLlama: An Open-Source Small Language Model\" in its text, so there is no citation relationship between the two papers. However, \"OpenMoE\" references \"Tiny"], "TTFT": 13.604034900665283, "answer": "{\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"], \"Citation\": [\"Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training\"]}", "f1 socre": 0.4047619047619047}
{"id": "c4a4e642-fa00-4377-a756-40f2f7ea7a74", "pred": ["{{\"Reference\": [\"A Survey of Hallucination in Large Foundation Models by Vipula Rawte, Amit Sheth, and Amitava Das\"], \"Citation\": [\"CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation by Quan Tu, Shilong Fan, Zihang Tian, Rui Yan\"]}}\n\nThis paper on CharacterEval references the survey paper on hallucination in large foundation models and"], "TTFT": 11.539960622787476, "answer": "{\"Reference\": [\"A Survey of Hallucination in \\\"Large\\\" Foundation Models\"], \"Citation\": [\"From Role-Play to Drama-Interaction: An LLM Solution\"]}", "f1 socre": 0.2745098039215686}
{"id": "835a4005-8c32-4083-b52d-4e014d1d35f5", "pred": ["Based on the provided instructions and papers, here is the longest citation chain:\n\n[\"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\" by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed, \"SUPERB: Speech processing Universal"], "TTFT": 11.292267322540283, "answer": "[\"CONTRASTIVE SEMI-SUPERVISED LEARNING FOR ASR\", \"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\", \"SUPERB: Speech processing Universal PERformance Benchmark\"]", "f1 socre": 0.38461538461538464}
{"id": "cd618a43-7176-432d-b21e-432e5c2ca4a7", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\" by Abien Fred M. Agarap, 2018, ICMLC 2018]\n\n[\"A Novel Approach Combining Recurrent Neural Network and"], "TTFT": 6.193939208984375, "answer": "[\"Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems\", \"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\"]", "f1 socre": 0.34374999999999994}
{"id": "13a4a371-6339-4c9d-82cf-fc9ab2bb017d", "pred": ["{\n\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"],\n\"Citation\": []\n}\n\nThe paper \"OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models\" does not mention \"TinyLlama: An Open-Source Small Language Model\" in its text, so there is no citation relationship between the two papers. However, \"OpenMoE\" references \"Tiny"], "TTFT": 13.604034900665283, "answer": "{\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"], \"Citation\": [\"Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training\"]}", "f1 socre": 0.4047619047619047}
{"id": "c4a4e642-fa00-4377-a756-40f2f7ea7a74", "pred": ["{{\"Reference\": [\"A Survey of Hallucination in Large Foundation Models by Vipula Rawte, Amit Sheth, and Amitava Das\"], \"Citation\": [\"CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation by Quan Tu, Shilong Fan, Zihang Tian, Rui Yan\"]}}\n\nThis paper on CharacterEval references the survey paper on hallucination in large foundation models and"], "TTFT": 11.539960622787476, "answer": "{\"Reference\": [\"A Survey of Hallucination in \\\"Large\\\" Foundation Models\"], \"Citation\": [\"From Role-Play to Drama-Interaction: An LLM Solution\"]}", "f1 socre": 0.2745098039215686}
{"id": "835a4005-8c32-4083-b52d-4e014d1d35f5", "pred": ["Based on the provided instructions and papers, here is the longest citation chain:\n\n[\"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\" by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed, \"SUPERB: Speech processing Universal"], "TTFT": 11.292267322540283, "answer": "[\"CONTRASTIVE SEMI-SUPERVISED LEARNING FOR ASR\", \"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\", \"SUPERB: Speech processing Universal PERformance Benchmark\"]", "f1 socre": 0.38461538461538464}
{"id": "cd618a43-7176-432d-b21e-432e5c2ca4a7", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\" by Abien Fred M. Agarap, 2018, ICMLC 2018]\n\n[\"A Novel Approach Combining Recurrent Neural Network and"], "TTFT": 6.193939208984375, "answer": "[\"Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems\", \"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\"]", "f1 socre": 0.34374999999999994}
{"id": "e904afd3-5cb9-4bf7-bbee-9c8defac858c", "pred": ["Based on the provided instructions and papers, the longest citation chain can be constructed as follows:\n\n[\"On the performativity of SDG classifications in large bibliometric databases\" by Ottaviani and Stahlschmidt (2024), \"Language models are unsupervised multitask learners\" by Radford et al. (2019), \"DistilBERT: Distilling BERT: Six-Order Distillation for a Fast and Che"], "TTFT": 7.292623519897461, "answer": "[\"Contrastive Decoding: Open-ended Text Generation as Optimization\", \"An Empirical Study On Contrastive Search And Contrastive Decoding For Open-ended Text Generation\", \"On the performativity of SDG classifications in large bibliometric databases\"]", "f1 socre": 0.380952380952381}
{"id": "13a4a371-6339-4c9d-82cf-fc9ab2bb017d", "pred": ["{\n\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"],\n\"Citation\": []\n}\n\nThe paper \"OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models\" does not mention \"TinyLlama: An Open-Source Small Language Model\" in its text, so there is no citation relationship between the two papers. However, \"OpenMoE\" references \"Tiny"], "TTFT": 13.604034900665283, "answer": "{\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"], \"Citation\": [\"Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training\"]}", "f1 socre": 0.4047619047619047}
{"id": "c4a4e642-fa00-4377-a756-40f2f7ea7a74", "pred": ["{{\"Reference\": [\"A Survey of Hallucination in Large Foundation Models by Vipula Rawte, Amit Sheth, and Amitava Das\"], \"Citation\": [\"CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation by Quan Tu, Shilong Fan, Zihang Tian, Rui Yan\"]}}\n\nThis paper on CharacterEval references the survey paper on hallucination in large foundation models and"], "TTFT": 11.539960622787476, "answer": "{\"Reference\": [\"A Survey of Hallucination in \\\"Large\\\" Foundation Models\"], \"Citation\": [\"From Role-Play to Drama-Interaction: An LLM Solution\"]}", "f1 socre": 0.2745098039215686}
{"id": "835a4005-8c32-4083-b52d-4e014d1d35f5", "pred": ["Based on the provided instructions and papers, here is the longest citation chain:\n\n[\"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\" by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed, \"SUPERB: Speech processing Universal"], "TTFT": 11.292267322540283, "answer": "[\"CONTRASTIVE SEMI-SUPERVISED LEARNING FOR ASR\", \"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\", \"SUPERB: Speech processing Universal PERformance Benchmark\"]", "f1 socre": 0.38461538461538464}
{"id": "cd618a43-7176-432d-b21e-432e5c2ca4a7", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\" by Abien Fred M. Agarap, 2018, ICMLC 2018]\n\n[\"A Novel Approach Combining Recurrent Neural Network and"], "TTFT": 6.193939208984375, "answer": "[\"Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems\", \"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\"]", "f1 socre": 0.34374999999999994}
{"id": "e904afd3-5cb9-4bf7-bbee-9c8defac858c", "pred": ["Based on the provided instructions and papers, the longest citation chain can be constructed as follows:\n\n[\"On the performativity of SDG classifications in large bibliometric databases\" by Ottaviani and Stahlschmidt (2024), \"Language models are unsupervised multitask learners\" by Radford et al. (2019), \"DistilBERT: Distilling BERT: Six-Order Distillation for a Fast and Che"], "TTFT": 7.292623519897461, "answer": "[\"Contrastive Decoding: Open-ended Text Generation as Optimization\", \"An Empirical Study On Contrastive Search And Contrastive Decoding For Open-ended Text Generation\", \"On the performativity of SDG classifications in large bibliometric databases\"]", "f1 socre": 0.380952380952381}
{"id": "683f4cfd-1cfe-475d-927a-08a71d4f69e9", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\" by Niimi et al., 2024]\n\n[\"Churn Prediction with"], "TTFT": 11.674830198287964, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\", \"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\", \"An Efficient Multimodal Learning Framework to Comprehend Consumer Preferences Using BERT and Cross-Attention\"]", "f1 socre": 0.45312500000000006}
{"id": "13a4a371-6339-4c9d-82cf-fc9ab2bb017d", "pred": ["{\n\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"],\n\"Citation\": []\n}\n\nThe paper \"OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models\" does not mention \"TinyLlama: An Open-Source Small Language Model\" in its text, so there is no citation relationship between the two papers. However, \"OpenMoE\" references \"Tiny"], "TTFT": 13.604034900665283, "answer": "{\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"], \"Citation\": [\"Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training\"]}", "f1 socre": 0.4047619047619047}
{"id": "c4a4e642-fa00-4377-a756-40f2f7ea7a74", "pred": ["{{\"Reference\": [\"A Survey of Hallucination in Large Foundation Models by Vipula Rawte, Amit Sheth, and Amitava Das\"], \"Citation\": [\"CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation by Quan Tu, Shilong Fan, Zihang Tian, Rui Yan\"]}}\n\nThis paper on CharacterEval references the survey paper on hallucination in large foundation models and"], "TTFT": 11.539960622787476, "answer": "{\"Reference\": [\"A Survey of Hallucination in \\\"Large\\\" Foundation Models\"], \"Citation\": [\"From Role-Play to Drama-Interaction: An LLM Solution\"]}", "f1 socre": 0.2745098039215686}
{"id": "835a4005-8c32-4083-b52d-4e014d1d35f5", "pred": ["Based on the provided instructions and papers, here is the longest citation chain:\n\n[\"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\" by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed, \"SUPERB: Speech processing Universal"], "TTFT": 11.292267322540283, "answer": "[\"CONTRASTIVE SEMI-SUPERVISED LEARNING FOR ASR\", \"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\", \"SUPERB: Speech processing Universal PERformance Benchmark\"]", "f1 socre": 0.38461538461538464}
{"id": "cd618a43-7176-432d-b21e-432e5c2ca4a7", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\" by Abien Fred M. Agarap, 2018, ICMLC 2018]\n\n[\"A Novel Approach Combining Recurrent Neural Network and"], "TTFT": 6.193939208984375, "answer": "[\"Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems\", \"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\"]", "f1 socre": 0.34374999999999994}
{"id": "e904afd3-5cb9-4bf7-bbee-9c8defac858c", "pred": ["Based on the provided instructions and papers, the longest citation chain can be constructed as follows:\n\n[\"On the performativity of SDG classifications in large bibliometric databases\" by Ottaviani and Stahlschmidt (2024), \"Language models are unsupervised multitask learners\" by Radford et al. (2019), \"DistilBERT: Distilling BERT: Six-Order Distillation for a Fast and Che"], "TTFT": 7.292623519897461, "answer": "[\"Contrastive Decoding: Open-ended Text Generation as Optimization\", \"An Empirical Study On Contrastive Search And Contrastive Decoding For Open-ended Text Generation\", \"On the performativity of SDG classifications in large bibliometric databases\"]", "f1 socre": 0.380952380952381}
{"id": "683f4cfd-1cfe-475d-927a-08a71d4f69e9", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\" by Niimi et al., 2024]\n\n[\"Churn Prediction with"], "TTFT": 11.674830198287964, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\", \"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\", \"An Efficient Multimodal Learning Framework to Comprehend Consumer Preferences Using BERT and Cross-Attention\"]", "f1 socre": 0.45312500000000006}
{"id": "0223faf0-f46f-459d-b1cd-45c4dab78fc2", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"A tutorial on the range variant of asymmetric numeral systems\" by James Townsend, \"Asymmetric numeral systems\" by Jarek Duda, \"Asymmetric numeral systems: entropy coding combining speed of Huffman coding with compression rate of arithmetic coding\" by Jarek Duda, Khalid Tahboub, Neeraj J. Gad"], "TTFT": 14.098109483718872, "answer": "[\"Lightweight compression with encryption based on Asymmetric Numeral Systems\", \"A tutorial on the range variant of asymmetric numeral systems\", \"ATC: an Advanced Tucker Compression library for multidimensional data\"]", "f1 socre": 0.3652173913043478}
{"id": "13a4a371-6339-4c9d-82cf-fc9ab2bb017d", "pred": ["{\n\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"],\n\"Citation\": []\n}\n\nThe paper \"OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models\" does not mention \"TinyLlama: An Open-Source Small Language Model\" in its text, so there is no citation relationship between the two papers. However, \"OpenMoE\" references \"Tiny"], "TTFT": 13.604034900665283, "answer": "{\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"], \"Citation\": [\"Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training\"]}", "f1 socre": 0.4047619047619047}
{"id": "c4a4e642-fa00-4377-a756-40f2f7ea7a74", "pred": ["{{\"Reference\": [\"A Survey of Hallucination in Large Foundation Models by Vipula Rawte, Amit Sheth, and Amitava Das\"], \"Citation\": [\"CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation by Quan Tu, Shilong Fan, Zihang Tian, Rui Yan\"]}}\n\nThis paper on CharacterEval references the survey paper on hallucination in large foundation models and"], "TTFT": 11.539960622787476, "answer": "{\"Reference\": [\"A Survey of Hallucination in \\\"Large\\\" Foundation Models\"], \"Citation\": [\"From Role-Play to Drama-Interaction: An LLM Solution\"]}", "f1 socre": 0.2745098039215686}
{"id": "835a4005-8c32-4083-b52d-4e014d1d35f5", "pred": ["Based on the provided instructions and papers, here is the longest citation chain:\n\n[\"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\" by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed, \"SUPERB: Speech processing Universal"], "TTFT": 11.292267322540283, "answer": "[\"CONTRASTIVE SEMI-SUPERVISED LEARNING FOR ASR\", \"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\", \"SUPERB: Speech processing Universal PERformance Benchmark\"]", "f1 socre": 0.38461538461538464}
{"id": "cd618a43-7176-432d-b21e-432e5c2ca4a7", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\" by Abien Fred M. Agarap, 2018, ICMLC 2018]\n\n[\"A Novel Approach Combining Recurrent Neural Network and"], "TTFT": 6.193939208984375, "answer": "[\"Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems\", \"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\"]", "f1 socre": 0.34374999999999994}
{"id": "e904afd3-5cb9-4bf7-bbee-9c8defac858c", "pred": ["Based on the provided instructions and papers, the longest citation chain can be constructed as follows:\n\n[\"On the performativity of SDG classifications in large bibliometric databases\" by Ottaviani and Stahlschmidt (2024), \"Language models are unsupervised multitask learners\" by Radford et al. (2019), \"DistilBERT: Distilling BERT: Six-Order Distillation for a Fast and Che"], "TTFT": 7.292623519897461, "answer": "[\"Contrastive Decoding: Open-ended Text Generation as Optimization\", \"An Empirical Study On Contrastive Search And Contrastive Decoding For Open-ended Text Generation\", \"On the performativity of SDG classifications in large bibliometric databases\"]", "f1 socre": 0.380952380952381}
{"id": "683f4cfd-1cfe-475d-927a-08a71d4f69e9", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\" by Niimi et al., 2024]\n\n[\"Churn Prediction with"], "TTFT": 11.674830198287964, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\", \"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\", \"An Efficient Multimodal Learning Framework to Comprehend Consumer Preferences Using BERT and Cross-Attention\"]", "f1 socre": 0.45312500000000006}
{"id": "0223faf0-f46f-459d-b1cd-45c4dab78fc2", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"A tutorial on the range variant of asymmetric numeral systems\" by James Townsend, \"Asymmetric numeral systems\" by Jarek Duda, \"Asymmetric numeral systems: entropy coding combining speed of Huffman coding with compression rate of arithmetic coding\" by Jarek Duda, Khalid Tahboub, Neeraj J. Gad"], "TTFT": 14.098109483718872, "answer": "[\"Lightweight compression with encryption based on Asymmetric Numeral Systems\", \"A tutorial on the range variant of asymmetric numeral systems\", \"ATC: an Advanced Tucker Compression library for multidimensional data\"]", "f1 socre": 0.3652173913043478}
{"id": "b7405d8e-13f0-4750-a25d-df3a67ffd39a", "pred": ["{{\"Reference\": [\"Benchmarking Large Language Models in Retrieval-Augmented Generation\", \"Retrieval-Augmented Generation in Question Answering: A Survey\"],\n\"Citation\": [\"Seven Failure Points When Engineering a Retrieval Augmented Generation System\"]}}\n\nThis paper presents the lessons learned and failure points from three case studies of implementing Retrieval-Augmented Generation (RAG) systems. It also discusses"], "TTFT": 10.79617714881897, "answer": "{\"Reference\": [\"Benchmarking Large Language Models in Retrieval-Augmented Generation\"], \"Citation\": [\"When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively\"]}", "f1 socre": 0.4186046511627907}
{"id": "13a4a371-6339-4c9d-82cf-fc9ab2bb017d", "pred": ["{\n\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"],\n\"Citation\": []\n}\n\nThe paper \"OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models\" does not mention \"TinyLlama: An Open-Source Small Language Model\" in its text, so there is no citation relationship between the two papers. However, \"OpenMoE\" references \"Tiny"], "TTFT": 13.604034900665283, "answer": "{\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"], \"Citation\": [\"Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training\"]}", "f1 socre": 0.4047619047619047}
{"id": "c4a4e642-fa00-4377-a756-40f2f7ea7a74", "pred": ["{{\"Reference\": [\"A Survey of Hallucination in Large Foundation Models by Vipula Rawte, Amit Sheth, and Amitava Das\"], \"Citation\": [\"CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation by Quan Tu, Shilong Fan, Zihang Tian, Rui Yan\"]}}\n\nThis paper on CharacterEval references the survey paper on hallucination in large foundation models and"], "TTFT": 11.539960622787476, "answer": "{\"Reference\": [\"A Survey of Hallucination in \\\"Large\\\" Foundation Models\"], \"Citation\": [\"From Role-Play to Drama-Interaction: An LLM Solution\"]}", "f1 socre": 0.2745098039215686}
{"id": "835a4005-8c32-4083-b52d-4e014d1d35f5", "pred": ["Based on the provided instructions and papers, here is the longest citation chain:\n\n[\"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\" by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed, \"SUPERB: Speech processing Universal"], "TTFT": 11.292267322540283, "answer": "[\"CONTRASTIVE SEMI-SUPERVISED LEARNING FOR ASR\", \"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\", \"SUPERB: Speech processing Universal PERformance Benchmark\"]", "f1 socre": 0.38461538461538464}
{"id": "cd618a43-7176-432d-b21e-432e5c2ca4a7", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\" by Abien Fred M. Agarap, 2018, ICMLC 2018]\n\n[\"A Novel Approach Combining Recurrent Neural Network and"], "TTFT": 6.193939208984375, "answer": "[\"Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems\", \"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\"]", "f1 socre": 0.34374999999999994}
{"id": "e904afd3-5cb9-4bf7-bbee-9c8defac858c", "pred": ["Based on the provided instructions and papers, the longest citation chain can be constructed as follows:\n\n[\"On the performativity of SDG classifications in large bibliometric databases\" by Ottaviani and Stahlschmidt (2024), \"Language models are unsupervised multitask learners\" by Radford et al. (2019), \"DistilBERT: Distilling BERT: Six-Order Distillation for a Fast and Che"], "TTFT": 7.292623519897461, "answer": "[\"Contrastive Decoding: Open-ended Text Generation as Optimization\", \"An Empirical Study On Contrastive Search And Contrastive Decoding For Open-ended Text Generation\", \"On the performativity of SDG classifications in large bibliometric databases\"]", "f1 socre": 0.380952380952381}
{"id": "683f4cfd-1cfe-475d-927a-08a71d4f69e9", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\" by Niimi et al., 2024]\n\n[\"Churn Prediction with"], "TTFT": 11.674830198287964, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\", \"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\", \"An Efficient Multimodal Learning Framework to Comprehend Consumer Preferences Using BERT and Cross-Attention\"]", "f1 socre": 0.45312500000000006}
{"id": "0223faf0-f46f-459d-b1cd-45c4dab78fc2", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"A tutorial on the range variant of asymmetric numeral systems\" by James Townsend, \"Asymmetric numeral systems\" by Jarek Duda, \"Asymmetric numeral systems: entropy coding combining speed of Huffman coding with compression rate of arithmetic coding\" by Jarek Duda, Khalid Tahboub, Neeraj J. Gad"], "TTFT": 14.098109483718872, "answer": "[\"Lightweight compression with encryption based on Asymmetric Numeral Systems\", \"A tutorial on the range variant of asymmetric numeral systems\", \"ATC: an Advanced Tucker Compression library for multidimensional data\"]", "f1 socre": 0.3652173913043478}
{"id": "b7405d8e-13f0-4750-a25d-df3a67ffd39a", "pred": ["{{\"Reference\": [\"Benchmarking Large Language Models in Retrieval-Augmented Generation\", \"Retrieval-Augmented Generation in Question Answering: A Survey\"],\n\"Citation\": [\"Seven Failure Points When Engineering a Retrieval Augmented Generation System\"]}}\n\nThis paper presents the lessons learned and failure points from three case studies of implementing Retrieval-Augmented Generation (RAG) systems. It also discusses"], "TTFT": 10.79617714881897, "answer": "{\"Reference\": [\"Benchmarking Large Language Models in Retrieval-Augmented Generation\"], \"Citation\": [\"When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively\"]}", "f1 socre": 0.4186046511627907}
{"id": "150529cd-8767-477e-aa2b-28bae4970fdd", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System\" by Santos et al., \"Massively scalable stencil algorithm for the 3D wave equation on the Cerebras Wafer-Scale Engine\" by Jacquelin and Araya-Polo]\n\nThe reason for this citation chain is that the paper by Sant"], "TTFT": 15.363740921020508, "answer": "[\"Massively scalable stencil algorithm\", \"Disruptive Changes in Field Equation Modeling A Simple Interface for Wafer Scale Engines\", \"Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System\"]", "f1 socre": 0.39285714285714285}
{"id": "13a4a371-6339-4c9d-82cf-fc9ab2bb017d", "pred": ["{\n\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"],\n\"Citation\": []\n}\n\nThe paper \"OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models\" does not mention \"TinyLlama: An Open-Source Small Language Model\" in its text, so there is no citation relationship between the two papers. However, \"OpenMoE\" references \"Tiny"], "TTFT": 13.604034900665283, "answer": "{\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"], \"Citation\": [\"Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training\"]}", "f1 socre": 0.4047619047619047}
{"id": "c4a4e642-fa00-4377-a756-40f2f7ea7a74", "pred": ["{{\"Reference\": [\"A Survey of Hallucination in Large Foundation Models by Vipula Rawte, Amit Sheth, and Amitava Das\"], \"Citation\": [\"CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation by Quan Tu, Shilong Fan, Zihang Tian, Rui Yan\"]}}\n\nThis paper on CharacterEval references the survey paper on hallucination in large foundation models and"], "TTFT": 11.539960622787476, "answer": "{\"Reference\": [\"A Survey of Hallucination in \\\"Large\\\" Foundation Models\"], \"Citation\": [\"From Role-Play to Drama-Interaction: An LLM Solution\"]}", "f1 socre": 0.2745098039215686}
{"id": "835a4005-8c32-4083-b52d-4e014d1d35f5", "pred": ["Based on the provided instructions and papers, here is the longest citation chain:\n\n[\"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\" by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed, \"SUPERB: Speech processing Universal"], "TTFT": 11.292267322540283, "answer": "[\"CONTRASTIVE SEMI-SUPERVISED LEARNING FOR ASR\", \"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\", \"SUPERB: Speech processing Universal PERformance Benchmark\"]", "f1 socre": 0.38461538461538464}
{"id": "cd618a43-7176-432d-b21e-432e5c2ca4a7", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\" by Abien Fred M. Agarap, 2018, ICMLC 2018]\n\n[\"A Novel Approach Combining Recurrent Neural Network and"], "TTFT": 6.193939208984375, "answer": "[\"Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems\", \"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\"]", "f1 socre": 0.34374999999999994}
{"id": "e904afd3-5cb9-4bf7-bbee-9c8defac858c", "pred": ["Based on the provided instructions and papers, the longest citation chain can be constructed as follows:\n\n[\"On the performativity of SDG classifications in large bibliometric databases\" by Ottaviani and Stahlschmidt (2024), \"Language models are unsupervised multitask learners\" by Radford et al. (2019), \"DistilBERT: Distilling BERT: Six-Order Distillation for a Fast and Che"], "TTFT": 7.292623519897461, "answer": "[\"Contrastive Decoding: Open-ended Text Generation as Optimization\", \"An Empirical Study On Contrastive Search And Contrastive Decoding For Open-ended Text Generation\", \"On the performativity of SDG classifications in large bibliometric databases\"]", "f1 socre": 0.380952380952381}
{"id": "683f4cfd-1cfe-475d-927a-08a71d4f69e9", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\" by Niimi et al., 2024]\n\n[\"Churn Prediction with"], "TTFT": 11.674830198287964, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\", \"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\", \"An Efficient Multimodal Learning Framework to Comprehend Consumer Preferences Using BERT and Cross-Attention\"]", "f1 socre": 0.45312500000000006}
{"id": "0223faf0-f46f-459d-b1cd-45c4dab78fc2", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"A tutorial on the range variant of asymmetric numeral systems\" by James Townsend, \"Asymmetric numeral systems\" by Jarek Duda, \"Asymmetric numeral systems: entropy coding combining speed of Huffman coding with compression rate of arithmetic coding\" by Jarek Duda, Khalid Tahboub, Neeraj J. Gad"], "TTFT": 14.098109483718872, "answer": "[\"Lightweight compression with encryption based on Asymmetric Numeral Systems\", \"A tutorial on the range variant of asymmetric numeral systems\", \"ATC: an Advanced Tucker Compression library for multidimensional data\"]", "f1 socre": 0.3652173913043478}
{"id": "b7405d8e-13f0-4750-a25d-df3a67ffd39a", "pred": ["{{\"Reference\": [\"Benchmarking Large Language Models in Retrieval-Augmented Generation\", \"Retrieval-Augmented Generation in Question Answering: A Survey\"],\n\"Citation\": [\"Seven Failure Points When Engineering a Retrieval Augmented Generation System\"]}}\n\nThis paper presents the lessons learned and failure points from three case studies of implementing Retrieval-Augmented Generation (RAG) systems. It also discusses"], "TTFT": 10.79617714881897, "answer": "{\"Reference\": [\"Benchmarking Large Language Models in Retrieval-Augmented Generation\"], \"Citation\": [\"When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively\"]}", "f1 socre": 0.4186046511627907}
{"id": "150529cd-8767-477e-aa2b-28bae4970fdd", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System\" by Santos et al., \"Massively scalable stencil algorithm for the 3D wave equation on the Cerebras Wafer-Scale Engine\" by Jacquelin and Araya-Polo]\n\nThe reason for this citation chain is that the paper by Sant"], "TTFT": 15.363740921020508, "answer": "[\"Massively scalable stencil algorithm\", \"Disruptive Changes in Field Equation Modeling A Simple Interface for Wafer Scale Engines\", \"Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System\"]", "f1 socre": 0.39285714285714285}
{"id": "a8fa7aa1-2198-419b-a0de-6ba4d1c1f160", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Dance Any Beat: Blending Beats with Visuals in Dance Video Generation\" by Xuanchen Wang, Heng Wang, Weidong Cai, and Dongnan Liu],\n[\"InMoDeGAN: Interpretable Motion Decomposing Generative Adversarial Network for Video Generation\" by Yaohui Wang, Francois Bremond,"], "TTFT": 14.07155442237854, "answer": "[\"InMoDeGAN: Interpretable Motion Decomposition Generative Adversarial Network for Video Generation\", \"LATENT IMAGE ANIMATOR: <br> LEARNING TO ANIMATE IMAGES VIA LATENT SPACE NAVIGATION\", \"Dance Any Beat: Blending Beats with Visuals in Dance Video Generation\"]", "f1 socre": 0.43548387096774194}
{"id": "13a4a371-6339-4c9d-82cf-fc9ab2bb017d", "pred": ["{\n\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"],\n\"Citation\": []\n}\n\nThe paper \"OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models\" does not mention \"TinyLlama: An Open-Source Small Language Model\" in its text, so there is no citation relationship between the two papers. However, \"OpenMoE\" references \"Tiny"], "TTFT": 13.604034900665283, "answer": "{\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"], \"Citation\": [\"Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training\"]}", "f1 socre": 0.4047619047619047}
{"id": "c4a4e642-fa00-4377-a756-40f2f7ea7a74", "pred": ["{{\"Reference\": [\"A Survey of Hallucination in Large Foundation Models by Vipula Rawte, Amit Sheth, and Amitava Das\"], \"Citation\": [\"CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation by Quan Tu, Shilong Fan, Zihang Tian, Rui Yan\"]}}\n\nThis paper on CharacterEval references the survey paper on hallucination in large foundation models and"], "TTFT": 11.539960622787476, "answer": "{\"Reference\": [\"A Survey of Hallucination in \\\"Large\\\" Foundation Models\"], \"Citation\": [\"From Role-Play to Drama-Interaction: An LLM Solution\"]}", "f1 socre": 0.2745098039215686}
{"id": "835a4005-8c32-4083-b52d-4e014d1d35f5", "pred": ["Based on the provided instructions and papers, here is the longest citation chain:\n\n[\"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\" by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed, \"SUPERB: Speech processing Universal"], "TTFT": 11.292267322540283, "answer": "[\"CONTRASTIVE SEMI-SUPERVISED LEARNING FOR ASR\", \"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\", \"SUPERB: Speech processing Universal PERformance Benchmark\"]", "f1 socre": 0.38461538461538464}
{"id": "cd618a43-7176-432d-b21e-432e5c2ca4a7", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\" by Abien Fred M. Agarap, 2018, ICMLC 2018]\n\n[\"A Novel Approach Combining Recurrent Neural Network and"], "TTFT": 6.193939208984375, "answer": "[\"Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems\", \"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\"]", "f1 socre": 0.34374999999999994}
{"id": "e904afd3-5cb9-4bf7-bbee-9c8defac858c", "pred": ["Based on the provided instructions and papers, the longest citation chain can be constructed as follows:\n\n[\"On the performativity of SDG classifications in large bibliometric databases\" by Ottaviani and Stahlschmidt (2024), \"Language models are unsupervised multitask learners\" by Radford et al. (2019), \"DistilBERT: Distilling BERT: Six-Order Distillation for a Fast and Che"], "TTFT": 7.292623519897461, "answer": "[\"Contrastive Decoding: Open-ended Text Generation as Optimization\", \"An Empirical Study On Contrastive Search And Contrastive Decoding For Open-ended Text Generation\", \"On the performativity of SDG classifications in large bibliometric databases\"]", "f1 socre": 0.380952380952381}
{"id": "683f4cfd-1cfe-475d-927a-08a71d4f69e9", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\" by Niimi et al., 2024]\n\n[\"Churn Prediction with"], "TTFT": 11.674830198287964, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\", \"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\", \"An Efficient Multimodal Learning Framework to Comprehend Consumer Preferences Using BERT and Cross-Attention\"]", "f1 socre": 0.45312500000000006}
{"id": "0223faf0-f46f-459d-b1cd-45c4dab78fc2", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"A tutorial on the range variant of asymmetric numeral systems\" by James Townsend, \"Asymmetric numeral systems\" by Jarek Duda, \"Asymmetric numeral systems: entropy coding combining speed of Huffman coding with compression rate of arithmetic coding\" by Jarek Duda, Khalid Tahboub, Neeraj J. Gad"], "TTFT": 14.098109483718872, "answer": "[\"Lightweight compression with encryption based on Asymmetric Numeral Systems\", \"A tutorial on the range variant of asymmetric numeral systems\", \"ATC: an Advanced Tucker Compression library for multidimensional data\"]", "f1 socre": 0.3652173913043478}
{"id": "b7405d8e-13f0-4750-a25d-df3a67ffd39a", "pred": ["{{\"Reference\": [\"Benchmarking Large Language Models in Retrieval-Augmented Generation\", \"Retrieval-Augmented Generation in Question Answering: A Survey\"],\n\"Citation\": [\"Seven Failure Points When Engineering a Retrieval Augmented Generation System\"]}}\n\nThis paper presents the lessons learned and failure points from three case studies of implementing Retrieval-Augmented Generation (RAG) systems. It also discusses"], "TTFT": 10.79617714881897, "answer": "{\"Reference\": [\"Benchmarking Large Language Models in Retrieval-Augmented Generation\"], \"Citation\": [\"When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively\"]}", "f1 socre": 0.4186046511627907}
{"id": "150529cd-8767-477e-aa2b-28bae4970fdd", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System\" by Santos et al., \"Massively scalable stencil algorithm for the 3D wave equation on the Cerebras Wafer-Scale Engine\" by Jacquelin and Araya-Polo]\n\nThe reason for this citation chain is that the paper by Sant"], "TTFT": 15.363740921020508, "answer": "[\"Massively scalable stencil algorithm\", \"Disruptive Changes in Field Equation Modeling A Simple Interface for Wafer Scale Engines\", \"Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System\"]", "f1 socre": 0.39285714285714285}
{"id": "a8fa7aa1-2198-419b-a0de-6ba4d1c1f160", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Dance Any Beat: Blending Beats with Visuals in Dance Video Generation\" by Xuanchen Wang, Heng Wang, Weidong Cai, and Dongnan Liu],\n[\"InMoDeGAN: Interpretable Motion Decomposing Generative Adversarial Network for Video Generation\" by Yaohui Wang, Francois Bremond,"], "TTFT": 14.07155442237854, "answer": "[\"InMoDeGAN: Interpretable Motion Decomposition Generative Adversarial Network for Video Generation\", \"LATENT IMAGE ANIMATOR: <br> LEARNING TO ANIMATE IMAGES VIA LATENT SPACE NAVIGATION\", \"Dance Any Beat: Blending Beats with Visuals in Dance Video Generation\"]", "f1 socre": 0.43548387096774194}
{"id": "49680a79-a974-49b0-b7e3-85ff515bc4f2", "pred": ["{{\"Reference\": [\"Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding\"], \"Citation\": [\"Smurfs: Leveraging Multiple Proficiency Agents for Effective Tool Utilization in LLMs\" ]}}\n\nThe paper \"EasyTool: Enhancing LLM-based Agents with Concise Tool Instruction\" references the paper \"Don't Fine-Tune, Dec"], "TTFT": 10.98206639289856, "answer": "{\"Reference\": [\"Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding\"], \"Citation\": [\"Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning\"]}", "f1 socre": 0.6521739130434783}
{"id": "13a4a371-6339-4c9d-82cf-fc9ab2bb017d", "pred": ["{\n\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"],\n\"Citation\": []\n}\n\nThe paper \"OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models\" does not mention \"TinyLlama: An Open-Source Small Language Model\" in its text, so there is no citation relationship between the two papers. However, \"OpenMoE\" references \"Tiny"], "TTFT": 13.604034900665283, "answer": "{\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"], \"Citation\": [\"Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training\"]}", "f1 socre": 0.4047619047619047}
{"id": "c4a4e642-fa00-4377-a756-40f2f7ea7a74", "pred": ["{{\"Reference\": [\"A Survey of Hallucination in Large Foundation Models by Vipula Rawte, Amit Sheth, and Amitava Das\"], \"Citation\": [\"CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation by Quan Tu, Shilong Fan, Zihang Tian, Rui Yan\"]}}\n\nThis paper on CharacterEval references the survey paper on hallucination in large foundation models and"], "TTFT": 11.539960622787476, "answer": "{\"Reference\": [\"A Survey of Hallucination in \\\"Large\\\" Foundation Models\"], \"Citation\": [\"From Role-Play to Drama-Interaction: An LLM Solution\"]}", "f1 socre": 0.2745098039215686}
{"id": "835a4005-8c32-4083-b52d-4e014d1d35f5", "pred": ["Based on the provided instructions and papers, here is the longest citation chain:\n\n[\"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\" by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed, \"SUPERB: Speech processing Universal"], "TTFT": 11.292267322540283, "answer": "[\"CONTRASTIVE SEMI-SUPERVISED LEARNING FOR ASR\", \"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\", \"SUPERB: Speech processing Universal PERformance Benchmark\"]", "f1 socre": 0.38461538461538464}
{"id": "cd618a43-7176-432d-b21e-432e5c2ca4a7", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\" by Abien Fred M. Agarap, 2018, ICMLC 2018]\n\n[\"A Novel Approach Combining Recurrent Neural Network and"], "TTFT": 6.193939208984375, "answer": "[\"Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems\", \"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\"]", "f1 socre": 0.34374999999999994}
{"id": "e904afd3-5cb9-4bf7-bbee-9c8defac858c", "pred": ["Based on the provided instructions and papers, the longest citation chain can be constructed as follows:\n\n[\"On the performativity of SDG classifications in large bibliometric databases\" by Ottaviani and Stahlschmidt (2024), \"Language models are unsupervised multitask learners\" by Radford et al. (2019), \"DistilBERT: Distilling BERT: Six-Order Distillation for a Fast and Che"], "TTFT": 7.292623519897461, "answer": "[\"Contrastive Decoding: Open-ended Text Generation as Optimization\", \"An Empirical Study On Contrastive Search And Contrastive Decoding For Open-ended Text Generation\", \"On the performativity of SDG classifications in large bibliometric databases\"]", "f1 socre": 0.380952380952381}
{"id": "683f4cfd-1cfe-475d-927a-08a71d4f69e9", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\" by Niimi et al., 2024]\n\n[\"Churn Prediction with"], "TTFT": 11.674830198287964, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\", \"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\", \"An Efficient Multimodal Learning Framework to Comprehend Consumer Preferences Using BERT and Cross-Attention\"]", "f1 socre": 0.45312500000000006}
{"id": "0223faf0-f46f-459d-b1cd-45c4dab78fc2", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"A tutorial on the range variant of asymmetric numeral systems\" by James Townsend, \"Asymmetric numeral systems\" by Jarek Duda, \"Asymmetric numeral systems: entropy coding combining speed of Huffman coding with compression rate of arithmetic coding\" by Jarek Duda, Khalid Tahboub, Neeraj J. Gad"], "TTFT": 14.098109483718872, "answer": "[\"Lightweight compression with encryption based on Asymmetric Numeral Systems\", \"A tutorial on the range variant of asymmetric numeral systems\", \"ATC: an Advanced Tucker Compression library for multidimensional data\"]", "f1 socre": 0.3652173913043478}
{"id": "b7405d8e-13f0-4750-a25d-df3a67ffd39a", "pred": ["{{\"Reference\": [\"Benchmarking Large Language Models in Retrieval-Augmented Generation\", \"Retrieval-Augmented Generation in Question Answering: A Survey\"],\n\"Citation\": [\"Seven Failure Points When Engineering a Retrieval Augmented Generation System\"]}}\n\nThis paper presents the lessons learned and failure points from three case studies of implementing Retrieval-Augmented Generation (RAG) systems. It also discusses"], "TTFT": 10.79617714881897, "answer": "{\"Reference\": [\"Benchmarking Large Language Models in Retrieval-Augmented Generation\"], \"Citation\": [\"When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively\"]}", "f1 socre": 0.4186046511627907}
{"id": "150529cd-8767-477e-aa2b-28bae4970fdd", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System\" by Santos et al., \"Massively scalable stencil algorithm for the 3D wave equation on the Cerebras Wafer-Scale Engine\" by Jacquelin and Araya-Polo]\n\nThe reason for this citation chain is that the paper by Sant"], "TTFT": 15.363740921020508, "answer": "[\"Massively scalable stencil algorithm\", \"Disruptive Changes in Field Equation Modeling A Simple Interface for Wafer Scale Engines\", \"Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System\"]", "f1 socre": 0.39285714285714285}
{"id": "a8fa7aa1-2198-419b-a0de-6ba4d1c1f160", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Dance Any Beat: Blending Beats with Visuals in Dance Video Generation\" by Xuanchen Wang, Heng Wang, Weidong Cai, and Dongnan Liu],\n[\"InMoDeGAN: Interpretable Motion Decomposing Generative Adversarial Network for Video Generation\" by Yaohui Wang, Francois Bremond,"], "TTFT": 14.07155442237854, "answer": "[\"InMoDeGAN: Interpretable Motion Decomposition Generative Adversarial Network for Video Generation\", \"LATENT IMAGE ANIMATOR: <br> LEARNING TO ANIMATE IMAGES VIA LATENT SPACE NAVIGATION\", \"Dance Any Beat: Blending Beats with Visuals in Dance Video Generation\"]", "f1 socre": 0.43548387096774194}
{"id": "49680a79-a974-49b0-b7e3-85ff515bc4f2", "pred": ["{{\"Reference\": [\"Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding\"], \"Citation\": [\"Smurfs: Leveraging Multiple Proficiency Agents for Effective Tool Utilization in LLMs\" ]}}\n\nThe paper \"EasyTool: Enhancing LLM-based Agents with Concise Tool Instruction\" references the paper \"Don't Fine-Tune, Dec"], "TTFT": 10.98206639289856, "answer": "{\"Reference\": [\"Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding\"], \"Citation\": [\"Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning\"]}", "f1 socre": 0.6521739130434783}
{"id": "9f29f552-c817-4c2a-8910-b5e8ae7c014f", "pred": ["[\"Fine-Tuning LLaMA for Multi-Stage Text Retrieval\" by Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin, 2023, cited by \"Improving Text Embeddings with Large Language Models\" by Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, Furu Wei, and Jimmy Lin,"], "TTFT": 13.682081937789917, "answer": "[\"RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large Language Models\", \"Fine-Tuning LLaMA for Multi-Stage Text Retrieval\", \"Improving Text Embeddings with Large Language Models\"]", "f1 socre": 0.3225806451612903}
{"id": "13a4a371-6339-4c9d-82cf-fc9ab2bb017d", "pred": ["{\n\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"],\n\"Citation\": []\n}\n\nThe paper \"OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models\" does not mention \"TinyLlama: An Open-Source Small Language Model\" in its text, so there is no citation relationship between the two papers. However, \"OpenMoE\" references \"Tiny"], "TTFT": 13.604034900665283, "answer": "{\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"], \"Citation\": [\"Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training\"]}", "f1 socre": 0.4047619047619047}
{"id": "c4a4e642-fa00-4377-a756-40f2f7ea7a74", "pred": ["{{\"Reference\": [\"A Survey of Hallucination in Large Foundation Models by Vipula Rawte, Amit Sheth, and Amitava Das\"], \"Citation\": [\"CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation by Quan Tu, Shilong Fan, Zihang Tian, Rui Yan\"]}}\n\nThis paper on CharacterEval references the survey paper on hallucination in large foundation models and"], "TTFT": 11.539960622787476, "answer": "{\"Reference\": [\"A Survey of Hallucination in \\\"Large\\\" Foundation Models\"], \"Citation\": [\"From Role-Play to Drama-Interaction: An LLM Solution\"]}", "f1 socre": 0.2745098039215686}
{"id": "835a4005-8c32-4083-b52d-4e014d1d35f5", "pred": ["Based on the provided instructions and papers, here is the longest citation chain:\n\n[\"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\" by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed, \"SUPERB: Speech processing Universal"], "TTFT": 11.292267322540283, "answer": "[\"CONTRASTIVE SEMI-SUPERVISED LEARNING FOR ASR\", \"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\", \"SUPERB: Speech processing Universal PERformance Benchmark\"]", "f1 socre": 0.38461538461538464}
{"id": "cd618a43-7176-432d-b21e-432e5c2ca4a7", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\" by Abien Fred M. Agarap, 2018, ICMLC 2018]\n\n[\"A Novel Approach Combining Recurrent Neural Network and"], "TTFT": 6.193939208984375, "answer": "[\"Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems\", \"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\"]", "f1 socre": 0.34374999999999994}
{"id": "e904afd3-5cb9-4bf7-bbee-9c8defac858c", "pred": ["Based on the provided instructions and papers, the longest citation chain can be constructed as follows:\n\n[\"On the performativity of SDG classifications in large bibliometric databases\" by Ottaviani and Stahlschmidt (2024), \"Language models are unsupervised multitask learners\" by Radford et al. (2019), \"DistilBERT: Distilling BERT: Six-Order Distillation for a Fast and Che"], "TTFT": 7.292623519897461, "answer": "[\"Contrastive Decoding: Open-ended Text Generation as Optimization\", \"An Empirical Study On Contrastive Search And Contrastive Decoding For Open-ended Text Generation\", \"On the performativity of SDG classifications in large bibliometric databases\"]", "f1 socre": 0.380952380952381}
{"id": "683f4cfd-1cfe-475d-927a-08a71d4f69e9", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\" by Niimi et al., 2024]\n\n[\"Churn Prediction with"], "TTFT": 11.674830198287964, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\", \"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\", \"An Efficient Multimodal Learning Framework to Comprehend Consumer Preferences Using BERT and Cross-Attention\"]", "f1 socre": 0.45312500000000006}
{"id": "0223faf0-f46f-459d-b1cd-45c4dab78fc2", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"A tutorial on the range variant of asymmetric numeral systems\" by James Townsend, \"Asymmetric numeral systems\" by Jarek Duda, \"Asymmetric numeral systems: entropy coding combining speed of Huffman coding with compression rate of arithmetic coding\" by Jarek Duda, Khalid Tahboub, Neeraj J. Gad"], "TTFT": 14.098109483718872, "answer": "[\"Lightweight compression with encryption based on Asymmetric Numeral Systems\", \"A tutorial on the range variant of asymmetric numeral systems\", \"ATC: an Advanced Tucker Compression library for multidimensional data\"]", "f1 socre": 0.3652173913043478}
{"id": "b7405d8e-13f0-4750-a25d-df3a67ffd39a", "pred": ["{{\"Reference\": [\"Benchmarking Large Language Models in Retrieval-Augmented Generation\", \"Retrieval-Augmented Generation in Question Answering: A Survey\"],\n\"Citation\": [\"Seven Failure Points When Engineering a Retrieval Augmented Generation System\"]}}\n\nThis paper presents the lessons learned and failure points from three case studies of implementing Retrieval-Augmented Generation (RAG) systems. It also discusses"], "TTFT": 10.79617714881897, "answer": "{\"Reference\": [\"Benchmarking Large Language Models in Retrieval-Augmented Generation\"], \"Citation\": [\"When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively\"]}", "f1 socre": 0.4186046511627907}
{"id": "150529cd-8767-477e-aa2b-28bae4970fdd", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System\" by Santos et al., \"Massively scalable stencil algorithm for the 3D wave equation on the Cerebras Wafer-Scale Engine\" by Jacquelin and Araya-Polo]\n\nThe reason for this citation chain is that the paper by Sant"], "TTFT": 15.363740921020508, "answer": "[\"Massively scalable stencil algorithm\", \"Disruptive Changes in Field Equation Modeling A Simple Interface for Wafer Scale Engines\", \"Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System\"]", "f1 socre": 0.39285714285714285}
{"id": "a8fa7aa1-2198-419b-a0de-6ba4d1c1f160", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Dance Any Beat: Blending Beats with Visuals in Dance Video Generation\" by Xuanchen Wang, Heng Wang, Weidong Cai, and Dongnan Liu],\n[\"InMoDeGAN: Interpretable Motion Decomposing Generative Adversarial Network for Video Generation\" by Yaohui Wang, Francois Bremond,"], "TTFT": 14.07155442237854, "answer": "[\"InMoDeGAN: Interpretable Motion Decomposition Generative Adversarial Network for Video Generation\", \"LATENT IMAGE ANIMATOR: <br> LEARNING TO ANIMATE IMAGES VIA LATENT SPACE NAVIGATION\", \"Dance Any Beat: Blending Beats with Visuals in Dance Video Generation\"]", "f1 socre": 0.43548387096774194}
{"id": "49680a79-a974-49b0-b7e3-85ff515bc4f2", "pred": ["{{\"Reference\": [\"Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding\"], \"Citation\": [\"Smurfs: Leveraging Multiple Proficiency Agents for Effective Tool Utilization in LLMs\" ]}}\n\nThe paper \"EasyTool: Enhancing LLM-based Agents with Concise Tool Instruction\" references the paper \"Don't Fine-Tune, Dec"], "TTFT": 10.98206639289856, "answer": "{\"Reference\": [\"Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding\"], \"Citation\": [\"Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning\"]}", "f1 socre": 0.6521739130434783}
{"id": "9f29f552-c817-4c2a-8910-b5e8ae7c014f", "pred": ["[\"Fine-Tuning LLaMA for Multi-Stage Text Retrieval\" by Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin, 2023, cited by \"Improving Text Embeddings with Large Language Models\" by Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, Furu Wei, and Jimmy Lin,"], "TTFT": 13.682081937789917, "answer": "[\"RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large Language Models\", \"Fine-Tuning LLaMA for Multi-Stage Text Retrieval\", \"Improving Text Embeddings with Large Language Models\"]", "f1 socre": 0.3225806451612903}
{"id": "30a2535f-7e73-46ad-a402-ee80ac6e4f92", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"Quantum Annealers Chain Strengths: A Simple Heuristic to Set Them All\" by Valentin Gilbert and St\u00e9phane Louise, 2024, arXiv: 2404.05443 [39]],\n[\"Benchmarking Quantum Annealers with Near-Optimal Minor Embeddings\" by Valentin Gilbert"], "TTFT": 16.3840811252594, "answer": "[\"Scaling Advantage in Approximate Optimization with Quantum Annealing\", \"4-clique Network Minor Embedding for Quantum Annealers\", \"Quantum Annealers Chain Strengths: A Simple Heuristic to Set Them All\", \"Benchmarking Quantum Annealers with Near-Optimal Minor-Embedded Instances\"]", "f1 socre": 0.40579710144927533}
{"id": "13a4a371-6339-4c9d-82cf-fc9ab2bb017d", "pred": ["{\n\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"],\n\"Citation\": []\n}\n\nThe paper \"OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models\" does not mention \"TinyLlama: An Open-Source Small Language Model\" in its text, so there is no citation relationship between the two papers. However, \"OpenMoE\" references \"Tiny"], "TTFT": 13.604034900665283, "answer": "{\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"], \"Citation\": [\"Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training\"]}", "f1 socre": 0.4047619047619047}
{"id": "c4a4e642-fa00-4377-a756-40f2f7ea7a74", "pred": ["{{\"Reference\": [\"A Survey of Hallucination in Large Foundation Models by Vipula Rawte, Amit Sheth, and Amitava Das\"], \"Citation\": [\"CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation by Quan Tu, Shilong Fan, Zihang Tian, Rui Yan\"]}}\n\nThis paper on CharacterEval references the survey paper on hallucination in large foundation models and"], "TTFT": 11.539960622787476, "answer": "{\"Reference\": [\"A Survey of Hallucination in \\\"Large\\\" Foundation Models\"], \"Citation\": [\"From Role-Play to Drama-Interaction: An LLM Solution\"]}", "f1 socre": 0.2745098039215686}
{"id": "835a4005-8c32-4083-b52d-4e014d1d35f5", "pred": ["Based on the provided instructions and papers, here is the longest citation chain:\n\n[\"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\" by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed, \"SUPERB: Speech processing Universal"], "TTFT": 11.292267322540283, "answer": "[\"CONTRASTIVE SEMI-SUPERVISED LEARNING FOR ASR\", \"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\", \"SUPERB: Speech processing Universal PERformance Benchmark\"]", "f1 socre": 0.38461538461538464}
{"id": "cd618a43-7176-432d-b21e-432e5c2ca4a7", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\" by Abien Fred M. Agarap, 2018, ICMLC 2018]\n\n[\"A Novel Approach Combining Recurrent Neural Network and"], "TTFT": 6.193939208984375, "answer": "[\"Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems\", \"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\"]", "f1 socre": 0.34374999999999994}
{"id": "e904afd3-5cb9-4bf7-bbee-9c8defac858c", "pred": ["Based on the provided instructions and papers, the longest citation chain can be constructed as follows:\n\n[\"On the performativity of SDG classifications in large bibliometric databases\" by Ottaviani and Stahlschmidt (2024), \"Language models are unsupervised multitask learners\" by Radford et al. (2019), \"DistilBERT: Distilling BERT: Six-Order Distillation for a Fast and Che"], "TTFT": 7.292623519897461, "answer": "[\"Contrastive Decoding: Open-ended Text Generation as Optimization\", \"An Empirical Study On Contrastive Search And Contrastive Decoding For Open-ended Text Generation\", \"On the performativity of SDG classifications in large bibliometric databases\"]", "f1 socre": 0.380952380952381}
{"id": "683f4cfd-1cfe-475d-927a-08a71d4f69e9", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\" by Niimi et al., 2024]\n\n[\"Churn Prediction with"], "TTFT": 11.674830198287964, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\", \"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\", \"An Efficient Multimodal Learning Framework to Comprehend Consumer Preferences Using BERT and Cross-Attention\"]", "f1 socre": 0.45312500000000006}
{"id": "0223faf0-f46f-459d-b1cd-45c4dab78fc2", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"A tutorial on the range variant of asymmetric numeral systems\" by James Townsend, \"Asymmetric numeral systems\" by Jarek Duda, \"Asymmetric numeral systems: entropy coding combining speed of Huffman coding with compression rate of arithmetic coding\" by Jarek Duda, Khalid Tahboub, Neeraj J. Gad"], "TTFT": 14.098109483718872, "answer": "[\"Lightweight compression with encryption based on Asymmetric Numeral Systems\", \"A tutorial on the range variant of asymmetric numeral systems\", \"ATC: an Advanced Tucker Compression library for multidimensional data\"]", "f1 socre": 0.3652173913043478}
{"id": "b7405d8e-13f0-4750-a25d-df3a67ffd39a", "pred": ["{{\"Reference\": [\"Benchmarking Large Language Models in Retrieval-Augmented Generation\", \"Retrieval-Augmented Generation in Question Answering: A Survey\"],\n\"Citation\": [\"Seven Failure Points When Engineering a Retrieval Augmented Generation System\"]}}\n\nThis paper presents the lessons learned and failure points from three case studies of implementing Retrieval-Augmented Generation (RAG) systems. It also discusses"], "TTFT": 10.79617714881897, "answer": "{\"Reference\": [\"Benchmarking Large Language Models in Retrieval-Augmented Generation\"], \"Citation\": [\"When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively\"]}", "f1 socre": 0.4186046511627907}
{"id": "150529cd-8767-477e-aa2b-28bae4970fdd", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System\" by Santos et al., \"Massively scalable stencil algorithm for the 3D wave equation on the Cerebras Wafer-Scale Engine\" by Jacquelin and Araya-Polo]\n\nThe reason for this citation chain is that the paper by Sant"], "TTFT": 15.363740921020508, "answer": "[\"Massively scalable stencil algorithm\", \"Disruptive Changes in Field Equation Modeling A Simple Interface for Wafer Scale Engines\", \"Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System\"]", "f1 socre": 0.39285714285714285}
{"id": "a8fa7aa1-2198-419b-a0de-6ba4d1c1f160", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Dance Any Beat: Blending Beats with Visuals in Dance Video Generation\" by Xuanchen Wang, Heng Wang, Weidong Cai, and Dongnan Liu],\n[\"InMoDeGAN: Interpretable Motion Decomposing Generative Adversarial Network for Video Generation\" by Yaohui Wang, Francois Bremond,"], "TTFT": 14.07155442237854, "answer": "[\"InMoDeGAN: Interpretable Motion Decomposition Generative Adversarial Network for Video Generation\", \"LATENT IMAGE ANIMATOR: <br> LEARNING TO ANIMATE IMAGES VIA LATENT SPACE NAVIGATION\", \"Dance Any Beat: Blending Beats with Visuals in Dance Video Generation\"]", "f1 socre": 0.43548387096774194}
{"id": "49680a79-a974-49b0-b7e3-85ff515bc4f2", "pred": ["{{\"Reference\": [\"Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding\"], \"Citation\": [\"Smurfs: Leveraging Multiple Proficiency Agents for Effective Tool Utilization in LLMs\" ]}}\n\nThe paper \"EasyTool: Enhancing LLM-based Agents with Concise Tool Instruction\" references the paper \"Don't Fine-Tune, Dec"], "TTFT": 10.98206639289856, "answer": "{\"Reference\": [\"Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding\"], \"Citation\": [\"Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning\"]}", "f1 socre": 0.6521739130434783}
{"id": "9f29f552-c817-4c2a-8910-b5e8ae7c014f", "pred": ["[\"Fine-Tuning LLaMA for Multi-Stage Text Retrieval\" by Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin, 2023, cited by \"Improving Text Embeddings with Large Language Models\" by Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, Furu Wei, and Jimmy Lin,"], "TTFT": 13.682081937789917, "answer": "[\"RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large Language Models\", \"Fine-Tuning LLaMA for Multi-Stage Text Retrieval\", \"Improving Text Embeddings with Large Language Models\"]", "f1 socre": 0.3225806451612903}
{"id": "30a2535f-7e73-46ad-a402-ee80ac6e4f92", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"Quantum Annealers Chain Strengths: A Simple Heuristic to Set Them All\" by Valentin Gilbert and St\u00e9phane Louise, 2024, arXiv: 2404.05443 [39]],\n[\"Benchmarking Quantum Annealers with Near-Optimal Minor Embeddings\" by Valentin Gilbert"], "TTFT": 16.3840811252594, "answer": "[\"Scaling Advantage in Approximate Optimization with Quantum Annealing\", \"4-clique Network Minor Embedding for Quantum Annealers\", \"Quantum Annealers Chain Strengths: A Simple Heuristic to Set Them All\", \"Benchmarking Quantum Annealers with Near-Optimal Minor-Embedded Instances\"]", "f1 socre": 0.40579710144927533}
{"id": "68e88dcc-6dec-4bb4-b51b-1fe8d6cd856d", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"LGDE: Local Graph-based Dictionary Expansion\" by Schindler et al., 2023]\n[\"Query2doc: Query Expansion with Large Language Models\" by Wang, Yang, and Wei, 2023]\n[\"From free text to clusters: A graph-based approach to text mining and information retrieval\" by Raghav"], "TTFT": 10.241957187652588, "answer": "[\"Query2doc: Query Expansion with Large Language Models\", \"Query Expansion by Prompting Large Language Models\", \"LGDE: Local Graph-based Dictionary Expansion\"]", "f1 socre": 0.32989690721649484}
{"id": "13a4a371-6339-4c9d-82cf-fc9ab2bb017d", "pred": ["{\n\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"],\n\"Citation\": []\n}\n\nThe paper \"OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models\" does not mention \"TinyLlama: An Open-Source Small Language Model\" in its text, so there is no citation relationship between the two papers. However, \"OpenMoE\" references \"Tiny"], "TTFT": 13.604034900665283, "answer": "{\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"], \"Citation\": [\"Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training\"]}", "f1 socre": 0.4047619047619047}
{"id": "c4a4e642-fa00-4377-a756-40f2f7ea7a74", "pred": ["{{\"Reference\": [\"A Survey of Hallucination in Large Foundation Models by Vipula Rawte, Amit Sheth, and Amitava Das\"], \"Citation\": [\"CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation by Quan Tu, Shilong Fan, Zihang Tian, Rui Yan\"]}}\n\nThis paper on CharacterEval references the survey paper on hallucination in large foundation models and"], "TTFT": 11.539960622787476, "answer": "{\"Reference\": [\"A Survey of Hallucination in \\\"Large\\\" Foundation Models\"], \"Citation\": [\"From Role-Play to Drama-Interaction: An LLM Solution\"]}", "f1 socre": 0.2745098039215686}
{"id": "835a4005-8c32-4083-b52d-4e014d1d35f5", "pred": ["Based on the provided instructions and papers, here is the longest citation chain:\n\n[\"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\" by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed, \"SUPERB: Speech processing Universal"], "TTFT": 11.292267322540283, "answer": "[\"CONTRASTIVE SEMI-SUPERVISED LEARNING FOR ASR\", \"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\", \"SUPERB: Speech processing Universal PERformance Benchmark\"]", "f1 socre": 0.38461538461538464}
{"id": "cd618a43-7176-432d-b21e-432e5c2ca4a7", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\" by Abien Fred M. Agarap, 2018, ICMLC 2018]\n\n[\"A Novel Approach Combining Recurrent Neural Network and"], "TTFT": 6.193939208984375, "answer": "[\"Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems\", \"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\"]", "f1 socre": 0.34374999999999994}
{"id": "e904afd3-5cb9-4bf7-bbee-9c8defac858c", "pred": ["Based on the provided instructions and papers, the longest citation chain can be constructed as follows:\n\n[\"On the performativity of SDG classifications in large bibliometric databases\" by Ottaviani and Stahlschmidt (2024), \"Language models are unsupervised multitask learners\" by Radford et al. (2019), \"DistilBERT: Distilling BERT: Six-Order Distillation for a Fast and Che"], "TTFT": 7.292623519897461, "answer": "[\"Contrastive Decoding: Open-ended Text Generation as Optimization\", \"An Empirical Study On Contrastive Search And Contrastive Decoding For Open-ended Text Generation\", \"On the performativity of SDG classifications in large bibliometric databases\"]", "f1 socre": 0.380952380952381}
{"id": "683f4cfd-1cfe-475d-927a-08a71d4f69e9", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\" by Niimi et al., 2024]\n\n[\"Churn Prediction with"], "TTFT": 11.674830198287964, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\", \"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\", \"An Efficient Multimodal Learning Framework to Comprehend Consumer Preferences Using BERT and Cross-Attention\"]", "f1 socre": 0.45312500000000006}
{"id": "0223faf0-f46f-459d-b1cd-45c4dab78fc2", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"A tutorial on the range variant of asymmetric numeral systems\" by James Townsend, \"Asymmetric numeral systems\" by Jarek Duda, \"Asymmetric numeral systems: entropy coding combining speed of Huffman coding with compression rate of arithmetic coding\" by Jarek Duda, Khalid Tahboub, Neeraj J. Gad"], "TTFT": 14.098109483718872, "answer": "[\"Lightweight compression with encryption based on Asymmetric Numeral Systems\", \"A tutorial on the range variant of asymmetric numeral systems\", \"ATC: an Advanced Tucker Compression library for multidimensional data\"]", "f1 socre": 0.3652173913043478}
{"id": "b7405d8e-13f0-4750-a25d-df3a67ffd39a", "pred": ["{{\"Reference\": [\"Benchmarking Large Language Models in Retrieval-Augmented Generation\", \"Retrieval-Augmented Generation in Question Answering: A Survey\"],\n\"Citation\": [\"Seven Failure Points When Engineering a Retrieval Augmented Generation System\"]}}\n\nThis paper presents the lessons learned and failure points from three case studies of implementing Retrieval-Augmented Generation (RAG) systems. It also discusses"], "TTFT": 10.79617714881897, "answer": "{\"Reference\": [\"Benchmarking Large Language Models in Retrieval-Augmented Generation\"], \"Citation\": [\"When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively\"]}", "f1 socre": 0.4186046511627907}
{"id": "150529cd-8767-477e-aa2b-28bae4970fdd", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System\" by Santos et al., \"Massively scalable stencil algorithm for the 3D wave equation on the Cerebras Wafer-Scale Engine\" by Jacquelin and Araya-Polo]\n\nThe reason for this citation chain is that the paper by Sant"], "TTFT": 15.363740921020508, "answer": "[\"Massively scalable stencil algorithm\", \"Disruptive Changes in Field Equation Modeling A Simple Interface for Wafer Scale Engines\", \"Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System\"]", "f1 socre": 0.39285714285714285}
{"id": "a8fa7aa1-2198-419b-a0de-6ba4d1c1f160", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Dance Any Beat: Blending Beats with Visuals in Dance Video Generation\" by Xuanchen Wang, Heng Wang, Weidong Cai, and Dongnan Liu],\n[\"InMoDeGAN: Interpretable Motion Decomposing Generative Adversarial Network for Video Generation\" by Yaohui Wang, Francois Bremond,"], "TTFT": 14.07155442237854, "answer": "[\"InMoDeGAN: Interpretable Motion Decomposition Generative Adversarial Network for Video Generation\", \"LATENT IMAGE ANIMATOR: <br> LEARNING TO ANIMATE IMAGES VIA LATENT SPACE NAVIGATION\", \"Dance Any Beat: Blending Beats with Visuals in Dance Video Generation\"]", "f1 socre": 0.43548387096774194}
{"id": "49680a79-a974-49b0-b7e3-85ff515bc4f2", "pred": ["{{\"Reference\": [\"Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding\"], \"Citation\": [\"Smurfs: Leveraging Multiple Proficiency Agents for Effective Tool Utilization in LLMs\" ]}}\n\nThe paper \"EasyTool: Enhancing LLM-based Agents with Concise Tool Instruction\" references the paper \"Don't Fine-Tune, Dec"], "TTFT": 10.98206639289856, "answer": "{\"Reference\": [\"Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding\"], \"Citation\": [\"Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning\"]}", "f1 socre": 0.6521739130434783}
{"id": "9f29f552-c817-4c2a-8910-b5e8ae7c014f", "pred": ["[\"Fine-Tuning LLaMA for Multi-Stage Text Retrieval\" by Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin, 2023, cited by \"Improving Text Embeddings with Large Language Models\" by Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, Furu Wei, and Jimmy Lin,"], "TTFT": 13.682081937789917, "answer": "[\"RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large Language Models\", \"Fine-Tuning LLaMA for Multi-Stage Text Retrieval\", \"Improving Text Embeddings with Large Language Models\"]", "f1 socre": 0.3225806451612903}
{"id": "30a2535f-7e73-46ad-a402-ee80ac6e4f92", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"Quantum Annealers Chain Strengths: A Simple Heuristic to Set Them All\" by Valentin Gilbert and St\u00e9phane Louise, 2024, arXiv: 2404.05443 [39]],\n[\"Benchmarking Quantum Annealers with Near-Optimal Minor Embeddings\" by Valentin Gilbert"], "TTFT": 16.3840811252594, "answer": "[\"Scaling Advantage in Approximate Optimization with Quantum Annealing\", \"4-clique Network Minor Embedding for Quantum Annealers\", \"Quantum Annealers Chain Strengths: A Simple Heuristic to Set Them All\", \"Benchmarking Quantum Annealers with Near-Optimal Minor-Embedded Instances\"]", "f1 socre": 0.40579710144927533}
{"id": "68e88dcc-6dec-4bb4-b51b-1fe8d6cd856d", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"LGDE: Local Graph-based Dictionary Expansion\" by Schindler et al., 2023]\n[\"Query2doc: Query Expansion with Large Language Models\" by Wang, Yang, and Wei, 2023]\n[\"From free text to clusters: A graph-based approach to text mining and information retrieval\" by Raghav"], "TTFT": 10.241957187652588, "answer": "[\"Query2doc: Query Expansion with Large Language Models\", \"Query Expansion by Prompting Large Language Models\", \"LGDE: Local Graph-based Dictionary Expansion\"]", "f1 socre": 0.32989690721649484}
{"id": "d7adf528-91a9-4978-a25c-29e61562a3be", "pred": ["{{\"Reference\": [\"The Pitfalls of Defining Hallucination\", \"Hallucination is Inevitable: An Innate Limitation of Large Language Models\"],\n\"Citation\": [\"Hallucination is Inevitable: An Innate Limitation of Large Language Models\", \"The Pitfalls of Defining Hallucination\"]}}\n\nIn this paper, the authors show that hallucination is inevitable for large language models (LLMs"], "TTFT": 11.103811502456665, "answer": "{\"Reference\": [\"The Pitfalls of Defining Hallucination\"], \"Citation\": [\"An ASSESSMENT OF MODEL-ON-MODEL DECEPTION\"]}", "f1 socre": 0.29333333333333333}
{"id": "13a4a371-6339-4c9d-82cf-fc9ab2bb017d", "pred": ["{\n\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"],\n\"Citation\": []\n}\n\nThe paper \"OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models\" does not mention \"TinyLlama: An Open-Source Small Language Model\" in its text, so there is no citation relationship between the two papers. However, \"OpenMoE\" references \"Tiny"], "TTFT": 13.604034900665283, "answer": "{\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"], \"Citation\": [\"Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training\"]}", "f1 socre": 0.4047619047619047}
{"id": "c4a4e642-fa00-4377-a756-40f2f7ea7a74", "pred": ["{{\"Reference\": [\"A Survey of Hallucination in Large Foundation Models by Vipula Rawte, Amit Sheth, and Amitava Das\"], \"Citation\": [\"CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation by Quan Tu, Shilong Fan, Zihang Tian, Rui Yan\"]}}\n\nThis paper on CharacterEval references the survey paper on hallucination in large foundation models and"], "TTFT": 11.539960622787476, "answer": "{\"Reference\": [\"A Survey of Hallucination in \\\"Large\\\" Foundation Models\"], \"Citation\": [\"From Role-Play to Drama-Interaction: An LLM Solution\"]}", "f1 socre": 0.2745098039215686}
{"id": "835a4005-8c32-4083-b52d-4e014d1d35f5", "pred": ["Based on the provided instructions and papers, here is the longest citation chain:\n\n[\"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\" by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed, \"SUPERB: Speech processing Universal"], "TTFT": 11.292267322540283, "answer": "[\"CONTRASTIVE SEMI-SUPERVISED LEARNING FOR ASR\", \"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\", \"SUPERB: Speech processing Universal PERformance Benchmark\"]", "f1 socre": 0.38461538461538464}
{"id": "cd618a43-7176-432d-b21e-432e5c2ca4a7", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\" by Abien Fred M. Agarap, 2018, ICMLC 2018]\n\n[\"A Novel Approach Combining Recurrent Neural Network and"], "TTFT": 6.193939208984375, "answer": "[\"Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems\", \"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\"]", "f1 socre": 0.34374999999999994}
{"id": "e904afd3-5cb9-4bf7-bbee-9c8defac858c", "pred": ["Based on the provided instructions and papers, the longest citation chain can be constructed as follows:\n\n[\"On the performativity of SDG classifications in large bibliometric databases\" by Ottaviani and Stahlschmidt (2024), \"Language models are unsupervised multitask learners\" by Radford et al. (2019), \"DistilBERT: Distilling BERT: Six-Order Distillation for a Fast and Che"], "TTFT": 7.292623519897461, "answer": "[\"Contrastive Decoding: Open-ended Text Generation as Optimization\", \"An Empirical Study On Contrastive Search And Contrastive Decoding For Open-ended Text Generation\", \"On the performativity of SDG classifications in large bibliometric databases\"]", "f1 socre": 0.380952380952381}
{"id": "683f4cfd-1cfe-475d-927a-08a71d4f69e9", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\" by Niimi et al., 2024]\n\n[\"Churn Prediction with"], "TTFT": 11.674830198287964, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\", \"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\", \"An Efficient Multimodal Learning Framework to Comprehend Consumer Preferences Using BERT and Cross-Attention\"]", "f1 socre": 0.45312500000000006}
{"id": "0223faf0-f46f-459d-b1cd-45c4dab78fc2", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"A tutorial on the range variant of asymmetric numeral systems\" by James Townsend, \"Asymmetric numeral systems\" by Jarek Duda, \"Asymmetric numeral systems: entropy coding combining speed of Huffman coding with compression rate of arithmetic coding\" by Jarek Duda, Khalid Tahboub, Neeraj J. Gad"], "TTFT": 14.098109483718872, "answer": "[\"Lightweight compression with encryption based on Asymmetric Numeral Systems\", \"A tutorial on the range variant of asymmetric numeral systems\", \"ATC: an Advanced Tucker Compression library for multidimensional data\"]", "f1 socre": 0.3652173913043478}
{"id": "b7405d8e-13f0-4750-a25d-df3a67ffd39a", "pred": ["{{\"Reference\": [\"Benchmarking Large Language Models in Retrieval-Augmented Generation\", \"Retrieval-Augmented Generation in Question Answering: A Survey\"],\n\"Citation\": [\"Seven Failure Points When Engineering a Retrieval Augmented Generation System\"]}}\n\nThis paper presents the lessons learned and failure points from three case studies of implementing Retrieval-Augmented Generation (RAG) systems. It also discusses"], "TTFT": 10.79617714881897, "answer": "{\"Reference\": [\"Benchmarking Large Language Models in Retrieval-Augmented Generation\"], \"Citation\": [\"When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively\"]}", "f1 socre": 0.4186046511627907}
{"id": "150529cd-8767-477e-aa2b-28bae4970fdd", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System\" by Santos et al., \"Massively scalable stencil algorithm for the 3D wave equation on the Cerebras Wafer-Scale Engine\" by Jacquelin and Araya-Polo]\n\nThe reason for this citation chain is that the paper by Sant"], "TTFT": 15.363740921020508, "answer": "[\"Massively scalable stencil algorithm\", \"Disruptive Changes in Field Equation Modeling A Simple Interface for Wafer Scale Engines\", \"Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System\"]", "f1 socre": 0.39285714285714285}
{"id": "a8fa7aa1-2198-419b-a0de-6ba4d1c1f160", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Dance Any Beat: Blending Beats with Visuals in Dance Video Generation\" by Xuanchen Wang, Heng Wang, Weidong Cai, and Dongnan Liu],\n[\"InMoDeGAN: Interpretable Motion Decomposing Generative Adversarial Network for Video Generation\" by Yaohui Wang, Francois Bremond,"], "TTFT": 14.07155442237854, "answer": "[\"InMoDeGAN: Interpretable Motion Decomposition Generative Adversarial Network for Video Generation\", \"LATENT IMAGE ANIMATOR: <br> LEARNING TO ANIMATE IMAGES VIA LATENT SPACE NAVIGATION\", \"Dance Any Beat: Blending Beats with Visuals in Dance Video Generation\"]", "f1 socre": 0.43548387096774194}
{"id": "49680a79-a974-49b0-b7e3-85ff515bc4f2", "pred": ["{{\"Reference\": [\"Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding\"], \"Citation\": [\"Smurfs: Leveraging Multiple Proficiency Agents for Effective Tool Utilization in LLMs\" ]}}\n\nThe paper \"EasyTool: Enhancing LLM-based Agents with Concise Tool Instruction\" references the paper \"Don't Fine-Tune, Dec"], "TTFT": 10.98206639289856, "answer": "{\"Reference\": [\"Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding\"], \"Citation\": [\"Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning\"]}", "f1 socre": 0.6521739130434783}
{"id": "9f29f552-c817-4c2a-8910-b5e8ae7c014f", "pred": ["[\"Fine-Tuning LLaMA for Multi-Stage Text Retrieval\" by Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin, 2023, cited by \"Improving Text Embeddings with Large Language Models\" by Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, Furu Wei, and Jimmy Lin,"], "TTFT": 13.682081937789917, "answer": "[\"RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large Language Models\", \"Fine-Tuning LLaMA for Multi-Stage Text Retrieval\", \"Improving Text Embeddings with Large Language Models\"]", "f1 socre": 0.3225806451612903}
{"id": "30a2535f-7e73-46ad-a402-ee80ac6e4f92", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"Quantum Annealers Chain Strengths: A Simple Heuristic to Set Them All\" by Valentin Gilbert and St\u00e9phane Louise, 2024, arXiv: 2404.05443 [39]],\n[\"Benchmarking Quantum Annealers with Near-Optimal Minor Embeddings\" by Valentin Gilbert"], "TTFT": 16.3840811252594, "answer": "[\"Scaling Advantage in Approximate Optimization with Quantum Annealing\", \"4-clique Network Minor Embedding for Quantum Annealers\", \"Quantum Annealers Chain Strengths: A Simple Heuristic to Set Them All\", \"Benchmarking Quantum Annealers with Near-Optimal Minor-Embedded Instances\"]", "f1 socre": 0.40579710144927533}
{"id": "68e88dcc-6dec-4bb4-b51b-1fe8d6cd856d", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"LGDE: Local Graph-based Dictionary Expansion\" by Schindler et al., 2023]\n[\"Query2doc: Query Expansion with Large Language Models\" by Wang, Yang, and Wei, 2023]\n[\"From free text to clusters: A graph-based approach to text mining and information retrieval\" by Raghav"], "TTFT": 10.241957187652588, "answer": "[\"Query2doc: Query Expansion with Large Language Models\", \"Query Expansion by Prompting Large Language Models\", \"LGDE: Local Graph-based Dictionary Expansion\"]", "f1 socre": 0.32989690721649484}
{"id": "d7adf528-91a9-4978-a25c-29e61562a3be", "pred": ["{{\"Reference\": [\"The Pitfalls of Defining Hallucination\", \"Hallucination is Inevitable: An Innate Limitation of Large Language Models\"],\n\"Citation\": [\"Hallucination is Inevitable: An Innate Limitation of Large Language Models\", \"The Pitfalls of Defining Hallucination\"]}}\n\nIn this paper, the authors show that hallucination is inevitable for large language models (LLMs"], "TTFT": 11.103811502456665, "answer": "{\"Reference\": [\"The Pitfalls of Defining Hallucination\"], \"Citation\": [\"An ASSESSMENT OF MODEL-ON-MODEL DECEPTION\"]}", "f1 socre": 0.29333333333333333}
{"id": "949f3450-9b30-4556-be5f-ca8afe9e072f", "pred": ["{\n\"Title\": \"LLamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource Language\",\n\"Authors\": [\"Cagri Toraman\"],\n\"Reference\": [\n{\"Title\": \"BELLE: A Large-Scale Chinese Instruction Tuning Dataset for Pretrained Language Models\"},\n{\"Title\": \"Bactrain-X: A Multilingual Instruction Tuning Dataset for Low-Resource L"], "TTFT": 12.38475251197815, "answer": "{\"Reference\": [], \"Citation\": [\"LlamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource Language\", \"InstructionCP: A fast approach to transfer Large Language Models into target language\"]}", "f1 socre": 0.43010752688172044}
{"id": "13a4a371-6339-4c9d-82cf-fc9ab2bb017d", "pred": ["{\n\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"],\n\"Citation\": []\n}\n\nThe paper \"OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models\" does not mention \"TinyLlama: An Open-Source Small Language Model\" in its text, so there is no citation relationship between the two papers. However, \"OpenMoE\" references \"Tiny"], "TTFT": 13.604034900665283, "answer": "{\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"], \"Citation\": [\"Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training\"]}", "f1 socre": 0.4047619047619047}
{"id": "c4a4e642-fa00-4377-a756-40f2f7ea7a74", "pred": ["{{\"Reference\": [\"A Survey of Hallucination in Large Foundation Models by Vipula Rawte, Amit Sheth, and Amitava Das\"], \"Citation\": [\"CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation by Quan Tu, Shilong Fan, Zihang Tian, Rui Yan\"]}}\n\nThis paper on CharacterEval references the survey paper on hallucination in large foundation models and"], "TTFT": 11.539960622787476, "answer": "{\"Reference\": [\"A Survey of Hallucination in \\\"Large\\\" Foundation Models\"], \"Citation\": [\"From Role-Play to Drama-Interaction: An LLM Solution\"]}", "f1 socre": 0.2745098039215686}
{"id": "835a4005-8c32-4083-b52d-4e014d1d35f5", "pred": ["Based on the provided instructions and papers, here is the longest citation chain:\n\n[\"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\" by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed, \"SUPERB: Speech processing Universal"], "TTFT": 11.292267322540283, "answer": "[\"CONTRASTIVE SEMI-SUPERVISED LEARNING FOR ASR\", \"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\", \"SUPERB: Speech processing Universal PERformance Benchmark\"]", "f1 socre": 0.38461538461538464}
{"id": "cd618a43-7176-432d-b21e-432e5c2ca4a7", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\" by Abien Fred M. Agarap, 2018, ICMLC 2018]\n\n[\"A Novel Approach Combining Recurrent Neural Network and"], "TTFT": 6.193939208984375, "answer": "[\"Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems\", \"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\"]", "f1 socre": 0.34374999999999994}
{"id": "e904afd3-5cb9-4bf7-bbee-9c8defac858c", "pred": ["Based on the provided instructions and papers, the longest citation chain can be constructed as follows:\n\n[\"On the performativity of SDG classifications in large bibliometric databases\" by Ottaviani and Stahlschmidt (2024), \"Language models are unsupervised multitask learners\" by Radford et al. (2019), \"DistilBERT: Distilling BERT: Six-Order Distillation for a Fast and Che"], "TTFT": 7.292623519897461, "answer": "[\"Contrastive Decoding: Open-ended Text Generation as Optimization\", \"An Empirical Study On Contrastive Search And Contrastive Decoding For Open-ended Text Generation\", \"On the performativity of SDG classifications in large bibliometric databases\"]", "f1 socre": 0.380952380952381}
{"id": "683f4cfd-1cfe-475d-927a-08a71d4f69e9", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\" by Niimi et al., 2024]\n\n[\"Churn Prediction with"], "TTFT": 11.674830198287964, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\", \"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\", \"An Efficient Multimodal Learning Framework to Comprehend Consumer Preferences Using BERT and Cross-Attention\"]", "f1 socre": 0.45312500000000006}
{"id": "0223faf0-f46f-459d-b1cd-45c4dab78fc2", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"A tutorial on the range variant of asymmetric numeral systems\" by James Townsend, \"Asymmetric numeral systems\" by Jarek Duda, \"Asymmetric numeral systems: entropy coding combining speed of Huffman coding with compression rate of arithmetic coding\" by Jarek Duda, Khalid Tahboub, Neeraj J. Gad"], "TTFT": 14.098109483718872, "answer": "[\"Lightweight compression with encryption based on Asymmetric Numeral Systems\", \"A tutorial on the range variant of asymmetric numeral systems\", \"ATC: an Advanced Tucker Compression library for multidimensional data\"]", "f1 socre": 0.3652173913043478}
{"id": "b7405d8e-13f0-4750-a25d-df3a67ffd39a", "pred": ["{{\"Reference\": [\"Benchmarking Large Language Models in Retrieval-Augmented Generation\", \"Retrieval-Augmented Generation in Question Answering: A Survey\"],\n\"Citation\": [\"Seven Failure Points When Engineering a Retrieval Augmented Generation System\"]}}\n\nThis paper presents the lessons learned and failure points from three case studies of implementing Retrieval-Augmented Generation (RAG) systems. It also discusses"], "TTFT": 10.79617714881897, "answer": "{\"Reference\": [\"Benchmarking Large Language Models in Retrieval-Augmented Generation\"], \"Citation\": [\"When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively\"]}", "f1 socre": 0.4186046511627907}
{"id": "150529cd-8767-477e-aa2b-28bae4970fdd", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System\" by Santos et al., \"Massively scalable stencil algorithm for the 3D wave equation on the Cerebras Wafer-Scale Engine\" by Jacquelin and Araya-Polo]\n\nThe reason for this citation chain is that the paper by Sant"], "TTFT": 15.363740921020508, "answer": "[\"Massively scalable stencil algorithm\", \"Disruptive Changes in Field Equation Modeling A Simple Interface for Wafer Scale Engines\", \"Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System\"]", "f1 socre": 0.39285714285714285}
{"id": "a8fa7aa1-2198-419b-a0de-6ba4d1c1f160", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Dance Any Beat: Blending Beats with Visuals in Dance Video Generation\" by Xuanchen Wang, Heng Wang, Weidong Cai, and Dongnan Liu],\n[\"InMoDeGAN: Interpretable Motion Decomposing Generative Adversarial Network for Video Generation\" by Yaohui Wang, Francois Bremond,"], "TTFT": 14.07155442237854, "answer": "[\"InMoDeGAN: Interpretable Motion Decomposition Generative Adversarial Network for Video Generation\", \"LATENT IMAGE ANIMATOR: <br> LEARNING TO ANIMATE IMAGES VIA LATENT SPACE NAVIGATION\", \"Dance Any Beat: Blending Beats with Visuals in Dance Video Generation\"]", "f1 socre": 0.43548387096774194}
{"id": "49680a79-a974-49b0-b7e3-85ff515bc4f2", "pred": ["{{\"Reference\": [\"Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding\"], \"Citation\": [\"Smurfs: Leveraging Multiple Proficiency Agents for Effective Tool Utilization in LLMs\" ]}}\n\nThe paper \"EasyTool: Enhancing LLM-based Agents with Concise Tool Instruction\" references the paper \"Don't Fine-Tune, Dec"], "TTFT": 10.98206639289856, "answer": "{\"Reference\": [\"Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding\"], \"Citation\": [\"Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning\"]}", "f1 socre": 0.6521739130434783}
{"id": "9f29f552-c817-4c2a-8910-b5e8ae7c014f", "pred": ["[\"Fine-Tuning LLaMA for Multi-Stage Text Retrieval\" by Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin, 2023, cited by \"Improving Text Embeddings with Large Language Models\" by Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, Furu Wei, and Jimmy Lin,"], "TTFT": 13.682081937789917, "answer": "[\"RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large Language Models\", \"Fine-Tuning LLaMA for Multi-Stage Text Retrieval\", \"Improving Text Embeddings with Large Language Models\"]", "f1 socre": 0.3225806451612903}
{"id": "30a2535f-7e73-46ad-a402-ee80ac6e4f92", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"Quantum Annealers Chain Strengths: A Simple Heuristic to Set Them All\" by Valentin Gilbert and St\u00e9phane Louise, 2024, arXiv: 2404.05443 [39]],\n[\"Benchmarking Quantum Annealers with Near-Optimal Minor Embeddings\" by Valentin Gilbert"], "TTFT": 16.3840811252594, "answer": "[\"Scaling Advantage in Approximate Optimization with Quantum Annealing\", \"4-clique Network Minor Embedding for Quantum Annealers\", \"Quantum Annealers Chain Strengths: A Simple Heuristic to Set Them All\", \"Benchmarking Quantum Annealers with Near-Optimal Minor-Embedded Instances\"]", "f1 socre": 0.40579710144927533}
{"id": "68e88dcc-6dec-4bb4-b51b-1fe8d6cd856d", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"LGDE: Local Graph-based Dictionary Expansion\" by Schindler et al., 2023]\n[\"Query2doc: Query Expansion with Large Language Models\" by Wang, Yang, and Wei, 2023]\n[\"From free text to clusters: A graph-based approach to text mining and information retrieval\" by Raghav"], "TTFT": 10.241957187652588, "answer": "[\"Query2doc: Query Expansion with Large Language Models\", \"Query Expansion by Prompting Large Language Models\", \"LGDE: Local Graph-based Dictionary Expansion\"]", "f1 socre": 0.32989690721649484}
{"id": "d7adf528-91a9-4978-a25c-29e61562a3be", "pred": ["{{\"Reference\": [\"The Pitfalls of Defining Hallucination\", \"Hallucination is Inevitable: An Innate Limitation of Large Language Models\"],\n\"Citation\": [\"Hallucination is Inevitable: An Innate Limitation of Large Language Models\", \"The Pitfalls of Defining Hallucination\"]}}\n\nIn this paper, the authors show that hallucination is inevitable for large language models (LLMs"], "TTFT": 11.103811502456665, "answer": "{\"Reference\": [\"The Pitfalls of Defining Hallucination\"], \"Citation\": [\"An ASSESSMENT OF MODEL-ON-MODEL DECEPTION\"]}", "f1 socre": 0.29333333333333333}
{"id": "949f3450-9b30-4556-be5f-ca8afe9e072f", "pred": ["{\n\"Title\": \"LLamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource Language\",\n\"Authors\": [\"Cagri Toraman\"],\n\"Reference\": [\n{\"Title\": \"BELLE: A Large-Scale Chinese Instruction Tuning Dataset for Pretrained Language Models\"},\n{\"Title\": \"Bactrain-X: A Multilingual Instruction Tuning Dataset for Low-Resource L"], "TTFT": 12.38475251197815, "answer": "{\"Reference\": [], \"Citation\": [\"LlamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource Language\", \"InstructionCP: A fast approach to transfer Large Language Models into target language\"]}", "f1 socre": 0.43010752688172044}
{"id": "8878fb61-6996-4984-9774-5ce9fb506d37", "pred": ["{\n\"Title\": \"Extreme Compression of Large Language Models via Additive Quantization\",\n\"Authors\": [\"Vage Egiazarian\", \"* 12\", \"Andrei Panferov\", \"* 12\", \"Denis Kuznedelev\", \"* 23\", \"Elias Frantar\", \"* 4\", \"Artem Babenko\", \"* 2\"],\n\"Reference\": [\n{\"Title\": \"Mistral"], "TTFT": 11.69446086883545, "answer": "{\"Reference\": [\"Mistral 7B\"], \"Citation\": [\"Sparse Expansion and Neuronal Disentanglement\"]}", "f1 socre": 0.10526315789473684}
{"id": "13a4a371-6339-4c9d-82cf-fc9ab2bb017d", "pred": ["{\n\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"],\n\"Citation\": []\n}\n\nThe paper \"OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models\" does not mention \"TinyLlama: An Open-Source Small Language Model\" in its text, so there is no citation relationship between the two papers. However, \"OpenMoE\" references \"Tiny"], "TTFT": 13.604034900665283, "answer": "{\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"], \"Citation\": [\"Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training\"]}", "f1 socre": 0.4047619047619047}
{"id": "c4a4e642-fa00-4377-a756-40f2f7ea7a74", "pred": ["{{\"Reference\": [\"A Survey of Hallucination in Large Foundation Models by Vipula Rawte, Amit Sheth, and Amitava Das\"], \"Citation\": [\"CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation by Quan Tu, Shilong Fan, Zihang Tian, Rui Yan\"]}}\n\nThis paper on CharacterEval references the survey paper on hallucination in large foundation models and"], "TTFT": 11.539960622787476, "answer": "{\"Reference\": [\"A Survey of Hallucination in \\\"Large\\\" Foundation Models\"], \"Citation\": [\"From Role-Play to Drama-Interaction: An LLM Solution\"]}", "f1 socre": 0.2745098039215686}
{"id": "835a4005-8c32-4083-b52d-4e014d1d35f5", "pred": ["Based on the provided instructions and papers, here is the longest citation chain:\n\n[\"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\" by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed, \"SUPERB: Speech processing Universal"], "TTFT": 11.292267322540283, "answer": "[\"CONTRASTIVE SEMI-SUPERVISED LEARNING FOR ASR\", \"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\", \"SUPERB: Speech processing Universal PERformance Benchmark\"]", "f1 socre": 0.38461538461538464}
{"id": "cd618a43-7176-432d-b21e-432e5c2ca4a7", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\" by Abien Fred M. Agarap, 2018, ICMLC 2018]\n\n[\"A Novel Approach Combining Recurrent Neural Network and"], "TTFT": 6.193939208984375, "answer": "[\"Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems\", \"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\"]", "f1 socre": 0.34374999999999994}
{"id": "e904afd3-5cb9-4bf7-bbee-9c8defac858c", "pred": ["Based on the provided instructions and papers, the longest citation chain can be constructed as follows:\n\n[\"On the performativity of SDG classifications in large bibliometric databases\" by Ottaviani and Stahlschmidt (2024), \"Language models are unsupervised multitask learners\" by Radford et al. (2019), \"DistilBERT: Distilling BERT: Six-Order Distillation for a Fast and Che"], "TTFT": 7.292623519897461, "answer": "[\"Contrastive Decoding: Open-ended Text Generation as Optimization\", \"An Empirical Study On Contrastive Search And Contrastive Decoding For Open-ended Text Generation\", \"On the performativity of SDG classifications in large bibliometric databases\"]", "f1 socre": 0.380952380952381}
{"id": "683f4cfd-1cfe-475d-927a-08a71d4f69e9", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\" by Niimi et al., 2024]\n\n[\"Churn Prediction with"], "TTFT": 11.674830198287964, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\", \"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\", \"An Efficient Multimodal Learning Framework to Comprehend Consumer Preferences Using BERT and Cross-Attention\"]", "f1 socre": 0.45312500000000006}
{"id": "0223faf0-f46f-459d-b1cd-45c4dab78fc2", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"A tutorial on the range variant of asymmetric numeral systems\" by James Townsend, \"Asymmetric numeral systems\" by Jarek Duda, \"Asymmetric numeral systems: entropy coding combining speed of Huffman coding with compression rate of arithmetic coding\" by Jarek Duda, Khalid Tahboub, Neeraj J. Gad"], "TTFT": 14.098109483718872, "answer": "[\"Lightweight compression with encryption based on Asymmetric Numeral Systems\", \"A tutorial on the range variant of asymmetric numeral systems\", \"ATC: an Advanced Tucker Compression library for multidimensional data\"]", "f1 socre": 0.3652173913043478}
{"id": "b7405d8e-13f0-4750-a25d-df3a67ffd39a", "pred": ["{{\"Reference\": [\"Benchmarking Large Language Models in Retrieval-Augmented Generation\", \"Retrieval-Augmented Generation in Question Answering: A Survey\"],\n\"Citation\": [\"Seven Failure Points When Engineering a Retrieval Augmented Generation System\"]}}\n\nThis paper presents the lessons learned and failure points from three case studies of implementing Retrieval-Augmented Generation (RAG) systems. It also discusses"], "TTFT": 10.79617714881897, "answer": "{\"Reference\": [\"Benchmarking Large Language Models in Retrieval-Augmented Generation\"], \"Citation\": [\"When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively\"]}", "f1 socre": 0.4186046511627907}
{"id": "150529cd-8767-477e-aa2b-28bae4970fdd", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System\" by Santos et al., \"Massively scalable stencil algorithm for the 3D wave equation on the Cerebras Wafer-Scale Engine\" by Jacquelin and Araya-Polo]\n\nThe reason for this citation chain is that the paper by Sant"], "TTFT": 15.363740921020508, "answer": "[\"Massively scalable stencil algorithm\", \"Disruptive Changes in Field Equation Modeling A Simple Interface for Wafer Scale Engines\", \"Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System\"]", "f1 socre": 0.39285714285714285}
{"id": "a8fa7aa1-2198-419b-a0de-6ba4d1c1f160", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Dance Any Beat: Blending Beats with Visuals in Dance Video Generation\" by Xuanchen Wang, Heng Wang, Weidong Cai, and Dongnan Liu],\n[\"InMoDeGAN: Interpretable Motion Decomposing Generative Adversarial Network for Video Generation\" by Yaohui Wang, Francois Bremond,"], "TTFT": 14.07155442237854, "answer": "[\"InMoDeGAN: Interpretable Motion Decomposition Generative Adversarial Network for Video Generation\", \"LATENT IMAGE ANIMATOR: <br> LEARNING TO ANIMATE IMAGES VIA LATENT SPACE NAVIGATION\", \"Dance Any Beat: Blending Beats with Visuals in Dance Video Generation\"]", "f1 socre": 0.43548387096774194}
{"id": "49680a79-a974-49b0-b7e3-85ff515bc4f2", "pred": ["{{\"Reference\": [\"Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding\"], \"Citation\": [\"Smurfs: Leveraging Multiple Proficiency Agents for Effective Tool Utilization in LLMs\" ]}}\n\nThe paper \"EasyTool: Enhancing LLM-based Agents with Concise Tool Instruction\" references the paper \"Don't Fine-Tune, Dec"], "TTFT": 10.98206639289856, "answer": "{\"Reference\": [\"Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding\"], \"Citation\": [\"Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning\"]}", "f1 socre": 0.6521739130434783}
{"id": "9f29f552-c817-4c2a-8910-b5e8ae7c014f", "pred": ["[\"Fine-Tuning LLaMA for Multi-Stage Text Retrieval\" by Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin, 2023, cited by \"Improving Text Embeddings with Large Language Models\" by Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, Furu Wei, and Jimmy Lin,"], "TTFT": 13.682081937789917, "answer": "[\"RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large Language Models\", \"Fine-Tuning LLaMA for Multi-Stage Text Retrieval\", \"Improving Text Embeddings with Large Language Models\"]", "f1 socre": 0.3225806451612903}
{"id": "30a2535f-7e73-46ad-a402-ee80ac6e4f92", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"Quantum Annealers Chain Strengths: A Simple Heuristic to Set Them All\" by Valentin Gilbert and St\u00e9phane Louise, 2024, arXiv: 2404.05443 [39]],\n[\"Benchmarking Quantum Annealers with Near-Optimal Minor Embeddings\" by Valentin Gilbert"], "TTFT": 16.3840811252594, "answer": "[\"Scaling Advantage in Approximate Optimization with Quantum Annealing\", \"4-clique Network Minor Embedding for Quantum Annealers\", \"Quantum Annealers Chain Strengths: A Simple Heuristic to Set Them All\", \"Benchmarking Quantum Annealers with Near-Optimal Minor-Embedded Instances\"]", "f1 socre": 0.40579710144927533}
{"id": "68e88dcc-6dec-4bb4-b51b-1fe8d6cd856d", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"LGDE: Local Graph-based Dictionary Expansion\" by Schindler et al., 2023]\n[\"Query2doc: Query Expansion with Large Language Models\" by Wang, Yang, and Wei, 2023]\n[\"From free text to clusters: A graph-based approach to text mining and information retrieval\" by Raghav"], "TTFT": 10.241957187652588, "answer": "[\"Query2doc: Query Expansion with Large Language Models\", \"Query Expansion by Prompting Large Language Models\", \"LGDE: Local Graph-based Dictionary Expansion\"]", "f1 socre": 0.32989690721649484}
{"id": "d7adf528-91a9-4978-a25c-29e61562a3be", "pred": ["{{\"Reference\": [\"The Pitfalls of Defining Hallucination\", \"Hallucination is Inevitable: An Innate Limitation of Large Language Models\"],\n\"Citation\": [\"Hallucination is Inevitable: An Innate Limitation of Large Language Models\", \"The Pitfalls of Defining Hallucination\"]}}\n\nIn this paper, the authors show that hallucination is inevitable for large language models (LLMs"], "TTFT": 11.103811502456665, "answer": "{\"Reference\": [\"The Pitfalls of Defining Hallucination\"], \"Citation\": [\"An ASSESSMENT OF MODEL-ON-MODEL DECEPTION\"]}", "f1 socre": 0.29333333333333333}
{"id": "949f3450-9b30-4556-be5f-ca8afe9e072f", "pred": ["{\n\"Title\": \"LLamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource Language\",\n\"Authors\": [\"Cagri Toraman\"],\n\"Reference\": [\n{\"Title\": \"BELLE: A Large-Scale Chinese Instruction Tuning Dataset for Pretrained Language Models\"},\n{\"Title\": \"Bactrain-X: A Multilingual Instruction Tuning Dataset for Low-Resource L"], "TTFT": 12.38475251197815, "answer": "{\"Reference\": [], \"Citation\": [\"LlamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource Language\", \"InstructionCP: A fast approach to transfer Large Language Models into target language\"]}", "f1 socre": 0.43010752688172044}
{"id": "8878fb61-6996-4984-9774-5ce9fb506d37", "pred": ["{\n\"Title\": \"Extreme Compression of Large Language Models via Additive Quantization\",\n\"Authors\": [\"Vage Egiazarian\", \"* 12\", \"Andrei Panferov\", \"* 12\", \"Denis Kuznedelev\", \"* 23\", \"Elias Frantar\", \"* 4\", \"Artem Babenko\", \"* 2\"],\n\"Reference\": [\n{\"Title\": \"Mistral"], "TTFT": 11.69446086883545, "answer": "{\"Reference\": [\"Mistral 7B\"], \"Citation\": [\"Sparse Expansion and Neuronal Disentanglement\"]}", "f1 socre": 0.10526315789473684}
{"id": "17539923-dd06-4712-becd-68fb8d68614d", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"Context-Aware Learning to Rank with Self-Attention\" by Przemys\u0142aw Pobrotyn, Tomasz Bartczak, Miko\u0142aj Synowiec, Rados\u0142aw Bia\u0142obrzeski, and Jaros\u0142aw Bojar]\n\ncites\n\n[\"Learning a ranking model with delta features to study the effect of neighborhood on item preference in e"], "TTFT": 6.013138055801392, "answer": "[\"Influence of Neighborhood on the Preference of an Item in eCommerce Search\", \"Context-Aware Learning to Rank with Self-Attention\"]", "f1 socre": 0.31249999999999994}
{"id": "13a4a371-6339-4c9d-82cf-fc9ab2bb017d", "pred": ["{\n\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"],\n\"Citation\": []\n}\n\nThe paper \"OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models\" does not mention \"TinyLlama: An Open-Source Small Language Model\" in its text, so there is no citation relationship between the two papers. However, \"OpenMoE\" references \"Tiny"], "TTFT": 13.604034900665283, "answer": "{\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"], \"Citation\": [\"Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training\"]}", "f1 socre": 0.4047619047619047}
{"id": "c4a4e642-fa00-4377-a756-40f2f7ea7a74", "pred": ["{{\"Reference\": [\"A Survey of Hallucination in Large Foundation Models by Vipula Rawte, Amit Sheth, and Amitava Das\"], \"Citation\": [\"CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation by Quan Tu, Shilong Fan, Zihang Tian, Rui Yan\"]}}\n\nThis paper on CharacterEval references the survey paper on hallucination in large foundation models and"], "TTFT": 11.539960622787476, "answer": "{\"Reference\": [\"A Survey of Hallucination in \\\"Large\\\" Foundation Models\"], \"Citation\": [\"From Role-Play to Drama-Interaction: An LLM Solution\"]}", "f1 socre": 0.2745098039215686}
{"id": "835a4005-8c32-4083-b52d-4e014d1d35f5", "pred": ["Based on the provided instructions and papers, here is the longest citation chain:\n\n[\"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\" by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed, \"SUPERB: Speech processing Universal"], "TTFT": 11.292267322540283, "answer": "[\"CONTRASTIVE SEMI-SUPERVISED LEARNING FOR ASR\", \"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\", \"SUPERB: Speech processing Universal PERformance Benchmark\"]", "f1 socre": 0.38461538461538464}
{"id": "cd618a43-7176-432d-b21e-432e5c2ca4a7", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\" by Abien Fred M. Agarap, 2018, ICMLC 2018]\n\n[\"A Novel Approach Combining Recurrent Neural Network and"], "TTFT": 6.193939208984375, "answer": "[\"Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems\", \"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\"]", "f1 socre": 0.34374999999999994}
{"id": "e904afd3-5cb9-4bf7-bbee-9c8defac858c", "pred": ["Based on the provided instructions and papers, the longest citation chain can be constructed as follows:\n\n[\"On the performativity of SDG classifications in large bibliometric databases\" by Ottaviani and Stahlschmidt (2024), \"Language models are unsupervised multitask learners\" by Radford et al. (2019), \"DistilBERT: Distilling BERT: Six-Order Distillation for a Fast and Che"], "TTFT": 7.292623519897461, "answer": "[\"Contrastive Decoding: Open-ended Text Generation as Optimization\", \"An Empirical Study On Contrastive Search And Contrastive Decoding For Open-ended Text Generation\", \"On the performativity of SDG classifications in large bibliometric databases\"]", "f1 socre": 0.380952380952381}
{"id": "683f4cfd-1cfe-475d-927a-08a71d4f69e9", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\" by Niimi et al., 2024]\n\n[\"Churn Prediction with"], "TTFT": 11.674830198287964, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\", \"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\", \"An Efficient Multimodal Learning Framework to Comprehend Consumer Preferences Using BERT and Cross-Attention\"]", "f1 socre": 0.45312500000000006}
{"id": "0223faf0-f46f-459d-b1cd-45c4dab78fc2", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"A tutorial on the range variant of asymmetric numeral systems\" by James Townsend, \"Asymmetric numeral systems\" by Jarek Duda, \"Asymmetric numeral systems: entropy coding combining speed of Huffman coding with compression rate of arithmetic coding\" by Jarek Duda, Khalid Tahboub, Neeraj J. Gad"], "TTFT": 14.098109483718872, "answer": "[\"Lightweight compression with encryption based on Asymmetric Numeral Systems\", \"A tutorial on the range variant of asymmetric numeral systems\", \"ATC: an Advanced Tucker Compression library for multidimensional data\"]", "f1 socre": 0.3652173913043478}
{"id": "b7405d8e-13f0-4750-a25d-df3a67ffd39a", "pred": ["{{\"Reference\": [\"Benchmarking Large Language Models in Retrieval-Augmented Generation\", \"Retrieval-Augmented Generation in Question Answering: A Survey\"],\n\"Citation\": [\"Seven Failure Points When Engineering a Retrieval Augmented Generation System\"]}}\n\nThis paper presents the lessons learned and failure points from three case studies of implementing Retrieval-Augmented Generation (RAG) systems. It also discusses"], "TTFT": 10.79617714881897, "answer": "{\"Reference\": [\"Benchmarking Large Language Models in Retrieval-Augmented Generation\"], \"Citation\": [\"When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively\"]}", "f1 socre": 0.4186046511627907}
{"id": "150529cd-8767-477e-aa2b-28bae4970fdd", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System\" by Santos et al., \"Massively scalable stencil algorithm for the 3D wave equation on the Cerebras Wafer-Scale Engine\" by Jacquelin and Araya-Polo]\n\nThe reason for this citation chain is that the paper by Sant"], "TTFT": 15.363740921020508, "answer": "[\"Massively scalable stencil algorithm\", \"Disruptive Changes in Field Equation Modeling A Simple Interface for Wafer Scale Engines\", \"Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System\"]", "f1 socre": 0.39285714285714285}
{"id": "a8fa7aa1-2198-419b-a0de-6ba4d1c1f160", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Dance Any Beat: Blending Beats with Visuals in Dance Video Generation\" by Xuanchen Wang, Heng Wang, Weidong Cai, and Dongnan Liu],\n[\"InMoDeGAN: Interpretable Motion Decomposing Generative Adversarial Network for Video Generation\" by Yaohui Wang, Francois Bremond,"], "TTFT": 14.07155442237854, "answer": "[\"InMoDeGAN: Interpretable Motion Decomposition Generative Adversarial Network for Video Generation\", \"LATENT IMAGE ANIMATOR: <br> LEARNING TO ANIMATE IMAGES VIA LATENT SPACE NAVIGATION\", \"Dance Any Beat: Blending Beats with Visuals in Dance Video Generation\"]", "f1 socre": 0.43548387096774194}
{"id": "49680a79-a974-49b0-b7e3-85ff515bc4f2", "pred": ["{{\"Reference\": [\"Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding\"], \"Citation\": [\"Smurfs: Leveraging Multiple Proficiency Agents for Effective Tool Utilization in LLMs\" ]}}\n\nThe paper \"EasyTool: Enhancing LLM-based Agents with Concise Tool Instruction\" references the paper \"Don't Fine-Tune, Dec"], "TTFT": 10.98206639289856, "answer": "{\"Reference\": [\"Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding\"], \"Citation\": [\"Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning\"]}", "f1 socre": 0.6521739130434783}
{"id": "9f29f552-c817-4c2a-8910-b5e8ae7c014f", "pred": ["[\"Fine-Tuning LLaMA for Multi-Stage Text Retrieval\" by Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin, 2023, cited by \"Improving Text Embeddings with Large Language Models\" by Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, Furu Wei, and Jimmy Lin,"], "TTFT": 13.682081937789917, "answer": "[\"RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large Language Models\", \"Fine-Tuning LLaMA for Multi-Stage Text Retrieval\", \"Improving Text Embeddings with Large Language Models\"]", "f1 socre": 0.3225806451612903}
{"id": "30a2535f-7e73-46ad-a402-ee80ac6e4f92", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"Quantum Annealers Chain Strengths: A Simple Heuristic to Set Them All\" by Valentin Gilbert and St\u00e9phane Louise, 2024, arXiv: 2404.05443 [39]],\n[\"Benchmarking Quantum Annealers with Near-Optimal Minor Embeddings\" by Valentin Gilbert"], "TTFT": 16.3840811252594, "answer": "[\"Scaling Advantage in Approximate Optimization with Quantum Annealing\", \"4-clique Network Minor Embedding for Quantum Annealers\", \"Quantum Annealers Chain Strengths: A Simple Heuristic to Set Them All\", \"Benchmarking Quantum Annealers with Near-Optimal Minor-Embedded Instances\"]", "f1 socre": 0.40579710144927533}
{"id": "68e88dcc-6dec-4bb4-b51b-1fe8d6cd856d", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"LGDE: Local Graph-based Dictionary Expansion\" by Schindler et al., 2023]\n[\"Query2doc: Query Expansion with Large Language Models\" by Wang, Yang, and Wei, 2023]\n[\"From free text to clusters: A graph-based approach to text mining and information retrieval\" by Raghav"], "TTFT": 10.241957187652588, "answer": "[\"Query2doc: Query Expansion with Large Language Models\", \"Query Expansion by Prompting Large Language Models\", \"LGDE: Local Graph-based Dictionary Expansion\"]", "f1 socre": 0.32989690721649484}
{"id": "d7adf528-91a9-4978-a25c-29e61562a3be", "pred": ["{{\"Reference\": [\"The Pitfalls of Defining Hallucination\", \"Hallucination is Inevitable: An Innate Limitation of Large Language Models\"],\n\"Citation\": [\"Hallucination is Inevitable: An Innate Limitation of Large Language Models\", \"The Pitfalls of Defining Hallucination\"]}}\n\nIn this paper, the authors show that hallucination is inevitable for large language models (LLMs"], "TTFT": 11.103811502456665, "answer": "{\"Reference\": [\"The Pitfalls of Defining Hallucination\"], \"Citation\": [\"An ASSESSMENT OF MODEL-ON-MODEL DECEPTION\"]}", "f1 socre": 0.29333333333333333}
{"id": "949f3450-9b30-4556-be5f-ca8afe9e072f", "pred": ["{\n\"Title\": \"LLamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource Language\",\n\"Authors\": [\"Cagri Toraman\"],\n\"Reference\": [\n{\"Title\": \"BELLE: A Large-Scale Chinese Instruction Tuning Dataset for Pretrained Language Models\"},\n{\"Title\": \"Bactrain-X: A Multilingual Instruction Tuning Dataset for Low-Resource L"], "TTFT": 12.38475251197815, "answer": "{\"Reference\": [], \"Citation\": [\"LlamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource Language\", \"InstructionCP: A fast approach to transfer Large Language Models into target language\"]}", "f1 socre": 0.43010752688172044}
{"id": "8878fb61-6996-4984-9774-5ce9fb506d37", "pred": ["{\n\"Title\": \"Extreme Compression of Large Language Models via Additive Quantization\",\n\"Authors\": [\"Vage Egiazarian\", \"* 12\", \"Andrei Panferov\", \"* 12\", \"Denis Kuznedelev\", \"* 23\", \"Elias Frantar\", \"* 4\", \"Artem Babenko\", \"* 2\"],\n\"Reference\": [\n{\"Title\": \"Mistral"], "TTFT": 11.69446086883545, "answer": "{\"Reference\": [\"Mistral 7B\"], \"Citation\": [\"Sparse Expansion and Neuronal Disentanglement\"]}", "f1 socre": 0.10526315789473684}
{"id": "17539923-dd06-4712-becd-68fb8d68614d", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"Context-Aware Learning to Rank with Self-Attention\" by Przemys\u0142aw Pobrotyn, Tomasz Bartczak, Miko\u0142aj Synowiec, Rados\u0142aw Bia\u0142obrzeski, and Jaros\u0142aw Bojar]\n\ncites\n\n[\"Learning a ranking model with delta features to study the effect of neighborhood on item preference in e"], "TTFT": 6.013138055801392, "answer": "[\"Influence of Neighborhood on the Preference of an Item in eCommerce Search\", \"Context-Aware Learning to Rank with Self-Attention\"]", "f1 socre": 0.31249999999999994}
{"id": "ad6d0a53-1f8a-46b5-9738-a34d0a3cafaf", "pred": ["Based on the provided instructions and the given papers, the longest citation chain is as follows:\n\n[\"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis\" by Mena, C. Gary et al. (2019), \"Churn prediction using deep convolutional neural networks and autoencoders\" by Wangperawong, A. et al. (2016)]\n\nExplanation:\n\nThe"], "TTFT": 3.672146797180176, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\"]", "f1 socre": 0.4000000000000001}
{"id": "13a4a371-6339-4c9d-82cf-fc9ab2bb017d", "pred": ["{\n\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"],\n\"Citation\": []\n}\n\nThe paper \"OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models\" does not mention \"TinyLlama: An Open-Source Small Language Model\" in its text, so there is no citation relationship between the two papers. However, \"OpenMoE\" references \"Tiny"], "TTFT": 13.604034900665283, "answer": "{\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"], \"Citation\": [\"Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training\"]}", "f1 socre": 0.4047619047619047}
{"id": "c4a4e642-fa00-4377-a756-40f2f7ea7a74", "pred": ["{{\"Reference\": [\"A Survey of Hallucination in Large Foundation Models by Vipula Rawte, Amit Sheth, and Amitava Das\"], \"Citation\": [\"CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation by Quan Tu, Shilong Fan, Zihang Tian, Rui Yan\"]}}\n\nThis paper on CharacterEval references the survey paper on hallucination in large foundation models and"], "TTFT": 11.539960622787476, "answer": "{\"Reference\": [\"A Survey of Hallucination in \\\"Large\\\" Foundation Models\"], \"Citation\": [\"From Role-Play to Drama-Interaction: An LLM Solution\"]}", "f1 socre": 0.2745098039215686}
{"id": "835a4005-8c32-4083-b52d-4e014d1d35f5", "pred": ["Based on the provided instructions and papers, here is the longest citation chain:\n\n[\"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\" by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed, \"SUPERB: Speech processing Universal"], "TTFT": 11.292267322540283, "answer": "[\"CONTRASTIVE SEMI-SUPERVISED LEARNING FOR ASR\", \"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\", \"SUPERB: Speech processing Universal PERformance Benchmark\"]", "f1 socre": 0.38461538461538464}
{"id": "cd618a43-7176-432d-b21e-432e5c2ca4a7", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\" by Abien Fred M. Agarap, 2018, ICMLC 2018]\n\n[\"A Novel Approach Combining Recurrent Neural Network and"], "TTFT": 6.193939208984375, "answer": "[\"Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems\", \"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\"]", "f1 socre": 0.34374999999999994}
{"id": "e904afd3-5cb9-4bf7-bbee-9c8defac858c", "pred": ["Based on the provided instructions and papers, the longest citation chain can be constructed as follows:\n\n[\"On the performativity of SDG classifications in large bibliometric databases\" by Ottaviani and Stahlschmidt (2024), \"Language models are unsupervised multitask learners\" by Radford et al. (2019), \"DistilBERT: Distilling BERT: Six-Order Distillation for a Fast and Che"], "TTFT": 7.292623519897461, "answer": "[\"Contrastive Decoding: Open-ended Text Generation as Optimization\", \"An Empirical Study On Contrastive Search And Contrastive Decoding For Open-ended Text Generation\", \"On the performativity of SDG classifications in large bibliometric databases\"]", "f1 socre": 0.380952380952381}
{"id": "683f4cfd-1cfe-475d-927a-08a71d4f69e9", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\" by Niimi et al., 2024]\n\n[\"Churn Prediction with"], "TTFT": 11.674830198287964, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\", \"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\", \"An Efficient Multimodal Learning Framework to Comprehend Consumer Preferences Using BERT and Cross-Attention\"]", "f1 socre": 0.45312500000000006}
{"id": "0223faf0-f46f-459d-b1cd-45c4dab78fc2", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"A tutorial on the range variant of asymmetric numeral systems\" by James Townsend, \"Asymmetric numeral systems\" by Jarek Duda, \"Asymmetric numeral systems: entropy coding combining speed of Huffman coding with compression rate of arithmetic coding\" by Jarek Duda, Khalid Tahboub, Neeraj J. Gad"], "TTFT": 14.098109483718872, "answer": "[\"Lightweight compression with encryption based on Asymmetric Numeral Systems\", \"A tutorial on the range variant of asymmetric numeral systems\", \"ATC: an Advanced Tucker Compression library for multidimensional data\"]", "f1 socre": 0.3652173913043478}
{"id": "b7405d8e-13f0-4750-a25d-df3a67ffd39a", "pred": ["{{\"Reference\": [\"Benchmarking Large Language Models in Retrieval-Augmented Generation\", \"Retrieval-Augmented Generation in Question Answering: A Survey\"],\n\"Citation\": [\"Seven Failure Points When Engineering a Retrieval Augmented Generation System\"]}}\n\nThis paper presents the lessons learned and failure points from three case studies of implementing Retrieval-Augmented Generation (RAG) systems. It also discusses"], "TTFT": 10.79617714881897, "answer": "{\"Reference\": [\"Benchmarking Large Language Models in Retrieval-Augmented Generation\"], \"Citation\": [\"When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively\"]}", "f1 socre": 0.4186046511627907}
{"id": "150529cd-8767-477e-aa2b-28bae4970fdd", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System\" by Santos et al., \"Massively scalable stencil algorithm for the 3D wave equation on the Cerebras Wafer-Scale Engine\" by Jacquelin and Araya-Polo]\n\nThe reason for this citation chain is that the paper by Sant"], "TTFT": 15.363740921020508, "answer": "[\"Massively scalable stencil algorithm\", \"Disruptive Changes in Field Equation Modeling A Simple Interface for Wafer Scale Engines\", \"Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System\"]", "f1 socre": 0.39285714285714285}
{"id": "a8fa7aa1-2198-419b-a0de-6ba4d1c1f160", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Dance Any Beat: Blending Beats with Visuals in Dance Video Generation\" by Xuanchen Wang, Heng Wang, Weidong Cai, and Dongnan Liu],\n[\"InMoDeGAN: Interpretable Motion Decomposing Generative Adversarial Network for Video Generation\" by Yaohui Wang, Francois Bremond,"], "TTFT": 14.07155442237854, "answer": "[\"InMoDeGAN: Interpretable Motion Decomposition Generative Adversarial Network for Video Generation\", \"LATENT IMAGE ANIMATOR: <br> LEARNING TO ANIMATE IMAGES VIA LATENT SPACE NAVIGATION\", \"Dance Any Beat: Blending Beats with Visuals in Dance Video Generation\"]", "f1 socre": 0.43548387096774194}
{"id": "49680a79-a974-49b0-b7e3-85ff515bc4f2", "pred": ["{{\"Reference\": [\"Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding\"], \"Citation\": [\"Smurfs: Leveraging Multiple Proficiency Agents for Effective Tool Utilization in LLMs\" ]}}\n\nThe paper \"EasyTool: Enhancing LLM-based Agents with Concise Tool Instruction\" references the paper \"Don't Fine-Tune, Dec"], "TTFT": 10.98206639289856, "answer": "{\"Reference\": [\"Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding\"], \"Citation\": [\"Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning\"]}", "f1 socre": 0.6521739130434783}
{"id": "9f29f552-c817-4c2a-8910-b5e8ae7c014f", "pred": ["[\"Fine-Tuning LLaMA for Multi-Stage Text Retrieval\" by Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin, 2023, cited by \"Improving Text Embeddings with Large Language Models\" by Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, Furu Wei, and Jimmy Lin,"], "TTFT": 13.682081937789917, "answer": "[\"RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large Language Models\", \"Fine-Tuning LLaMA for Multi-Stage Text Retrieval\", \"Improving Text Embeddings with Large Language Models\"]", "f1 socre": 0.3225806451612903}
{"id": "30a2535f-7e73-46ad-a402-ee80ac6e4f92", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"Quantum Annealers Chain Strengths: A Simple Heuristic to Set Them All\" by Valentin Gilbert and St\u00e9phane Louise, 2024, arXiv: 2404.05443 [39]],\n[\"Benchmarking Quantum Annealers with Near-Optimal Minor Embeddings\" by Valentin Gilbert"], "TTFT": 16.3840811252594, "answer": "[\"Scaling Advantage in Approximate Optimization with Quantum Annealing\", \"4-clique Network Minor Embedding for Quantum Annealers\", \"Quantum Annealers Chain Strengths: A Simple Heuristic to Set Them All\", \"Benchmarking Quantum Annealers with Near-Optimal Minor-Embedded Instances\"]", "f1 socre": 0.40579710144927533}
{"id": "68e88dcc-6dec-4bb4-b51b-1fe8d6cd856d", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"LGDE: Local Graph-based Dictionary Expansion\" by Schindler et al., 2023]\n[\"Query2doc: Query Expansion with Large Language Models\" by Wang, Yang, and Wei, 2023]\n[\"From free text to clusters: A graph-based approach to text mining and information retrieval\" by Raghav"], "TTFT": 10.241957187652588, "answer": "[\"Query2doc: Query Expansion with Large Language Models\", \"Query Expansion by Prompting Large Language Models\", \"LGDE: Local Graph-based Dictionary Expansion\"]", "f1 socre": 0.32989690721649484}
{"id": "d7adf528-91a9-4978-a25c-29e61562a3be", "pred": ["{{\"Reference\": [\"The Pitfalls of Defining Hallucination\", \"Hallucination is Inevitable: An Innate Limitation of Large Language Models\"],\n\"Citation\": [\"Hallucination is Inevitable: An Innate Limitation of Large Language Models\", \"The Pitfalls of Defining Hallucination\"]}}\n\nIn this paper, the authors show that hallucination is inevitable for large language models (LLMs"], "TTFT": 11.103811502456665, "answer": "{\"Reference\": [\"The Pitfalls of Defining Hallucination\"], \"Citation\": [\"An ASSESSMENT OF MODEL-ON-MODEL DECEPTION\"]}", "f1 socre": 0.29333333333333333}
{"id": "949f3450-9b30-4556-be5f-ca8afe9e072f", "pred": ["{\n\"Title\": \"LLamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource Language\",\n\"Authors\": [\"Cagri Toraman\"],\n\"Reference\": [\n{\"Title\": \"BELLE: A Large-Scale Chinese Instruction Tuning Dataset for Pretrained Language Models\"},\n{\"Title\": \"Bactrain-X: A Multilingual Instruction Tuning Dataset for Low-Resource L"], "TTFT": 12.38475251197815, "answer": "{\"Reference\": [], \"Citation\": [\"LlamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource Language\", \"InstructionCP: A fast approach to transfer Large Language Models into target language\"]}", "f1 socre": 0.43010752688172044}
{"id": "8878fb61-6996-4984-9774-5ce9fb506d37", "pred": ["{\n\"Title\": \"Extreme Compression of Large Language Models via Additive Quantization\",\n\"Authors\": [\"Vage Egiazarian\", \"* 12\", \"Andrei Panferov\", \"* 12\", \"Denis Kuznedelev\", \"* 23\", \"Elias Frantar\", \"* 4\", \"Artem Babenko\", \"* 2\"],\n\"Reference\": [\n{\"Title\": \"Mistral"], "TTFT": 11.69446086883545, "answer": "{\"Reference\": [\"Mistral 7B\"], \"Citation\": [\"Sparse Expansion and Neuronal Disentanglement\"]}", "f1 socre": 0.10526315789473684}
{"id": "17539923-dd06-4712-becd-68fb8d68614d", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"Context-Aware Learning to Rank with Self-Attention\" by Przemys\u0142aw Pobrotyn, Tomasz Bartczak, Miko\u0142aj Synowiec, Rados\u0142aw Bia\u0142obrzeski, and Jaros\u0142aw Bojar]\n\ncites\n\n[\"Learning a ranking model with delta features to study the effect of neighborhood on item preference in e"], "TTFT": 6.013138055801392, "answer": "[\"Influence of Neighborhood on the Preference of an Item in eCommerce Search\", \"Context-Aware Learning to Rank with Self-Attention\"]", "f1 socre": 0.31249999999999994}
{"id": "ad6d0a53-1f8a-46b5-9738-a34d0a3cafaf", "pred": ["Based on the provided instructions and the given papers, the longest citation chain is as follows:\n\n[\"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis\" by Mena, C. Gary et al. (2019), \"Churn prediction using deep convolutional neural networks and autoencoders\" by Wangperawong, A. et al. (2016)]\n\nExplanation:\n\nThe"], "TTFT": 3.672146797180176, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\"]", "f1 socre": 0.4000000000000001}
{"id": "b5b963fe-30ab-435f-9574-dcdfd96f3707", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Minimod: A Finite Difference solver for Seismic Modeling\" by Jie Meng, Andreas Atle, and Henri Calandra, 2020]\n[\"Massively scalable stencil algorithm for seismic modeling on the Cerebras WSE-2\" by Mathias Jacquelin, Mauricio Araya-Polo,"], "TTFT": 8.336519241333008, "answer": "[\"Minimod: A Finite Difference solver for Seismic Modeling\", \"Massively scalable stencil algorithm\"]", "f1 socre": 0.3368421052631579}
{"id": "13a4a371-6339-4c9d-82cf-fc9ab2bb017d", "pred": ["{\n\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"],\n\"Citation\": []\n}\n\nThe paper \"OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models\" does not mention \"TinyLlama: An Open-Source Small Language Model\" in its text, so there is no citation relationship between the two papers. However, \"OpenMoE\" references \"Tiny"], "TTFT": 13.604034900665283, "answer": "{\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"], \"Citation\": [\"Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training\"]}", "f1 socre": 0.4047619047619047}
{"id": "c4a4e642-fa00-4377-a756-40f2f7ea7a74", "pred": ["{{\"Reference\": [\"A Survey of Hallucination in Large Foundation Models by Vipula Rawte, Amit Sheth, and Amitava Das\"], \"Citation\": [\"CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation by Quan Tu, Shilong Fan, Zihang Tian, Rui Yan\"]}}\n\nThis paper on CharacterEval references the survey paper on hallucination in large foundation models and"], "TTFT": 11.539960622787476, "answer": "{\"Reference\": [\"A Survey of Hallucination in \\\"Large\\\" Foundation Models\"], \"Citation\": [\"From Role-Play to Drama-Interaction: An LLM Solution\"]}", "f1 socre": 0.2745098039215686}
{"id": "835a4005-8c32-4083-b52d-4e014d1d35f5", "pred": ["Based on the provided instructions and papers, here is the longest citation chain:\n\n[\"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\" by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed, \"SUPERB: Speech processing Universal"], "TTFT": 11.292267322540283, "answer": "[\"CONTRASTIVE SEMI-SUPERVISED LEARNING FOR ASR\", \"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\", \"SUPERB: Speech processing Universal PERformance Benchmark\"]", "f1 socre": 0.38461538461538464}
{"id": "cd618a43-7176-432d-b21e-432e5c2ca4a7", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\" by Abien Fred M. Agarap, 2018, ICMLC 2018]\n\n[\"A Novel Approach Combining Recurrent Neural Network and"], "TTFT": 6.193939208984375, "answer": "[\"Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems\", \"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\"]", "f1 socre": 0.34374999999999994}
{"id": "e904afd3-5cb9-4bf7-bbee-9c8defac858c", "pred": ["Based on the provided instructions and papers, the longest citation chain can be constructed as follows:\n\n[\"On the performativity of SDG classifications in large bibliometric databases\" by Ottaviani and Stahlschmidt (2024), \"Language models are unsupervised multitask learners\" by Radford et al. (2019), \"DistilBERT: Distilling BERT: Six-Order Distillation for a Fast and Che"], "TTFT": 7.292623519897461, "answer": "[\"Contrastive Decoding: Open-ended Text Generation as Optimization\", \"An Empirical Study On Contrastive Search And Contrastive Decoding For Open-ended Text Generation\", \"On the performativity of SDG classifications in large bibliometric databases\"]", "f1 socre": 0.380952380952381}
{"id": "683f4cfd-1cfe-475d-927a-08a71d4f69e9", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\" by Niimi et al., 2024]\n\n[\"Churn Prediction with"], "TTFT": 11.674830198287964, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\", \"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\", \"An Efficient Multimodal Learning Framework to Comprehend Consumer Preferences Using BERT and Cross-Attention\"]", "f1 socre": 0.45312500000000006}
{"id": "0223faf0-f46f-459d-b1cd-45c4dab78fc2", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"A tutorial on the range variant of asymmetric numeral systems\" by James Townsend, \"Asymmetric numeral systems\" by Jarek Duda, \"Asymmetric numeral systems: entropy coding combining speed of Huffman coding with compression rate of arithmetic coding\" by Jarek Duda, Khalid Tahboub, Neeraj J. Gad"], "TTFT": 14.098109483718872, "answer": "[\"Lightweight compression with encryption based on Asymmetric Numeral Systems\", \"A tutorial on the range variant of asymmetric numeral systems\", \"ATC: an Advanced Tucker Compression library for multidimensional data\"]", "f1 socre": 0.3652173913043478}
{"id": "b7405d8e-13f0-4750-a25d-df3a67ffd39a", "pred": ["{{\"Reference\": [\"Benchmarking Large Language Models in Retrieval-Augmented Generation\", \"Retrieval-Augmented Generation in Question Answering: A Survey\"],\n\"Citation\": [\"Seven Failure Points When Engineering a Retrieval Augmented Generation System\"]}}\n\nThis paper presents the lessons learned and failure points from three case studies of implementing Retrieval-Augmented Generation (RAG) systems. It also discusses"], "TTFT": 10.79617714881897, "answer": "{\"Reference\": [\"Benchmarking Large Language Models in Retrieval-Augmented Generation\"], \"Citation\": [\"When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively\"]}", "f1 socre": 0.4186046511627907}
{"id": "150529cd-8767-477e-aa2b-28bae4970fdd", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System\" by Santos et al., \"Massively scalable stencil algorithm for the 3D wave equation on the Cerebras Wafer-Scale Engine\" by Jacquelin and Araya-Polo]\n\nThe reason for this citation chain is that the paper by Sant"], "TTFT": 15.363740921020508, "answer": "[\"Massively scalable stencil algorithm\", \"Disruptive Changes in Field Equation Modeling A Simple Interface for Wafer Scale Engines\", \"Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System\"]", "f1 socre": 0.39285714285714285}
{"id": "a8fa7aa1-2198-419b-a0de-6ba4d1c1f160", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Dance Any Beat: Blending Beats with Visuals in Dance Video Generation\" by Xuanchen Wang, Heng Wang, Weidong Cai, and Dongnan Liu],\n[\"InMoDeGAN: Interpretable Motion Decomposing Generative Adversarial Network for Video Generation\" by Yaohui Wang, Francois Bremond,"], "TTFT": 14.07155442237854, "answer": "[\"InMoDeGAN: Interpretable Motion Decomposition Generative Adversarial Network for Video Generation\", \"LATENT IMAGE ANIMATOR: <br> LEARNING TO ANIMATE IMAGES VIA LATENT SPACE NAVIGATION\", \"Dance Any Beat: Blending Beats with Visuals in Dance Video Generation\"]", "f1 socre": 0.43548387096774194}
{"id": "49680a79-a974-49b0-b7e3-85ff515bc4f2", "pred": ["{{\"Reference\": [\"Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding\"], \"Citation\": [\"Smurfs: Leveraging Multiple Proficiency Agents for Effective Tool Utilization in LLMs\" ]}}\n\nThe paper \"EasyTool: Enhancing LLM-based Agents with Concise Tool Instruction\" references the paper \"Don't Fine-Tune, Dec"], "TTFT": 10.98206639289856, "answer": "{\"Reference\": [\"Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding\"], \"Citation\": [\"Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning\"]}", "f1 socre": 0.6521739130434783}
{"id": "9f29f552-c817-4c2a-8910-b5e8ae7c014f", "pred": ["[\"Fine-Tuning LLaMA for Multi-Stage Text Retrieval\" by Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin, 2023, cited by \"Improving Text Embeddings with Large Language Models\" by Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, Furu Wei, and Jimmy Lin,"], "TTFT": 13.682081937789917, "answer": "[\"RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large Language Models\", \"Fine-Tuning LLaMA for Multi-Stage Text Retrieval\", \"Improving Text Embeddings with Large Language Models\"]", "f1 socre": 0.3225806451612903}
{"id": "30a2535f-7e73-46ad-a402-ee80ac6e4f92", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"Quantum Annealers Chain Strengths: A Simple Heuristic to Set Them All\" by Valentin Gilbert and St\u00e9phane Louise, 2024, arXiv: 2404.05443 [39]],\n[\"Benchmarking Quantum Annealers with Near-Optimal Minor Embeddings\" by Valentin Gilbert"], "TTFT": 16.3840811252594, "answer": "[\"Scaling Advantage in Approximate Optimization with Quantum Annealing\", \"4-clique Network Minor Embedding for Quantum Annealers\", \"Quantum Annealers Chain Strengths: A Simple Heuristic to Set Them All\", \"Benchmarking Quantum Annealers with Near-Optimal Minor-Embedded Instances\"]", "f1 socre": 0.40579710144927533}
{"id": "68e88dcc-6dec-4bb4-b51b-1fe8d6cd856d", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"LGDE: Local Graph-based Dictionary Expansion\" by Schindler et al., 2023]\n[\"Query2doc: Query Expansion with Large Language Models\" by Wang, Yang, and Wei, 2023]\n[\"From free text to clusters: A graph-based approach to text mining and information retrieval\" by Raghav"], "TTFT": 10.241957187652588, "answer": "[\"Query2doc: Query Expansion with Large Language Models\", \"Query Expansion by Prompting Large Language Models\", \"LGDE: Local Graph-based Dictionary Expansion\"]", "f1 socre": 0.32989690721649484}
{"id": "d7adf528-91a9-4978-a25c-29e61562a3be", "pred": ["{{\"Reference\": [\"The Pitfalls of Defining Hallucination\", \"Hallucination is Inevitable: An Innate Limitation of Large Language Models\"],\n\"Citation\": [\"Hallucination is Inevitable: An Innate Limitation of Large Language Models\", \"The Pitfalls of Defining Hallucination\"]}}\n\nIn this paper, the authors show that hallucination is inevitable for large language models (LLMs"], "TTFT": 11.103811502456665, "answer": "{\"Reference\": [\"The Pitfalls of Defining Hallucination\"], \"Citation\": [\"An ASSESSMENT OF MODEL-ON-MODEL DECEPTION\"]}", "f1 socre": 0.29333333333333333}
{"id": "949f3450-9b30-4556-be5f-ca8afe9e072f", "pred": ["{\n\"Title\": \"LLamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource Language\",\n\"Authors\": [\"Cagri Toraman\"],\n\"Reference\": [\n{\"Title\": \"BELLE: A Large-Scale Chinese Instruction Tuning Dataset for Pretrained Language Models\"},\n{\"Title\": \"Bactrain-X: A Multilingual Instruction Tuning Dataset for Low-Resource L"], "TTFT": 12.38475251197815, "answer": "{\"Reference\": [], \"Citation\": [\"LlamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource Language\", \"InstructionCP: A fast approach to transfer Large Language Models into target language\"]}", "f1 socre": 0.43010752688172044}
{"id": "8878fb61-6996-4984-9774-5ce9fb506d37", "pred": ["{\n\"Title\": \"Extreme Compression of Large Language Models via Additive Quantization\",\n\"Authors\": [\"Vage Egiazarian\", \"* 12\", \"Andrei Panferov\", \"* 12\", \"Denis Kuznedelev\", \"* 23\", \"Elias Frantar\", \"* 4\", \"Artem Babenko\", \"* 2\"],\n\"Reference\": [\n{\"Title\": \"Mistral"], "TTFT": 11.69446086883545, "answer": "{\"Reference\": [\"Mistral 7B\"], \"Citation\": [\"Sparse Expansion and Neuronal Disentanglement\"]}", "f1 socre": 0.10526315789473684}
{"id": "17539923-dd06-4712-becd-68fb8d68614d", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"Context-Aware Learning to Rank with Self-Attention\" by Przemys\u0142aw Pobrotyn, Tomasz Bartczak, Miko\u0142aj Synowiec, Rados\u0142aw Bia\u0142obrzeski, and Jaros\u0142aw Bojar]\n\ncites\n\n[\"Learning a ranking model with delta features to study the effect of neighborhood on item preference in e"], "TTFT": 6.013138055801392, "answer": "[\"Influence of Neighborhood on the Preference of an Item in eCommerce Search\", \"Context-Aware Learning to Rank with Self-Attention\"]", "f1 socre": 0.31249999999999994}
{"id": "ad6d0a53-1f8a-46b5-9738-a34d0a3cafaf", "pred": ["Based on the provided instructions and the given papers, the longest citation chain is as follows:\n\n[\"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis\" by Mena, C. Gary et al. (2019), \"Churn prediction using deep convolutional neural networks and autoencoders\" by Wangperawong, A. et al. (2016)]\n\nExplanation:\n\nThe"], "TTFT": 3.672146797180176, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\"]", "f1 socre": 0.4000000000000001}
{"id": "b5b963fe-30ab-435f-9574-dcdfd96f3707", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Minimod: A Finite Difference solver for Seismic Modeling\" by Jie Meng, Andreas Atle, and Henri Calandra, 2020]\n[\"Massively scalable stencil algorithm for seismic modeling on the Cerebras WSE-2\" by Mathias Jacquelin, Mauricio Araya-Polo,"], "TTFT": 8.336519241333008, "answer": "[\"Minimod: A Finite Difference solver for Seismic Modeling\", \"Massively scalable stencil algorithm\"]", "f1 socre": 0.3368421052631579}
{"id": "7b104919-2fba-44d2-91dd-a56bd2f30e91", "pred": ["{{\"Reference\": [\"LLM A*: Human in the Loop Large Language Models Enabled A* Search for Robotics\"], \"Citation\": [\"FunSearch\", \"Various papers on search-based MAPF methods (e.g., CBS, ECBS, EECBS)\"]}}\n\nThe paper \"Evolution of Heuristics: Towards Efficient Automatic Algorithm Design Using Large Language Model\" references the paper \"LLM A*: Human in"], "TTFT": 12.563227891921997, "answer": "{\"Reference\": [\"LLM A*: Human in the Loop Large Language Models Enabled A* Search for Robotics\"], \"Citation\": [\"Ensembling Prioritized Hybrid Policies for Multi-agent Pathfinding\"]}", "f1 socre": 0.5}
{"id": "13a4a371-6339-4c9d-82cf-fc9ab2bb017d", "pred": ["{\n\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"],\n\"Citation\": []\n}\n\nThe paper \"OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models\" does not mention \"TinyLlama: An Open-Source Small Language Model\" in its text, so there is no citation relationship between the two papers. However, \"OpenMoE\" references \"Tiny"], "TTFT": 13.604034900665283, "answer": "{\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"], \"Citation\": [\"Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training\"]}", "f1 socre": 0.4047619047619047}
{"id": "c4a4e642-fa00-4377-a756-40f2f7ea7a74", "pred": ["{{\"Reference\": [\"A Survey of Hallucination in Large Foundation Models by Vipula Rawte, Amit Sheth, and Amitava Das\"], \"Citation\": [\"CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation by Quan Tu, Shilong Fan, Zihang Tian, Rui Yan\"]}}\n\nThis paper on CharacterEval references the survey paper on hallucination in large foundation models and"], "TTFT": 11.539960622787476, "answer": "{\"Reference\": [\"A Survey of Hallucination in \\\"Large\\\" Foundation Models\"], \"Citation\": [\"From Role-Play to Drama-Interaction: An LLM Solution\"]}", "f1 socre": 0.2745098039215686}
{"id": "835a4005-8c32-4083-b52d-4e014d1d35f5", "pred": ["Based on the provided instructions and papers, here is the longest citation chain:\n\n[\"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\" by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed, \"SUPERB: Speech processing Universal"], "TTFT": 11.292267322540283, "answer": "[\"CONTRASTIVE SEMI-SUPERVISED LEARNING FOR ASR\", \"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\", \"SUPERB: Speech processing Universal PERformance Benchmark\"]", "f1 socre": 0.38461538461538464}
{"id": "cd618a43-7176-432d-b21e-432e5c2ca4a7", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\" by Abien Fred M. Agarap, 2018, ICMLC 2018]\n\n[\"A Novel Approach Combining Recurrent Neural Network and"], "TTFT": 6.193939208984375, "answer": "[\"Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems\", \"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\"]", "f1 socre": 0.34374999999999994}
{"id": "e904afd3-5cb9-4bf7-bbee-9c8defac858c", "pred": ["Based on the provided instructions and papers, the longest citation chain can be constructed as follows:\n\n[\"On the performativity of SDG classifications in large bibliometric databases\" by Ottaviani and Stahlschmidt (2024), \"Language models are unsupervised multitask learners\" by Radford et al. (2019), \"DistilBERT: Distilling BERT: Six-Order Distillation for a Fast and Che"], "TTFT": 7.292623519897461, "answer": "[\"Contrastive Decoding: Open-ended Text Generation as Optimization\", \"An Empirical Study On Contrastive Search And Contrastive Decoding For Open-ended Text Generation\", \"On the performativity of SDG classifications in large bibliometric databases\"]", "f1 socre": 0.380952380952381}
{"id": "683f4cfd-1cfe-475d-927a-08a71d4f69e9", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\" by Niimi et al., 2024]\n\n[\"Churn Prediction with"], "TTFT": 11.674830198287964, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\", \"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\", \"An Efficient Multimodal Learning Framework to Comprehend Consumer Preferences Using BERT and Cross-Attention\"]", "f1 socre": 0.45312500000000006}
{"id": "0223faf0-f46f-459d-b1cd-45c4dab78fc2", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"A tutorial on the range variant of asymmetric numeral systems\" by James Townsend, \"Asymmetric numeral systems\" by Jarek Duda, \"Asymmetric numeral systems: entropy coding combining speed of Huffman coding with compression rate of arithmetic coding\" by Jarek Duda, Khalid Tahboub, Neeraj J. Gad"], "TTFT": 14.098109483718872, "answer": "[\"Lightweight compression with encryption based on Asymmetric Numeral Systems\", \"A tutorial on the range variant of asymmetric numeral systems\", \"ATC: an Advanced Tucker Compression library for multidimensional data\"]", "f1 socre": 0.3652173913043478}
{"id": "b7405d8e-13f0-4750-a25d-df3a67ffd39a", "pred": ["{{\"Reference\": [\"Benchmarking Large Language Models in Retrieval-Augmented Generation\", \"Retrieval-Augmented Generation in Question Answering: A Survey\"],\n\"Citation\": [\"Seven Failure Points When Engineering a Retrieval Augmented Generation System\"]}}\n\nThis paper presents the lessons learned and failure points from three case studies of implementing Retrieval-Augmented Generation (RAG) systems. It also discusses"], "TTFT": 10.79617714881897, "answer": "{\"Reference\": [\"Benchmarking Large Language Models in Retrieval-Augmented Generation\"], \"Citation\": [\"When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively\"]}", "f1 socre": 0.4186046511627907}
{"id": "150529cd-8767-477e-aa2b-28bae4970fdd", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System\" by Santos et al., \"Massively scalable stencil algorithm for the 3D wave equation on the Cerebras Wafer-Scale Engine\" by Jacquelin and Araya-Polo]\n\nThe reason for this citation chain is that the paper by Sant"], "TTFT": 15.363740921020508, "answer": "[\"Massively scalable stencil algorithm\", \"Disruptive Changes in Field Equation Modeling A Simple Interface for Wafer Scale Engines\", \"Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System\"]", "f1 socre": 0.39285714285714285}
{"id": "a8fa7aa1-2198-419b-a0de-6ba4d1c1f160", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Dance Any Beat: Blending Beats with Visuals in Dance Video Generation\" by Xuanchen Wang, Heng Wang, Weidong Cai, and Dongnan Liu],\n[\"InMoDeGAN: Interpretable Motion Decomposing Generative Adversarial Network for Video Generation\" by Yaohui Wang, Francois Bremond,"], "TTFT": 14.07155442237854, "answer": "[\"InMoDeGAN: Interpretable Motion Decomposition Generative Adversarial Network for Video Generation\", \"LATENT IMAGE ANIMATOR: <br> LEARNING TO ANIMATE IMAGES VIA LATENT SPACE NAVIGATION\", \"Dance Any Beat: Blending Beats with Visuals in Dance Video Generation\"]", "f1 socre": 0.43548387096774194}
{"id": "49680a79-a974-49b0-b7e3-85ff515bc4f2", "pred": ["{{\"Reference\": [\"Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding\"], \"Citation\": [\"Smurfs: Leveraging Multiple Proficiency Agents for Effective Tool Utilization in LLMs\" ]}}\n\nThe paper \"EasyTool: Enhancing LLM-based Agents with Concise Tool Instruction\" references the paper \"Don't Fine-Tune, Dec"], "TTFT": 10.98206639289856, "answer": "{\"Reference\": [\"Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding\"], \"Citation\": [\"Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning\"]}", "f1 socre": 0.6521739130434783}
{"id": "9f29f552-c817-4c2a-8910-b5e8ae7c014f", "pred": ["[\"Fine-Tuning LLaMA for Multi-Stage Text Retrieval\" by Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin, 2023, cited by \"Improving Text Embeddings with Large Language Models\" by Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, Furu Wei, and Jimmy Lin,"], "TTFT": 13.682081937789917, "answer": "[\"RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large Language Models\", \"Fine-Tuning LLaMA for Multi-Stage Text Retrieval\", \"Improving Text Embeddings with Large Language Models\"]", "f1 socre": 0.3225806451612903}
{"id": "30a2535f-7e73-46ad-a402-ee80ac6e4f92", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"Quantum Annealers Chain Strengths: A Simple Heuristic to Set Them All\" by Valentin Gilbert and St\u00e9phane Louise, 2024, arXiv: 2404.05443 [39]],\n[\"Benchmarking Quantum Annealers with Near-Optimal Minor Embeddings\" by Valentin Gilbert"], "TTFT": 16.3840811252594, "answer": "[\"Scaling Advantage in Approximate Optimization with Quantum Annealing\", \"4-clique Network Minor Embedding for Quantum Annealers\", \"Quantum Annealers Chain Strengths: A Simple Heuristic to Set Them All\", \"Benchmarking Quantum Annealers with Near-Optimal Minor-Embedded Instances\"]", "f1 socre": 0.40579710144927533}
{"id": "68e88dcc-6dec-4bb4-b51b-1fe8d6cd856d", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"LGDE: Local Graph-based Dictionary Expansion\" by Schindler et al., 2023]\n[\"Query2doc: Query Expansion with Large Language Models\" by Wang, Yang, and Wei, 2023]\n[\"From free text to clusters: A graph-based approach to text mining and information retrieval\" by Raghav"], "TTFT": 10.241957187652588, "answer": "[\"Query2doc: Query Expansion with Large Language Models\", \"Query Expansion by Prompting Large Language Models\", \"LGDE: Local Graph-based Dictionary Expansion\"]", "f1 socre": 0.32989690721649484}
{"id": "d7adf528-91a9-4978-a25c-29e61562a3be", "pred": ["{{\"Reference\": [\"The Pitfalls of Defining Hallucination\", \"Hallucination is Inevitable: An Innate Limitation of Large Language Models\"],\n\"Citation\": [\"Hallucination is Inevitable: An Innate Limitation of Large Language Models\", \"The Pitfalls of Defining Hallucination\"]}}\n\nIn this paper, the authors show that hallucination is inevitable for large language models (LLMs"], "TTFT": 11.103811502456665, "answer": "{\"Reference\": [\"The Pitfalls of Defining Hallucination\"], \"Citation\": [\"An ASSESSMENT OF MODEL-ON-MODEL DECEPTION\"]}", "f1 socre": 0.29333333333333333}
{"id": "949f3450-9b30-4556-be5f-ca8afe9e072f", "pred": ["{\n\"Title\": \"LLamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource Language\",\n\"Authors\": [\"Cagri Toraman\"],\n\"Reference\": [\n{\"Title\": \"BELLE: A Large-Scale Chinese Instruction Tuning Dataset for Pretrained Language Models\"},\n{\"Title\": \"Bactrain-X: A Multilingual Instruction Tuning Dataset for Low-Resource L"], "TTFT": 12.38475251197815, "answer": "{\"Reference\": [], \"Citation\": [\"LlamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource Language\", \"InstructionCP: A fast approach to transfer Large Language Models into target language\"]}", "f1 socre": 0.43010752688172044}
{"id": "8878fb61-6996-4984-9774-5ce9fb506d37", "pred": ["{\n\"Title\": \"Extreme Compression of Large Language Models via Additive Quantization\",\n\"Authors\": [\"Vage Egiazarian\", \"* 12\", \"Andrei Panferov\", \"* 12\", \"Denis Kuznedelev\", \"* 23\", \"Elias Frantar\", \"* 4\", \"Artem Babenko\", \"* 2\"],\n\"Reference\": [\n{\"Title\": \"Mistral"], "TTFT": 11.69446086883545, "answer": "{\"Reference\": [\"Mistral 7B\"], \"Citation\": [\"Sparse Expansion and Neuronal Disentanglement\"]}", "f1 socre": 0.10526315789473684}
{"id": "17539923-dd06-4712-becd-68fb8d68614d", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"Context-Aware Learning to Rank with Self-Attention\" by Przemys\u0142aw Pobrotyn, Tomasz Bartczak, Miko\u0142aj Synowiec, Rados\u0142aw Bia\u0142obrzeski, and Jaros\u0142aw Bojar]\n\ncites\n\n[\"Learning a ranking model with delta features to study the effect of neighborhood on item preference in e"], "TTFT": 6.013138055801392, "answer": "[\"Influence of Neighborhood on the Preference of an Item in eCommerce Search\", \"Context-Aware Learning to Rank with Self-Attention\"]", "f1 socre": 0.31249999999999994}
{"id": "ad6d0a53-1f8a-46b5-9738-a34d0a3cafaf", "pred": ["Based on the provided instructions and the given papers, the longest citation chain is as follows:\n\n[\"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis\" by Mena, C. Gary et al. (2019), \"Churn prediction using deep convolutional neural networks and autoencoders\" by Wangperawong, A. et al. (2016)]\n\nExplanation:\n\nThe"], "TTFT": 3.672146797180176, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\"]", "f1 socre": 0.4000000000000001}
{"id": "b5b963fe-30ab-435f-9574-dcdfd96f3707", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Minimod: A Finite Difference solver for Seismic Modeling\" by Jie Meng, Andreas Atle, and Henri Calandra, 2020]\n[\"Massively scalable stencil algorithm for seismic modeling on the Cerebras WSE-2\" by Mathias Jacquelin, Mauricio Araya-Polo,"], "TTFT": 8.336519241333008, "answer": "[\"Minimod: A Finite Difference solver for Seismic Modeling\", \"Massively scalable stencil algorithm\"]", "f1 socre": 0.3368421052631579}
{"id": "7b104919-2fba-44d2-91dd-a56bd2f30e91", "pred": ["{{\"Reference\": [\"LLM A*: Human in the Loop Large Language Models Enabled A* Search for Robotics\"], \"Citation\": [\"FunSearch\", \"Various papers on search-based MAPF methods (e.g., CBS, ECBS, EECBS)\"]}}\n\nThe paper \"Evolution of Heuristics: Towards Efficient Automatic Algorithm Design Using Large Language Model\" references the paper \"LLM A*: Human in"], "TTFT": 12.563227891921997, "answer": "{\"Reference\": [\"LLM A*: Human in the Loop Large Language Models Enabled A* Search for Robotics\"], \"Citation\": [\"Ensembling Prioritized Hybrid Policies for Multi-agent Pathfinding\"]}", "f1 socre": 0.5}
{"id": "31ee8539-2e9e-40e2-adc3-082c872698f7", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\" by Niimi et al., 2024]\n\ncites\n\n[\"A PRE"], "TTFT": 11.793168783187866, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\", \"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\", \"An Efficient Multimodal Learning Framework to Comprehend Consumer Preferences Using BERT and Cross-Attention\"]", "f1 socre": 0.39682539682539686}
{"id": "13a4a371-6339-4c9d-82cf-fc9ab2bb017d", "pred": ["{\n\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"],\n\"Citation\": []\n}\n\nThe paper \"OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models\" does not mention \"TinyLlama: An Open-Source Small Language Model\" in its text, so there is no citation relationship between the two papers. However, \"OpenMoE\" references \"Tiny"], "TTFT": 13.604034900665283, "answer": "{\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"], \"Citation\": [\"Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training\"]}", "f1 socre": 0.4047619047619047}
{"id": "c4a4e642-fa00-4377-a756-40f2f7ea7a74", "pred": ["{{\"Reference\": [\"A Survey of Hallucination in Large Foundation Models by Vipula Rawte, Amit Sheth, and Amitava Das\"], \"Citation\": [\"CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation by Quan Tu, Shilong Fan, Zihang Tian, Rui Yan\"]}}\n\nThis paper on CharacterEval references the survey paper on hallucination in large foundation models and"], "TTFT": 11.539960622787476, "answer": "{\"Reference\": [\"A Survey of Hallucination in \\\"Large\\\" Foundation Models\"], \"Citation\": [\"From Role-Play to Drama-Interaction: An LLM Solution\"]}", "f1 socre": 0.2745098039215686}
{"id": "835a4005-8c32-4083-b52d-4e014d1d35f5", "pred": ["Based on the provided instructions and papers, here is the longest citation chain:\n\n[\"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\" by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed, \"SUPERB: Speech processing Universal"], "TTFT": 11.292267322540283, "answer": "[\"CONTRASTIVE SEMI-SUPERVISED LEARNING FOR ASR\", \"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\", \"SUPERB: Speech processing Universal PERformance Benchmark\"]", "f1 socre": 0.38461538461538464}
{"id": "cd618a43-7176-432d-b21e-432e5c2ca4a7", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\" by Abien Fred M. Agarap, 2018, ICMLC 2018]\n\n[\"A Novel Approach Combining Recurrent Neural Network and"], "TTFT": 6.193939208984375, "answer": "[\"Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems\", \"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\"]", "f1 socre": 0.34374999999999994}
{"id": "e904afd3-5cb9-4bf7-bbee-9c8defac858c", "pred": ["Based on the provided instructions and papers, the longest citation chain can be constructed as follows:\n\n[\"On the performativity of SDG classifications in large bibliometric databases\" by Ottaviani and Stahlschmidt (2024), \"Language models are unsupervised multitask learners\" by Radford et al. (2019), \"DistilBERT: Distilling BERT: Six-Order Distillation for a Fast and Che"], "TTFT": 7.292623519897461, "answer": "[\"Contrastive Decoding: Open-ended Text Generation as Optimization\", \"An Empirical Study On Contrastive Search And Contrastive Decoding For Open-ended Text Generation\", \"On the performativity of SDG classifications in large bibliometric databases\"]", "f1 socre": 0.380952380952381}
{"id": "683f4cfd-1cfe-475d-927a-08a71d4f69e9", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\" by Niimi et al., 2024]\n\n[\"Churn Prediction with"], "TTFT": 11.674830198287964, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\", \"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\", \"An Efficient Multimodal Learning Framework to Comprehend Consumer Preferences Using BERT and Cross-Attention\"]", "f1 socre": 0.45312500000000006}
{"id": "0223faf0-f46f-459d-b1cd-45c4dab78fc2", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"A tutorial on the range variant of asymmetric numeral systems\" by James Townsend, \"Asymmetric numeral systems\" by Jarek Duda, \"Asymmetric numeral systems: entropy coding combining speed of Huffman coding with compression rate of arithmetic coding\" by Jarek Duda, Khalid Tahboub, Neeraj J. Gad"], "TTFT": 14.098109483718872, "answer": "[\"Lightweight compression with encryption based on Asymmetric Numeral Systems\", \"A tutorial on the range variant of asymmetric numeral systems\", \"ATC: an Advanced Tucker Compression library for multidimensional data\"]", "f1 socre": 0.3652173913043478}
{"id": "b7405d8e-13f0-4750-a25d-df3a67ffd39a", "pred": ["{{\"Reference\": [\"Benchmarking Large Language Models in Retrieval-Augmented Generation\", \"Retrieval-Augmented Generation in Question Answering: A Survey\"],\n\"Citation\": [\"Seven Failure Points When Engineering a Retrieval Augmented Generation System\"]}}\n\nThis paper presents the lessons learned and failure points from three case studies of implementing Retrieval-Augmented Generation (RAG) systems. It also discusses"], "TTFT": 10.79617714881897, "answer": "{\"Reference\": [\"Benchmarking Large Language Models in Retrieval-Augmented Generation\"], \"Citation\": [\"When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively\"]}", "f1 socre": 0.4186046511627907}
{"id": "150529cd-8767-477e-aa2b-28bae4970fdd", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System\" by Santos et al., \"Massively scalable stencil algorithm for the 3D wave equation on the Cerebras Wafer-Scale Engine\" by Jacquelin and Araya-Polo]\n\nThe reason for this citation chain is that the paper by Sant"], "TTFT": 15.363740921020508, "answer": "[\"Massively scalable stencil algorithm\", \"Disruptive Changes in Field Equation Modeling A Simple Interface for Wafer Scale Engines\", \"Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System\"]", "f1 socre": 0.39285714285714285}
{"id": "a8fa7aa1-2198-419b-a0de-6ba4d1c1f160", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Dance Any Beat: Blending Beats with Visuals in Dance Video Generation\" by Xuanchen Wang, Heng Wang, Weidong Cai, and Dongnan Liu],\n[\"InMoDeGAN: Interpretable Motion Decomposing Generative Adversarial Network for Video Generation\" by Yaohui Wang, Francois Bremond,"], "TTFT": 14.07155442237854, "answer": "[\"InMoDeGAN: Interpretable Motion Decomposition Generative Adversarial Network for Video Generation\", \"LATENT IMAGE ANIMATOR: <br> LEARNING TO ANIMATE IMAGES VIA LATENT SPACE NAVIGATION\", \"Dance Any Beat: Blending Beats with Visuals in Dance Video Generation\"]", "f1 socre": 0.43548387096774194}
{"id": "49680a79-a974-49b0-b7e3-85ff515bc4f2", "pred": ["{{\"Reference\": [\"Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding\"], \"Citation\": [\"Smurfs: Leveraging Multiple Proficiency Agents for Effective Tool Utilization in LLMs\" ]}}\n\nThe paper \"EasyTool: Enhancing LLM-based Agents with Concise Tool Instruction\" references the paper \"Don't Fine-Tune, Dec"], "TTFT": 10.98206639289856, "answer": "{\"Reference\": [\"Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding\"], \"Citation\": [\"Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning\"]}", "f1 socre": 0.6521739130434783}
{"id": "9f29f552-c817-4c2a-8910-b5e8ae7c014f", "pred": ["[\"Fine-Tuning LLaMA for Multi-Stage Text Retrieval\" by Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin, 2023, cited by \"Improving Text Embeddings with Large Language Models\" by Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, Furu Wei, and Jimmy Lin,"], "TTFT": 13.682081937789917, "answer": "[\"RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large Language Models\", \"Fine-Tuning LLaMA for Multi-Stage Text Retrieval\", \"Improving Text Embeddings with Large Language Models\"]", "f1 socre": 0.3225806451612903}
{"id": "30a2535f-7e73-46ad-a402-ee80ac6e4f92", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"Quantum Annealers Chain Strengths: A Simple Heuristic to Set Them All\" by Valentin Gilbert and St\u00e9phane Louise, 2024, arXiv: 2404.05443 [39]],\n[\"Benchmarking Quantum Annealers with Near-Optimal Minor Embeddings\" by Valentin Gilbert"], "TTFT": 16.3840811252594, "answer": "[\"Scaling Advantage in Approximate Optimization with Quantum Annealing\", \"4-clique Network Minor Embedding for Quantum Annealers\", \"Quantum Annealers Chain Strengths: A Simple Heuristic to Set Them All\", \"Benchmarking Quantum Annealers with Near-Optimal Minor-Embedded Instances\"]", "f1 socre": 0.40579710144927533}
{"id": "68e88dcc-6dec-4bb4-b51b-1fe8d6cd856d", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"LGDE: Local Graph-based Dictionary Expansion\" by Schindler et al., 2023]\n[\"Query2doc: Query Expansion with Large Language Models\" by Wang, Yang, and Wei, 2023]\n[\"From free text to clusters: A graph-based approach to text mining and information retrieval\" by Raghav"], "TTFT": 10.241957187652588, "answer": "[\"Query2doc: Query Expansion with Large Language Models\", \"Query Expansion by Prompting Large Language Models\", \"LGDE: Local Graph-based Dictionary Expansion\"]", "f1 socre": 0.32989690721649484}
{"id": "d7adf528-91a9-4978-a25c-29e61562a3be", "pred": ["{{\"Reference\": [\"The Pitfalls of Defining Hallucination\", \"Hallucination is Inevitable: An Innate Limitation of Large Language Models\"],\n\"Citation\": [\"Hallucination is Inevitable: An Innate Limitation of Large Language Models\", \"The Pitfalls of Defining Hallucination\"]}}\n\nIn this paper, the authors show that hallucination is inevitable for large language models (LLMs"], "TTFT": 11.103811502456665, "answer": "{\"Reference\": [\"The Pitfalls of Defining Hallucination\"], \"Citation\": [\"An ASSESSMENT OF MODEL-ON-MODEL DECEPTION\"]}", "f1 socre": 0.29333333333333333}
{"id": "949f3450-9b30-4556-be5f-ca8afe9e072f", "pred": ["{\n\"Title\": \"LLamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource Language\",\n\"Authors\": [\"Cagri Toraman\"],\n\"Reference\": [\n{\"Title\": \"BELLE: A Large-Scale Chinese Instruction Tuning Dataset for Pretrained Language Models\"},\n{\"Title\": \"Bactrain-X: A Multilingual Instruction Tuning Dataset for Low-Resource L"], "TTFT": 12.38475251197815, "answer": "{\"Reference\": [], \"Citation\": [\"LlamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource Language\", \"InstructionCP: A fast approach to transfer Large Language Models into target language\"]}", "f1 socre": 0.43010752688172044}
{"id": "8878fb61-6996-4984-9774-5ce9fb506d37", "pred": ["{\n\"Title\": \"Extreme Compression of Large Language Models via Additive Quantization\",\n\"Authors\": [\"Vage Egiazarian\", \"* 12\", \"Andrei Panferov\", \"* 12\", \"Denis Kuznedelev\", \"* 23\", \"Elias Frantar\", \"* 4\", \"Artem Babenko\", \"* 2\"],\n\"Reference\": [\n{\"Title\": \"Mistral"], "TTFT": 11.69446086883545, "answer": "{\"Reference\": [\"Mistral 7B\"], \"Citation\": [\"Sparse Expansion and Neuronal Disentanglement\"]}", "f1 socre": 0.10526315789473684}
{"id": "17539923-dd06-4712-becd-68fb8d68614d", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"Context-Aware Learning to Rank with Self-Attention\" by Przemys\u0142aw Pobrotyn, Tomasz Bartczak, Miko\u0142aj Synowiec, Rados\u0142aw Bia\u0142obrzeski, and Jaros\u0142aw Bojar]\n\ncites\n\n[\"Learning a ranking model with delta features to study the effect of neighborhood on item preference in e"], "TTFT": 6.013138055801392, "answer": "[\"Influence of Neighborhood on the Preference of an Item in eCommerce Search\", \"Context-Aware Learning to Rank with Self-Attention\"]", "f1 socre": 0.31249999999999994}
{"id": "ad6d0a53-1f8a-46b5-9738-a34d0a3cafaf", "pred": ["Based on the provided instructions and the given papers, the longest citation chain is as follows:\n\n[\"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis\" by Mena, C. Gary et al. (2019), \"Churn prediction using deep convolutional neural networks and autoencoders\" by Wangperawong, A. et al. (2016)]\n\nExplanation:\n\nThe"], "TTFT": 3.672146797180176, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\"]", "f1 socre": 0.4000000000000001}
{"id": "b5b963fe-30ab-435f-9574-dcdfd96f3707", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Minimod: A Finite Difference solver for Seismic Modeling\" by Jie Meng, Andreas Atle, and Henri Calandra, 2020]\n[\"Massively scalable stencil algorithm for seismic modeling on the Cerebras WSE-2\" by Mathias Jacquelin, Mauricio Araya-Polo,"], "TTFT": 8.336519241333008, "answer": "[\"Minimod: A Finite Difference solver for Seismic Modeling\", \"Massively scalable stencil algorithm\"]", "f1 socre": 0.3368421052631579}
{"id": "7b104919-2fba-44d2-91dd-a56bd2f30e91", "pred": ["{{\"Reference\": [\"LLM A*: Human in the Loop Large Language Models Enabled A* Search for Robotics\"], \"Citation\": [\"FunSearch\", \"Various papers on search-based MAPF methods (e.g., CBS, ECBS, EECBS)\"]}}\n\nThe paper \"Evolution of Heuristics: Towards Efficient Automatic Algorithm Design Using Large Language Model\" references the paper \"LLM A*: Human in"], "TTFT": 12.563227891921997, "answer": "{\"Reference\": [\"LLM A*: Human in the Loop Large Language Models Enabled A* Search for Robotics\"], \"Citation\": [\"Ensembling Prioritized Hybrid Policies for Multi-agent Pathfinding\"]}", "f1 socre": 0.5}
{"id": "31ee8539-2e9e-40e2-adc3-082c872698f7", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\" by Niimi et al., 2024]\n\ncites\n\n[\"A PRE"], "TTFT": 11.793168783187866, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\", \"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\", \"An Efficient Multimodal Learning Framework to Comprehend Consumer Preferences Using BERT and Cross-Attention\"]", "f1 socre": 0.39682539682539686}
{"id": "9b982095-daad-4632-9573-b0d7037dd6a7", "pred": ["{{\"Reference\": [\"A Prompt for Investigating the Effectiveness of LLMs in Self-Critiquing for Planning: An Empirical Study\", \"Large Language Models Cannot Self-Correct Reasoning Intrinsically\"],\n\"Citation\": [\"LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks\"]}}"], "TTFT": 10.443064212799072, "answer": "{\"Reference\": [\"LARGE LANGUAGE MoDELS CANNOT SELF-CorRECT REASONING YET\", \"Can Large Language Models Really Improve by Self-critiquing Their Own Plans?\"], \"Citation\": []}", "f1 socre": 0.3943661971830986}
{"id": "13a4a371-6339-4c9d-82cf-fc9ab2bb017d", "pred": ["{\n\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"],\n\"Citation\": []\n}\n\nThe paper \"OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models\" does not mention \"TinyLlama: An Open-Source Small Language Model\" in its text, so there is no citation relationship between the two papers. However, \"OpenMoE\" references \"Tiny"], "TTFT": 13.604034900665283, "answer": "{\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"], \"Citation\": [\"Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training\"]}", "f1 socre": 0.4047619047619047}
{"id": "c4a4e642-fa00-4377-a756-40f2f7ea7a74", "pred": ["{{\"Reference\": [\"A Survey of Hallucination in Large Foundation Models by Vipula Rawte, Amit Sheth, and Amitava Das\"], \"Citation\": [\"CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation by Quan Tu, Shilong Fan, Zihang Tian, Rui Yan\"]}}\n\nThis paper on CharacterEval references the survey paper on hallucination in large foundation models and"], "TTFT": 11.539960622787476, "answer": "{\"Reference\": [\"A Survey of Hallucination in \\\"Large\\\" Foundation Models\"], \"Citation\": [\"From Role-Play to Drama-Interaction: An LLM Solution\"]}", "f1 socre": 0.2745098039215686}
{"id": "835a4005-8c32-4083-b52d-4e014d1d35f5", "pred": ["Based on the provided instructions and papers, here is the longest citation chain:\n\n[\"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\" by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed, \"SUPERB: Speech processing Universal"], "TTFT": 11.292267322540283, "answer": "[\"CONTRASTIVE SEMI-SUPERVISED LEARNING FOR ASR\", \"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\", \"SUPERB: Speech processing Universal PERformance Benchmark\"]", "f1 socre": 0.38461538461538464}
{"id": "cd618a43-7176-432d-b21e-432e5c2ca4a7", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\" by Abien Fred M. Agarap, 2018, ICMLC 2018]\n\n[\"A Novel Approach Combining Recurrent Neural Network and"], "TTFT": 6.193939208984375, "answer": "[\"Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems\", \"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\"]", "f1 socre": 0.34374999999999994}
{"id": "e904afd3-5cb9-4bf7-bbee-9c8defac858c", "pred": ["Based on the provided instructions and papers, the longest citation chain can be constructed as follows:\n\n[\"On the performativity of SDG classifications in large bibliometric databases\" by Ottaviani and Stahlschmidt (2024), \"Language models are unsupervised multitask learners\" by Radford et al. (2019), \"DistilBERT: Distilling BERT: Six-Order Distillation for a Fast and Che"], "TTFT": 7.292623519897461, "answer": "[\"Contrastive Decoding: Open-ended Text Generation as Optimization\", \"An Empirical Study On Contrastive Search And Contrastive Decoding For Open-ended Text Generation\", \"On the performativity of SDG classifications in large bibliometric databases\"]", "f1 socre": 0.380952380952381}
{"id": "683f4cfd-1cfe-475d-927a-08a71d4f69e9", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\" by Niimi et al., 2024]\n\n[\"Churn Prediction with"], "TTFT": 11.674830198287964, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\", \"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\", \"An Efficient Multimodal Learning Framework to Comprehend Consumer Preferences Using BERT and Cross-Attention\"]", "f1 socre": 0.45312500000000006}
{"id": "0223faf0-f46f-459d-b1cd-45c4dab78fc2", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"A tutorial on the range variant of asymmetric numeral systems\" by James Townsend, \"Asymmetric numeral systems\" by Jarek Duda, \"Asymmetric numeral systems: entropy coding combining speed of Huffman coding with compression rate of arithmetic coding\" by Jarek Duda, Khalid Tahboub, Neeraj J. Gad"], "TTFT": 14.098109483718872, "answer": "[\"Lightweight compression with encryption based on Asymmetric Numeral Systems\", \"A tutorial on the range variant of asymmetric numeral systems\", \"ATC: an Advanced Tucker Compression library for multidimensional data\"]", "f1 socre": 0.3652173913043478}
{"id": "b7405d8e-13f0-4750-a25d-df3a67ffd39a", "pred": ["{{\"Reference\": [\"Benchmarking Large Language Models in Retrieval-Augmented Generation\", \"Retrieval-Augmented Generation in Question Answering: A Survey\"],\n\"Citation\": [\"Seven Failure Points When Engineering a Retrieval Augmented Generation System\"]}}\n\nThis paper presents the lessons learned and failure points from three case studies of implementing Retrieval-Augmented Generation (RAG) systems. It also discusses"], "TTFT": 10.79617714881897, "answer": "{\"Reference\": [\"Benchmarking Large Language Models in Retrieval-Augmented Generation\"], \"Citation\": [\"When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively\"]}", "f1 socre": 0.4186046511627907}
{"id": "150529cd-8767-477e-aa2b-28bae4970fdd", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System\" by Santos et al., \"Massively scalable stencil algorithm for the 3D wave equation on the Cerebras Wafer-Scale Engine\" by Jacquelin and Araya-Polo]\n\nThe reason for this citation chain is that the paper by Sant"], "TTFT": 15.363740921020508, "answer": "[\"Massively scalable stencil algorithm\", \"Disruptive Changes in Field Equation Modeling A Simple Interface for Wafer Scale Engines\", \"Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System\"]", "f1 socre": 0.39285714285714285}
{"id": "a8fa7aa1-2198-419b-a0de-6ba4d1c1f160", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Dance Any Beat: Blending Beats with Visuals in Dance Video Generation\" by Xuanchen Wang, Heng Wang, Weidong Cai, and Dongnan Liu],\n[\"InMoDeGAN: Interpretable Motion Decomposing Generative Adversarial Network for Video Generation\" by Yaohui Wang, Francois Bremond,"], "TTFT": 14.07155442237854, "answer": "[\"InMoDeGAN: Interpretable Motion Decomposition Generative Adversarial Network for Video Generation\", \"LATENT IMAGE ANIMATOR: <br> LEARNING TO ANIMATE IMAGES VIA LATENT SPACE NAVIGATION\", \"Dance Any Beat: Blending Beats with Visuals in Dance Video Generation\"]", "f1 socre": 0.43548387096774194}
{"id": "49680a79-a974-49b0-b7e3-85ff515bc4f2", "pred": ["{{\"Reference\": [\"Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding\"], \"Citation\": [\"Smurfs: Leveraging Multiple Proficiency Agents for Effective Tool Utilization in LLMs\" ]}}\n\nThe paper \"EasyTool: Enhancing LLM-based Agents with Concise Tool Instruction\" references the paper \"Don't Fine-Tune, Dec"], "TTFT": 10.98206639289856, "answer": "{\"Reference\": [\"Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding\"], \"Citation\": [\"Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning\"]}", "f1 socre": 0.6521739130434783}
{"id": "9f29f552-c817-4c2a-8910-b5e8ae7c014f", "pred": ["[\"Fine-Tuning LLaMA for Multi-Stage Text Retrieval\" by Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin, 2023, cited by \"Improving Text Embeddings with Large Language Models\" by Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, Furu Wei, and Jimmy Lin,"], "TTFT": 13.682081937789917, "answer": "[\"RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large Language Models\", \"Fine-Tuning LLaMA for Multi-Stage Text Retrieval\", \"Improving Text Embeddings with Large Language Models\"]", "f1 socre": 0.3225806451612903}
{"id": "30a2535f-7e73-46ad-a402-ee80ac6e4f92", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"Quantum Annealers Chain Strengths: A Simple Heuristic to Set Them All\" by Valentin Gilbert and St\u00e9phane Louise, 2024, arXiv: 2404.05443 [39]],\n[\"Benchmarking Quantum Annealers with Near-Optimal Minor Embeddings\" by Valentin Gilbert"], "TTFT": 16.3840811252594, "answer": "[\"Scaling Advantage in Approximate Optimization with Quantum Annealing\", \"4-clique Network Minor Embedding for Quantum Annealers\", \"Quantum Annealers Chain Strengths: A Simple Heuristic to Set Them All\", \"Benchmarking Quantum Annealers with Near-Optimal Minor-Embedded Instances\"]", "f1 socre": 0.40579710144927533}
{"id": "68e88dcc-6dec-4bb4-b51b-1fe8d6cd856d", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"LGDE: Local Graph-based Dictionary Expansion\" by Schindler et al., 2023]\n[\"Query2doc: Query Expansion with Large Language Models\" by Wang, Yang, and Wei, 2023]\n[\"From free text to clusters: A graph-based approach to text mining and information retrieval\" by Raghav"], "TTFT": 10.241957187652588, "answer": "[\"Query2doc: Query Expansion with Large Language Models\", \"Query Expansion by Prompting Large Language Models\", \"LGDE: Local Graph-based Dictionary Expansion\"]", "f1 socre": 0.32989690721649484}
{"id": "d7adf528-91a9-4978-a25c-29e61562a3be", "pred": ["{{\"Reference\": [\"The Pitfalls of Defining Hallucination\", \"Hallucination is Inevitable: An Innate Limitation of Large Language Models\"],\n\"Citation\": [\"Hallucination is Inevitable: An Innate Limitation of Large Language Models\", \"The Pitfalls of Defining Hallucination\"]}}\n\nIn this paper, the authors show that hallucination is inevitable for large language models (LLMs"], "TTFT": 11.103811502456665, "answer": "{\"Reference\": [\"The Pitfalls of Defining Hallucination\"], \"Citation\": [\"An ASSESSMENT OF MODEL-ON-MODEL DECEPTION\"]}", "f1 socre": 0.29333333333333333}
{"id": "949f3450-9b30-4556-be5f-ca8afe9e072f", "pred": ["{\n\"Title\": \"LLamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource Language\",\n\"Authors\": [\"Cagri Toraman\"],\n\"Reference\": [\n{\"Title\": \"BELLE: A Large-Scale Chinese Instruction Tuning Dataset for Pretrained Language Models\"},\n{\"Title\": \"Bactrain-X: A Multilingual Instruction Tuning Dataset for Low-Resource L"], "TTFT": 12.38475251197815, "answer": "{\"Reference\": [], \"Citation\": [\"LlamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource Language\", \"InstructionCP: A fast approach to transfer Large Language Models into target language\"]}", "f1 socre": 0.43010752688172044}
{"id": "8878fb61-6996-4984-9774-5ce9fb506d37", "pred": ["{\n\"Title\": \"Extreme Compression of Large Language Models via Additive Quantization\",\n\"Authors\": [\"Vage Egiazarian\", \"* 12\", \"Andrei Panferov\", \"* 12\", \"Denis Kuznedelev\", \"* 23\", \"Elias Frantar\", \"* 4\", \"Artem Babenko\", \"* 2\"],\n\"Reference\": [\n{\"Title\": \"Mistral"], "TTFT": 11.69446086883545, "answer": "{\"Reference\": [\"Mistral 7B\"], \"Citation\": [\"Sparse Expansion and Neuronal Disentanglement\"]}", "f1 socre": 0.10526315789473684}
{"id": "17539923-dd06-4712-becd-68fb8d68614d", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"Context-Aware Learning to Rank with Self-Attention\" by Przemys\u0142aw Pobrotyn, Tomasz Bartczak, Miko\u0142aj Synowiec, Rados\u0142aw Bia\u0142obrzeski, and Jaros\u0142aw Bojar]\n\ncites\n\n[\"Learning a ranking model with delta features to study the effect of neighborhood on item preference in e"], "TTFT": 6.013138055801392, "answer": "[\"Influence of Neighborhood on the Preference of an Item in eCommerce Search\", \"Context-Aware Learning to Rank with Self-Attention\"]", "f1 socre": 0.31249999999999994}
{"id": "ad6d0a53-1f8a-46b5-9738-a34d0a3cafaf", "pred": ["Based on the provided instructions and the given papers, the longest citation chain is as follows:\n\n[\"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis\" by Mena, C. Gary et al. (2019), \"Churn prediction using deep convolutional neural networks and autoencoders\" by Wangperawong, A. et al. (2016)]\n\nExplanation:\n\nThe"], "TTFT": 3.672146797180176, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\"]", "f1 socre": 0.4000000000000001}
{"id": "b5b963fe-30ab-435f-9574-dcdfd96f3707", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Minimod: A Finite Difference solver for Seismic Modeling\" by Jie Meng, Andreas Atle, and Henri Calandra, 2020]\n[\"Massively scalable stencil algorithm for seismic modeling on the Cerebras WSE-2\" by Mathias Jacquelin, Mauricio Araya-Polo,"], "TTFT": 8.336519241333008, "answer": "[\"Minimod: A Finite Difference solver for Seismic Modeling\", \"Massively scalable stencil algorithm\"]", "f1 socre": 0.3368421052631579}
{"id": "7b104919-2fba-44d2-91dd-a56bd2f30e91", "pred": ["{{\"Reference\": [\"LLM A*: Human in the Loop Large Language Models Enabled A* Search for Robotics\"], \"Citation\": [\"FunSearch\", \"Various papers on search-based MAPF methods (e.g., CBS, ECBS, EECBS)\"]}}\n\nThe paper \"Evolution of Heuristics: Towards Efficient Automatic Algorithm Design Using Large Language Model\" references the paper \"LLM A*: Human in"], "TTFT": 12.563227891921997, "answer": "{\"Reference\": [\"LLM A*: Human in the Loop Large Language Models Enabled A* Search for Robotics\"], \"Citation\": [\"Ensembling Prioritized Hybrid Policies for Multi-agent Pathfinding\"]}", "f1 socre": 0.5}
{"id": "31ee8539-2e9e-40e2-adc3-082c872698f7", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\" by Niimi et al., 2024]\n\ncites\n\n[\"A PRE"], "TTFT": 11.793168783187866, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\", \"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\", \"An Efficient Multimodal Learning Framework to Comprehend Consumer Preferences Using BERT and Cross-Attention\"]", "f1 socre": 0.39682539682539686}
{"id": "9b982095-daad-4632-9573-b0d7037dd6a7", "pred": ["{{\"Reference\": [\"A Prompt for Investigating the Effectiveness of LLMs in Self-Critiquing for Planning: An Empirical Study\", \"Large Language Models Cannot Self-Correct Reasoning Intrinsically\"],\n\"Citation\": [\"LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks\"]}}"], "TTFT": 10.443064212799072, "answer": "{\"Reference\": [\"LARGE LANGUAGE MoDELS CANNOT SELF-CorRECT REASONING YET\", \"Can Large Language Models Really Improve by Self-critiquing Their Own Plans?\"], \"Citation\": []}", "f1 socre": 0.3943661971830986}
{"id": "ecded47a-3741-4458-ba61-c04271c33651", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"MonaCoBERT: Monotonic attention based ConvBERT for Knowledge Tracing\" by Unggi Lee, Yonghyun Park, Yujin Kim, Seongyune Choi, and Hyeoncheol Kim, 2021]\n\nThis paper cites:\n\n[\"Understanding the Instability of Transformer-based Mod"], "TTFT": 10.399053812026978, "answer": "[\"Very Deep Transformers for Neural Machine Translation\", \"Understanding the Difficulty of Training Transformers\", \"MonaCoBERT: Monotonic attention based ConvBERT for Knowledge Tracing\"]", "f1 socre": 0.29411764705882354}
{"id": "13a4a371-6339-4c9d-82cf-fc9ab2bb017d", "pred": ["{\n\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"],\n\"Citation\": []\n}\n\nThe paper \"OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models\" does not mention \"TinyLlama: An Open-Source Small Language Model\" in its text, so there is no citation relationship between the two papers. However, \"OpenMoE\" references \"Tiny"], "TTFT": 13.604034900665283, "answer": "{\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"], \"Citation\": [\"Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training\"]}", "f1 socre": 0.4047619047619047}
{"id": "c4a4e642-fa00-4377-a756-40f2f7ea7a74", "pred": ["{{\"Reference\": [\"A Survey of Hallucination in Large Foundation Models by Vipula Rawte, Amit Sheth, and Amitava Das\"], \"Citation\": [\"CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation by Quan Tu, Shilong Fan, Zihang Tian, Rui Yan\"]}}\n\nThis paper on CharacterEval references the survey paper on hallucination in large foundation models and"], "TTFT": 11.539960622787476, "answer": "{\"Reference\": [\"A Survey of Hallucination in \\\"Large\\\" Foundation Models\"], \"Citation\": [\"From Role-Play to Drama-Interaction: An LLM Solution\"]}", "f1 socre": 0.2745098039215686}
{"id": "835a4005-8c32-4083-b52d-4e014d1d35f5", "pred": ["Based on the provided instructions and papers, here is the longest citation chain:\n\n[\"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\" by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed, \"SUPERB: Speech processing Universal"], "TTFT": 11.292267322540283, "answer": "[\"CONTRASTIVE SEMI-SUPERVISED LEARNING FOR ASR\", \"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\", \"SUPERB: Speech processing Universal PERformance Benchmark\"]", "f1 socre": 0.38461538461538464}
{"id": "cd618a43-7176-432d-b21e-432e5c2ca4a7", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\" by Abien Fred M. Agarap, 2018, ICMLC 2018]\n\n[\"A Novel Approach Combining Recurrent Neural Network and"], "TTFT": 6.193939208984375, "answer": "[\"Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems\", \"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\"]", "f1 socre": 0.34374999999999994}
{"id": "e904afd3-5cb9-4bf7-bbee-9c8defac858c", "pred": ["Based on the provided instructions and papers, the longest citation chain can be constructed as follows:\n\n[\"On the performativity of SDG classifications in large bibliometric databases\" by Ottaviani and Stahlschmidt (2024), \"Language models are unsupervised multitask learners\" by Radford et al. (2019), \"DistilBERT: Distilling BERT: Six-Order Distillation for a Fast and Che"], "TTFT": 7.292623519897461, "answer": "[\"Contrastive Decoding: Open-ended Text Generation as Optimization\", \"An Empirical Study On Contrastive Search And Contrastive Decoding For Open-ended Text Generation\", \"On the performativity of SDG classifications in large bibliometric databases\"]", "f1 socre": 0.380952380952381}
{"id": "683f4cfd-1cfe-475d-927a-08a71d4f69e9", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\" by Niimi et al., 2024]\n\n[\"Churn Prediction with"], "TTFT": 11.674830198287964, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\", \"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\", \"An Efficient Multimodal Learning Framework to Comprehend Consumer Preferences Using BERT and Cross-Attention\"]", "f1 socre": 0.45312500000000006}
{"id": "0223faf0-f46f-459d-b1cd-45c4dab78fc2", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"A tutorial on the range variant of asymmetric numeral systems\" by James Townsend, \"Asymmetric numeral systems\" by Jarek Duda, \"Asymmetric numeral systems: entropy coding combining speed of Huffman coding with compression rate of arithmetic coding\" by Jarek Duda, Khalid Tahboub, Neeraj J. Gad"], "TTFT": 14.098109483718872, "answer": "[\"Lightweight compression with encryption based on Asymmetric Numeral Systems\", \"A tutorial on the range variant of asymmetric numeral systems\", \"ATC: an Advanced Tucker Compression library for multidimensional data\"]", "f1 socre": 0.3652173913043478}
{"id": "b7405d8e-13f0-4750-a25d-df3a67ffd39a", "pred": ["{{\"Reference\": [\"Benchmarking Large Language Models in Retrieval-Augmented Generation\", \"Retrieval-Augmented Generation in Question Answering: A Survey\"],\n\"Citation\": [\"Seven Failure Points When Engineering a Retrieval Augmented Generation System\"]}}\n\nThis paper presents the lessons learned and failure points from three case studies of implementing Retrieval-Augmented Generation (RAG) systems. It also discusses"], "TTFT": 10.79617714881897, "answer": "{\"Reference\": [\"Benchmarking Large Language Models in Retrieval-Augmented Generation\"], \"Citation\": [\"When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively\"]}", "f1 socre": 0.4186046511627907}
{"id": "150529cd-8767-477e-aa2b-28bae4970fdd", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System\" by Santos et al., \"Massively scalable stencil algorithm for the 3D wave equation on the Cerebras Wafer-Scale Engine\" by Jacquelin and Araya-Polo]\n\nThe reason for this citation chain is that the paper by Sant"], "TTFT": 15.363740921020508, "answer": "[\"Massively scalable stencil algorithm\", \"Disruptive Changes in Field Equation Modeling A Simple Interface for Wafer Scale Engines\", \"Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System\"]", "f1 socre": 0.39285714285714285}
{"id": "a8fa7aa1-2198-419b-a0de-6ba4d1c1f160", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Dance Any Beat: Blending Beats with Visuals in Dance Video Generation\" by Xuanchen Wang, Heng Wang, Weidong Cai, and Dongnan Liu],\n[\"InMoDeGAN: Interpretable Motion Decomposing Generative Adversarial Network for Video Generation\" by Yaohui Wang, Francois Bremond,"], "TTFT": 14.07155442237854, "answer": "[\"InMoDeGAN: Interpretable Motion Decomposition Generative Adversarial Network for Video Generation\", \"LATENT IMAGE ANIMATOR: <br> LEARNING TO ANIMATE IMAGES VIA LATENT SPACE NAVIGATION\", \"Dance Any Beat: Blending Beats with Visuals in Dance Video Generation\"]", "f1 socre": 0.43548387096774194}
{"id": "49680a79-a974-49b0-b7e3-85ff515bc4f2", "pred": ["{{\"Reference\": [\"Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding\"], \"Citation\": [\"Smurfs: Leveraging Multiple Proficiency Agents for Effective Tool Utilization in LLMs\" ]}}\n\nThe paper \"EasyTool: Enhancing LLM-based Agents with Concise Tool Instruction\" references the paper \"Don't Fine-Tune, Dec"], "TTFT": 10.98206639289856, "answer": "{\"Reference\": [\"Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding\"], \"Citation\": [\"Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning\"]}", "f1 socre": 0.6521739130434783}
{"id": "9f29f552-c817-4c2a-8910-b5e8ae7c014f", "pred": ["[\"Fine-Tuning LLaMA for Multi-Stage Text Retrieval\" by Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin, 2023, cited by \"Improving Text Embeddings with Large Language Models\" by Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, Furu Wei, and Jimmy Lin,"], "TTFT": 13.682081937789917, "answer": "[\"RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large Language Models\", \"Fine-Tuning LLaMA for Multi-Stage Text Retrieval\", \"Improving Text Embeddings with Large Language Models\"]", "f1 socre": 0.3225806451612903}
{"id": "30a2535f-7e73-46ad-a402-ee80ac6e4f92", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"Quantum Annealers Chain Strengths: A Simple Heuristic to Set Them All\" by Valentin Gilbert and St\u00e9phane Louise, 2024, arXiv: 2404.05443 [39]],\n[\"Benchmarking Quantum Annealers with Near-Optimal Minor Embeddings\" by Valentin Gilbert"], "TTFT": 16.3840811252594, "answer": "[\"Scaling Advantage in Approximate Optimization with Quantum Annealing\", \"4-clique Network Minor Embedding for Quantum Annealers\", \"Quantum Annealers Chain Strengths: A Simple Heuristic to Set Them All\", \"Benchmarking Quantum Annealers with Near-Optimal Minor-Embedded Instances\"]", "f1 socre": 0.40579710144927533}
{"id": "68e88dcc-6dec-4bb4-b51b-1fe8d6cd856d", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"LGDE: Local Graph-based Dictionary Expansion\" by Schindler et al., 2023]\n[\"Query2doc: Query Expansion with Large Language Models\" by Wang, Yang, and Wei, 2023]\n[\"From free text to clusters: A graph-based approach to text mining and information retrieval\" by Raghav"], "TTFT": 10.241957187652588, "answer": "[\"Query2doc: Query Expansion with Large Language Models\", \"Query Expansion by Prompting Large Language Models\", \"LGDE: Local Graph-based Dictionary Expansion\"]", "f1 socre": 0.32989690721649484}
{"id": "d7adf528-91a9-4978-a25c-29e61562a3be", "pred": ["{{\"Reference\": [\"The Pitfalls of Defining Hallucination\", \"Hallucination is Inevitable: An Innate Limitation of Large Language Models\"],\n\"Citation\": [\"Hallucination is Inevitable: An Innate Limitation of Large Language Models\", \"The Pitfalls of Defining Hallucination\"]}}\n\nIn this paper, the authors show that hallucination is inevitable for large language models (LLMs"], "TTFT": 11.103811502456665, "answer": "{\"Reference\": [\"The Pitfalls of Defining Hallucination\"], \"Citation\": [\"An ASSESSMENT OF MODEL-ON-MODEL DECEPTION\"]}", "f1 socre": 0.29333333333333333}
{"id": "949f3450-9b30-4556-be5f-ca8afe9e072f", "pred": ["{\n\"Title\": \"LLamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource Language\",\n\"Authors\": [\"Cagri Toraman\"],\n\"Reference\": [\n{\"Title\": \"BELLE: A Large-Scale Chinese Instruction Tuning Dataset for Pretrained Language Models\"},\n{\"Title\": \"Bactrain-X: A Multilingual Instruction Tuning Dataset for Low-Resource L"], "TTFT": 12.38475251197815, "answer": "{\"Reference\": [], \"Citation\": [\"LlamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource Language\", \"InstructionCP: A fast approach to transfer Large Language Models into target language\"]}", "f1 socre": 0.43010752688172044}
{"id": "8878fb61-6996-4984-9774-5ce9fb506d37", "pred": ["{\n\"Title\": \"Extreme Compression of Large Language Models via Additive Quantization\",\n\"Authors\": [\"Vage Egiazarian\", \"* 12\", \"Andrei Panferov\", \"* 12\", \"Denis Kuznedelev\", \"* 23\", \"Elias Frantar\", \"* 4\", \"Artem Babenko\", \"* 2\"],\n\"Reference\": [\n{\"Title\": \"Mistral"], "TTFT": 11.69446086883545, "answer": "{\"Reference\": [\"Mistral 7B\"], \"Citation\": [\"Sparse Expansion and Neuronal Disentanglement\"]}", "f1 socre": 0.10526315789473684}
{"id": "17539923-dd06-4712-becd-68fb8d68614d", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"Context-Aware Learning to Rank with Self-Attention\" by Przemys\u0142aw Pobrotyn, Tomasz Bartczak, Miko\u0142aj Synowiec, Rados\u0142aw Bia\u0142obrzeski, and Jaros\u0142aw Bojar]\n\ncites\n\n[\"Learning a ranking model with delta features to study the effect of neighborhood on item preference in e"], "TTFT": 6.013138055801392, "answer": "[\"Influence of Neighborhood on the Preference of an Item in eCommerce Search\", \"Context-Aware Learning to Rank with Self-Attention\"]", "f1 socre": 0.31249999999999994}
{"id": "ad6d0a53-1f8a-46b5-9738-a34d0a3cafaf", "pred": ["Based on the provided instructions and the given papers, the longest citation chain is as follows:\n\n[\"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis\" by Mena, C. Gary et al. (2019), \"Churn prediction using deep convolutional neural networks and autoencoders\" by Wangperawong, A. et al. (2016)]\n\nExplanation:\n\nThe"], "TTFT": 3.672146797180176, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\"]", "f1 socre": 0.4000000000000001}
{"id": "b5b963fe-30ab-435f-9574-dcdfd96f3707", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Minimod: A Finite Difference solver for Seismic Modeling\" by Jie Meng, Andreas Atle, and Henri Calandra, 2020]\n[\"Massively scalable stencil algorithm for seismic modeling on the Cerebras WSE-2\" by Mathias Jacquelin, Mauricio Araya-Polo,"], "TTFT": 8.336519241333008, "answer": "[\"Minimod: A Finite Difference solver for Seismic Modeling\", \"Massively scalable stencil algorithm\"]", "f1 socre": 0.3368421052631579}
{"id": "7b104919-2fba-44d2-91dd-a56bd2f30e91", "pred": ["{{\"Reference\": [\"LLM A*: Human in the Loop Large Language Models Enabled A* Search for Robotics\"], \"Citation\": [\"FunSearch\", \"Various papers on search-based MAPF methods (e.g., CBS, ECBS, EECBS)\"]}}\n\nThe paper \"Evolution of Heuristics: Towards Efficient Automatic Algorithm Design Using Large Language Model\" references the paper \"LLM A*: Human in"], "TTFT": 12.563227891921997, "answer": "{\"Reference\": [\"LLM A*: Human in the Loop Large Language Models Enabled A* Search for Robotics\"], \"Citation\": [\"Ensembling Prioritized Hybrid Policies for Multi-agent Pathfinding\"]}", "f1 socre": 0.5}
{"id": "31ee8539-2e9e-40e2-adc3-082c872698f7", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\" by Niimi et al., 2024]\n\ncites\n\n[\"A PRE"], "TTFT": 11.793168783187866, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\", \"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\", \"An Efficient Multimodal Learning Framework to Comprehend Consumer Preferences Using BERT and Cross-Attention\"]", "f1 socre": 0.39682539682539686}
{"id": "9b982095-daad-4632-9573-b0d7037dd6a7", "pred": ["{{\"Reference\": [\"A Prompt for Investigating the Effectiveness of LLMs in Self-Critiquing for Planning: An Empirical Study\", \"Large Language Models Cannot Self-Correct Reasoning Intrinsically\"],\n\"Citation\": [\"LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks\"]}}"], "TTFT": 10.443064212799072, "answer": "{\"Reference\": [\"LARGE LANGUAGE MoDELS CANNOT SELF-CorRECT REASONING YET\", \"Can Large Language Models Really Improve by Self-critiquing Their Own Plans?\"], \"Citation\": []}", "f1 socre": 0.3943661971830986}
{"id": "ecded47a-3741-4458-ba61-c04271c33651", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"MonaCoBERT: Monotonic attention based ConvBERT for Knowledge Tracing\" by Unggi Lee, Yonghyun Park, Yujin Kim, Seongyune Choi, and Hyeoncheol Kim, 2021]\n\nThis paper cites:\n\n[\"Understanding the Instability of Transformer-based Mod"], "TTFT": 10.399053812026978, "answer": "[\"Very Deep Transformers for Neural Machine Translation\", \"Understanding the Difficulty of Training Transformers\", \"MonaCoBERT: Monotonic attention based ConvBERT for Knowledge Tracing\"]", "f1 socre": 0.29411764705882354}
{"id": "71d0fb39-d315-4e33-beec-c53271663f7f", "pred": ["{{\"Reference\": [\"LoRA-Switch: Boosting the Efficiency of Dynamic LLM Adapters via System-Algorithm Co-design\"], \"Citation\": []}}"], "TTFT": 13.256085872650146, "answer": "{\"Reference\": [\"ONLINE SPECULATIVE DECODING\"], \"Citation\": [\"LoRA-Switch: Boosting the Efficiency of Dynamic LLM Adapters via System-Algorithm Co-design\"]}", "f1 socre": 0.9019607843137255}
{"id": "13a4a371-6339-4c9d-82cf-fc9ab2bb017d", "pred": ["{\n\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"],\n\"Citation\": []\n}\n\nThe paper \"OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models\" does not mention \"TinyLlama: An Open-Source Small Language Model\" in its text, so there is no citation relationship between the two papers. However, \"OpenMoE\" references \"Tiny"], "TTFT": 13.604034900665283, "answer": "{\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"], \"Citation\": [\"Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training\"]}", "f1 socre": 0.4047619047619047}
{"id": "c4a4e642-fa00-4377-a756-40f2f7ea7a74", "pred": ["{{\"Reference\": [\"A Survey of Hallucination in Large Foundation Models by Vipula Rawte, Amit Sheth, and Amitava Das\"], \"Citation\": [\"CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation by Quan Tu, Shilong Fan, Zihang Tian, Rui Yan\"]}}\n\nThis paper on CharacterEval references the survey paper on hallucination in large foundation models and"], "TTFT": 11.539960622787476, "answer": "{\"Reference\": [\"A Survey of Hallucination in \\\"Large\\\" Foundation Models\"], \"Citation\": [\"From Role-Play to Drama-Interaction: An LLM Solution\"]}", "f1 socre": 0.2745098039215686}
{"id": "835a4005-8c32-4083-b52d-4e014d1d35f5", "pred": ["Based on the provided instructions and papers, here is the longest citation chain:\n\n[\"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\" by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed, \"SUPERB: Speech processing Universal"], "TTFT": 11.292267322540283, "answer": "[\"CONTRASTIVE SEMI-SUPERVISED LEARNING FOR ASR\", \"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\", \"SUPERB: Speech processing Universal PERformance Benchmark\"]", "f1 socre": 0.38461538461538464}
{"id": "cd618a43-7176-432d-b21e-432e5c2ca4a7", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\" by Abien Fred M. Agarap, 2018, ICMLC 2018]\n\n[\"A Novel Approach Combining Recurrent Neural Network and"], "TTFT": 6.193939208984375, "answer": "[\"Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems\", \"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\"]", "f1 socre": 0.34374999999999994}
{"id": "e904afd3-5cb9-4bf7-bbee-9c8defac858c", "pred": ["Based on the provided instructions and papers, the longest citation chain can be constructed as follows:\n\n[\"On the performativity of SDG classifications in large bibliometric databases\" by Ottaviani and Stahlschmidt (2024), \"Language models are unsupervised multitask learners\" by Radford et al. (2019), \"DistilBERT: Distilling BERT: Six-Order Distillation for a Fast and Che"], "TTFT": 7.292623519897461, "answer": "[\"Contrastive Decoding: Open-ended Text Generation as Optimization\", \"An Empirical Study On Contrastive Search And Contrastive Decoding For Open-ended Text Generation\", \"On the performativity of SDG classifications in large bibliometric databases\"]", "f1 socre": 0.380952380952381}
{"id": "683f4cfd-1cfe-475d-927a-08a71d4f69e9", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\" by Niimi et al., 2024]\n\n[\"Churn Prediction with"], "TTFT": 11.674830198287964, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\", \"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\", \"An Efficient Multimodal Learning Framework to Comprehend Consumer Preferences Using BERT and Cross-Attention\"]", "f1 socre": 0.45312500000000006}
{"id": "0223faf0-f46f-459d-b1cd-45c4dab78fc2", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"A tutorial on the range variant of asymmetric numeral systems\" by James Townsend, \"Asymmetric numeral systems\" by Jarek Duda, \"Asymmetric numeral systems: entropy coding combining speed of Huffman coding with compression rate of arithmetic coding\" by Jarek Duda, Khalid Tahboub, Neeraj J. Gad"], "TTFT": 14.098109483718872, "answer": "[\"Lightweight compression with encryption based on Asymmetric Numeral Systems\", \"A tutorial on the range variant of asymmetric numeral systems\", \"ATC: an Advanced Tucker Compression library for multidimensional data\"]", "f1 socre": 0.3652173913043478}
{"id": "b7405d8e-13f0-4750-a25d-df3a67ffd39a", "pred": ["{{\"Reference\": [\"Benchmarking Large Language Models in Retrieval-Augmented Generation\", \"Retrieval-Augmented Generation in Question Answering: A Survey\"],\n\"Citation\": [\"Seven Failure Points When Engineering a Retrieval Augmented Generation System\"]}}\n\nThis paper presents the lessons learned and failure points from three case studies of implementing Retrieval-Augmented Generation (RAG) systems. It also discusses"], "TTFT": 10.79617714881897, "answer": "{\"Reference\": [\"Benchmarking Large Language Models in Retrieval-Augmented Generation\"], \"Citation\": [\"When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively\"]}", "f1 socre": 0.4186046511627907}
{"id": "150529cd-8767-477e-aa2b-28bae4970fdd", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System\" by Santos et al., \"Massively scalable stencil algorithm for the 3D wave equation on the Cerebras Wafer-Scale Engine\" by Jacquelin and Araya-Polo]\n\nThe reason for this citation chain is that the paper by Sant"], "TTFT": 15.363740921020508, "answer": "[\"Massively scalable stencil algorithm\", \"Disruptive Changes in Field Equation Modeling A Simple Interface for Wafer Scale Engines\", \"Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System\"]", "f1 socre": 0.39285714285714285}
{"id": "a8fa7aa1-2198-419b-a0de-6ba4d1c1f160", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Dance Any Beat: Blending Beats with Visuals in Dance Video Generation\" by Xuanchen Wang, Heng Wang, Weidong Cai, and Dongnan Liu],\n[\"InMoDeGAN: Interpretable Motion Decomposing Generative Adversarial Network for Video Generation\" by Yaohui Wang, Francois Bremond,"], "TTFT": 14.07155442237854, "answer": "[\"InMoDeGAN: Interpretable Motion Decomposition Generative Adversarial Network for Video Generation\", \"LATENT IMAGE ANIMATOR: <br> LEARNING TO ANIMATE IMAGES VIA LATENT SPACE NAVIGATION\", \"Dance Any Beat: Blending Beats with Visuals in Dance Video Generation\"]", "f1 socre": 0.43548387096774194}
{"id": "49680a79-a974-49b0-b7e3-85ff515bc4f2", "pred": ["{{\"Reference\": [\"Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding\"], \"Citation\": [\"Smurfs: Leveraging Multiple Proficiency Agents for Effective Tool Utilization in LLMs\" ]}}\n\nThe paper \"EasyTool: Enhancing LLM-based Agents with Concise Tool Instruction\" references the paper \"Don't Fine-Tune, Dec"], "TTFT": 10.98206639289856, "answer": "{\"Reference\": [\"Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding\"], \"Citation\": [\"Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning\"]}", "f1 socre": 0.6521739130434783}
{"id": "9f29f552-c817-4c2a-8910-b5e8ae7c014f", "pred": ["[\"Fine-Tuning LLaMA for Multi-Stage Text Retrieval\" by Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin, 2023, cited by \"Improving Text Embeddings with Large Language Models\" by Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, Furu Wei, and Jimmy Lin,"], "TTFT": 13.682081937789917, "answer": "[\"RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large Language Models\", \"Fine-Tuning LLaMA for Multi-Stage Text Retrieval\", \"Improving Text Embeddings with Large Language Models\"]", "f1 socre": 0.3225806451612903}
{"id": "30a2535f-7e73-46ad-a402-ee80ac6e4f92", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"Quantum Annealers Chain Strengths: A Simple Heuristic to Set Them All\" by Valentin Gilbert and St\u00e9phane Louise, 2024, arXiv: 2404.05443 [39]],\n[\"Benchmarking Quantum Annealers with Near-Optimal Minor Embeddings\" by Valentin Gilbert"], "TTFT": 16.3840811252594, "answer": "[\"Scaling Advantage in Approximate Optimization with Quantum Annealing\", \"4-clique Network Minor Embedding for Quantum Annealers\", \"Quantum Annealers Chain Strengths: A Simple Heuristic to Set Them All\", \"Benchmarking Quantum Annealers with Near-Optimal Minor-Embedded Instances\"]", "f1 socre": 0.40579710144927533}
{"id": "68e88dcc-6dec-4bb4-b51b-1fe8d6cd856d", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"LGDE: Local Graph-based Dictionary Expansion\" by Schindler et al., 2023]\n[\"Query2doc: Query Expansion with Large Language Models\" by Wang, Yang, and Wei, 2023]\n[\"From free text to clusters: A graph-based approach to text mining and information retrieval\" by Raghav"], "TTFT": 10.241957187652588, "answer": "[\"Query2doc: Query Expansion with Large Language Models\", \"Query Expansion by Prompting Large Language Models\", \"LGDE: Local Graph-based Dictionary Expansion\"]", "f1 socre": 0.32989690721649484}
{"id": "d7adf528-91a9-4978-a25c-29e61562a3be", "pred": ["{{\"Reference\": [\"The Pitfalls of Defining Hallucination\", \"Hallucination is Inevitable: An Innate Limitation of Large Language Models\"],\n\"Citation\": [\"Hallucination is Inevitable: An Innate Limitation of Large Language Models\", \"The Pitfalls of Defining Hallucination\"]}}\n\nIn this paper, the authors show that hallucination is inevitable for large language models (LLMs"], "TTFT": 11.103811502456665, "answer": "{\"Reference\": [\"The Pitfalls of Defining Hallucination\"], \"Citation\": [\"An ASSESSMENT OF MODEL-ON-MODEL DECEPTION\"]}", "f1 socre": 0.29333333333333333}
{"id": "949f3450-9b30-4556-be5f-ca8afe9e072f", "pred": ["{\n\"Title\": \"LLamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource Language\",\n\"Authors\": [\"Cagri Toraman\"],\n\"Reference\": [\n{\"Title\": \"BELLE: A Large-Scale Chinese Instruction Tuning Dataset for Pretrained Language Models\"},\n{\"Title\": \"Bactrain-X: A Multilingual Instruction Tuning Dataset for Low-Resource L"], "TTFT": 12.38475251197815, "answer": "{\"Reference\": [], \"Citation\": [\"LlamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource Language\", \"InstructionCP: A fast approach to transfer Large Language Models into target language\"]}", "f1 socre": 0.43010752688172044}
{"id": "8878fb61-6996-4984-9774-5ce9fb506d37", "pred": ["{\n\"Title\": \"Extreme Compression of Large Language Models via Additive Quantization\",\n\"Authors\": [\"Vage Egiazarian\", \"* 12\", \"Andrei Panferov\", \"* 12\", \"Denis Kuznedelev\", \"* 23\", \"Elias Frantar\", \"* 4\", \"Artem Babenko\", \"* 2\"],\n\"Reference\": [\n{\"Title\": \"Mistral"], "TTFT": 11.69446086883545, "answer": "{\"Reference\": [\"Mistral 7B\"], \"Citation\": [\"Sparse Expansion and Neuronal Disentanglement\"]}", "f1 socre": 0.10526315789473684}
{"id": "17539923-dd06-4712-becd-68fb8d68614d", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"Context-Aware Learning to Rank with Self-Attention\" by Przemys\u0142aw Pobrotyn, Tomasz Bartczak, Miko\u0142aj Synowiec, Rados\u0142aw Bia\u0142obrzeski, and Jaros\u0142aw Bojar]\n\ncites\n\n[\"Learning a ranking model with delta features to study the effect of neighborhood on item preference in e"], "TTFT": 6.013138055801392, "answer": "[\"Influence of Neighborhood on the Preference of an Item in eCommerce Search\", \"Context-Aware Learning to Rank with Self-Attention\"]", "f1 socre": 0.31249999999999994}
{"id": "ad6d0a53-1f8a-46b5-9738-a34d0a3cafaf", "pred": ["Based on the provided instructions and the given papers, the longest citation chain is as follows:\n\n[\"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis\" by Mena, C. Gary et al. (2019), \"Churn prediction using deep convolutional neural networks and autoencoders\" by Wangperawong, A. et al. (2016)]\n\nExplanation:\n\nThe"], "TTFT": 3.672146797180176, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\"]", "f1 socre": 0.4000000000000001}
{"id": "b5b963fe-30ab-435f-9574-dcdfd96f3707", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Minimod: A Finite Difference solver for Seismic Modeling\" by Jie Meng, Andreas Atle, and Henri Calandra, 2020]\n[\"Massively scalable stencil algorithm for seismic modeling on the Cerebras WSE-2\" by Mathias Jacquelin, Mauricio Araya-Polo,"], "TTFT": 8.336519241333008, "answer": "[\"Minimod: A Finite Difference solver for Seismic Modeling\", \"Massively scalable stencil algorithm\"]", "f1 socre": 0.3368421052631579}
{"id": "7b104919-2fba-44d2-91dd-a56bd2f30e91", "pred": ["{{\"Reference\": [\"LLM A*: Human in the Loop Large Language Models Enabled A* Search for Robotics\"], \"Citation\": [\"FunSearch\", \"Various papers on search-based MAPF methods (e.g., CBS, ECBS, EECBS)\"]}}\n\nThe paper \"Evolution of Heuristics: Towards Efficient Automatic Algorithm Design Using Large Language Model\" references the paper \"LLM A*: Human in"], "TTFT": 12.563227891921997, "answer": "{\"Reference\": [\"LLM A*: Human in the Loop Large Language Models Enabled A* Search for Robotics\"], \"Citation\": [\"Ensembling Prioritized Hybrid Policies for Multi-agent Pathfinding\"]}", "f1 socre": 0.5}
{"id": "31ee8539-2e9e-40e2-adc3-082c872698f7", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\" by Niimi et al., 2024]\n\ncites\n\n[\"A PRE"], "TTFT": 11.793168783187866, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\", \"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\", \"An Efficient Multimodal Learning Framework to Comprehend Consumer Preferences Using BERT and Cross-Attention\"]", "f1 socre": 0.39682539682539686}
{"id": "9b982095-daad-4632-9573-b0d7037dd6a7", "pred": ["{{\"Reference\": [\"A Prompt for Investigating the Effectiveness of LLMs in Self-Critiquing for Planning: An Empirical Study\", \"Large Language Models Cannot Self-Correct Reasoning Intrinsically\"],\n\"Citation\": [\"LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks\"]}}"], "TTFT": 10.443064212799072, "answer": "{\"Reference\": [\"LARGE LANGUAGE MoDELS CANNOT SELF-CorRECT REASONING YET\", \"Can Large Language Models Really Improve by Self-critiquing Their Own Plans?\"], \"Citation\": []}", "f1 socre": 0.3943661971830986}
{"id": "ecded47a-3741-4458-ba61-c04271c33651", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"MonaCoBERT: Monotonic attention based ConvBERT for Knowledge Tracing\" by Unggi Lee, Yonghyun Park, Yujin Kim, Seongyune Choi, and Hyeoncheol Kim, 2021]\n\nThis paper cites:\n\n[\"Understanding the Instability of Transformer-based Mod"], "TTFT": 10.399053812026978, "answer": "[\"Very Deep Transformers for Neural Machine Translation\", \"Understanding the Difficulty of Training Transformers\", \"MonaCoBERT: Monotonic attention based ConvBERT for Knowledge Tracing\"]", "f1 socre": 0.29411764705882354}
{"id": "71d0fb39-d315-4e33-beec-c53271663f7f", "pred": ["{{\"Reference\": [\"LoRA-Switch: Boosting the Efficiency of Dynamic LLM Adapters via System-Algorithm Co-design\"], \"Citation\": []}}"], "TTFT": 13.256085872650146, "answer": "{\"Reference\": [\"ONLINE SPECULATIVE DECODING\"], \"Citation\": [\"LoRA-Switch: Boosting the Efficiency of Dynamic LLM Adapters via System-Algorithm Co-design\"]}", "f1 socre": 0.9019607843137255}
{"id": "5d482043-88d2-4246-8191-e603b7bf6ed4", "pred": ["{\n\"Title\": \"Scalable Pre-training of Large Autoregressive Image Models\",\n\"Authors\": [\"Alaaeldin El-Nouby\", \"Ali Farhat\", \"Mohamed Ragab\", \"Mostafa Dehghani\", \"Georg Heigold\", \"Chen Sun\", \"Mario Lu\u010di\u0107\", \"Cordelia Schmid\"],\n\"Reference\": [\n{\"Title\": \"Vivit"], "TTFT": 15.165497541427612, "answer": "{\"Reference\": [\"Data Filtering Networks\"], \"Citation\": [\"ARVideo: Autoregressive Pretraining for Self-Supervised Video Representation Learning\"]}", "f1 socre": 0.17073170731707318}
{"id": "13a4a371-6339-4c9d-82cf-fc9ab2bb017d", "pred": ["{\n\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"],\n\"Citation\": []\n}\n\nThe paper \"OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models\" does not mention \"TinyLlama: An Open-Source Small Language Model\" in its text, so there is no citation relationship between the two papers. However, \"OpenMoE\" references \"Tiny"], "TTFT": 13.604034900665283, "answer": "{\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"], \"Citation\": [\"Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training\"]}", "f1 socre": 0.4047619047619047}
{"id": "c4a4e642-fa00-4377-a756-40f2f7ea7a74", "pred": ["{{\"Reference\": [\"A Survey of Hallucination in Large Foundation Models by Vipula Rawte, Amit Sheth, and Amitava Das\"], \"Citation\": [\"CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation by Quan Tu, Shilong Fan, Zihang Tian, Rui Yan\"]}}\n\nThis paper on CharacterEval references the survey paper on hallucination in large foundation models and"], "TTFT": 11.539960622787476, "answer": "{\"Reference\": [\"A Survey of Hallucination in \\\"Large\\\" Foundation Models\"], \"Citation\": [\"From Role-Play to Drama-Interaction: An LLM Solution\"]}", "f1 socre": 0.2745098039215686}
{"id": "835a4005-8c32-4083-b52d-4e014d1d35f5", "pred": ["Based on the provided instructions and papers, here is the longest citation chain:\n\n[\"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\" by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed, \"SUPERB: Speech processing Universal"], "TTFT": 11.292267322540283, "answer": "[\"CONTRASTIVE SEMI-SUPERVISED LEARNING FOR ASR\", \"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\", \"SUPERB: Speech processing Universal PERformance Benchmark\"]", "f1 socre": 0.38461538461538464}
{"id": "cd618a43-7176-432d-b21e-432e5c2ca4a7", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\" by Abien Fred M. Agarap, 2018, ICMLC 2018]\n\n[\"A Novel Approach Combining Recurrent Neural Network and"], "TTFT": 6.193939208984375, "answer": "[\"Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems\", \"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\"]", "f1 socre": 0.34374999999999994}
{"id": "e904afd3-5cb9-4bf7-bbee-9c8defac858c", "pred": ["Based on the provided instructions and papers, the longest citation chain can be constructed as follows:\n\n[\"On the performativity of SDG classifications in large bibliometric databases\" by Ottaviani and Stahlschmidt (2024), \"Language models are unsupervised multitask learners\" by Radford et al. (2019), \"DistilBERT: Distilling BERT: Six-Order Distillation for a Fast and Che"], "TTFT": 7.292623519897461, "answer": "[\"Contrastive Decoding: Open-ended Text Generation as Optimization\", \"An Empirical Study On Contrastive Search And Contrastive Decoding For Open-ended Text Generation\", \"On the performativity of SDG classifications in large bibliometric databases\"]", "f1 socre": 0.380952380952381}
{"id": "683f4cfd-1cfe-475d-927a-08a71d4f69e9", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\" by Niimi et al., 2024]\n\n[\"Churn Prediction with"], "TTFT": 11.674830198287964, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\", \"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\", \"An Efficient Multimodal Learning Framework to Comprehend Consumer Preferences Using BERT and Cross-Attention\"]", "f1 socre": 0.45312500000000006}
{"id": "0223faf0-f46f-459d-b1cd-45c4dab78fc2", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"A tutorial on the range variant of asymmetric numeral systems\" by James Townsend, \"Asymmetric numeral systems\" by Jarek Duda, \"Asymmetric numeral systems: entropy coding combining speed of Huffman coding with compression rate of arithmetic coding\" by Jarek Duda, Khalid Tahboub, Neeraj J. Gad"], "TTFT": 14.098109483718872, "answer": "[\"Lightweight compression with encryption based on Asymmetric Numeral Systems\", \"A tutorial on the range variant of asymmetric numeral systems\", \"ATC: an Advanced Tucker Compression library for multidimensional data\"]", "f1 socre": 0.3652173913043478}
{"id": "b7405d8e-13f0-4750-a25d-df3a67ffd39a", "pred": ["{{\"Reference\": [\"Benchmarking Large Language Models in Retrieval-Augmented Generation\", \"Retrieval-Augmented Generation in Question Answering: A Survey\"],\n\"Citation\": [\"Seven Failure Points When Engineering a Retrieval Augmented Generation System\"]}}\n\nThis paper presents the lessons learned and failure points from three case studies of implementing Retrieval-Augmented Generation (RAG) systems. It also discusses"], "TTFT": 10.79617714881897, "answer": "{\"Reference\": [\"Benchmarking Large Language Models in Retrieval-Augmented Generation\"], \"Citation\": [\"When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively\"]}", "f1 socre": 0.4186046511627907}
{"id": "150529cd-8767-477e-aa2b-28bae4970fdd", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System\" by Santos et al., \"Massively scalable stencil algorithm for the 3D wave equation on the Cerebras Wafer-Scale Engine\" by Jacquelin and Araya-Polo]\n\nThe reason for this citation chain is that the paper by Sant"], "TTFT": 15.363740921020508, "answer": "[\"Massively scalable stencil algorithm\", \"Disruptive Changes in Field Equation Modeling A Simple Interface for Wafer Scale Engines\", \"Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System\"]", "f1 socre": 0.39285714285714285}
{"id": "a8fa7aa1-2198-419b-a0de-6ba4d1c1f160", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Dance Any Beat: Blending Beats with Visuals in Dance Video Generation\" by Xuanchen Wang, Heng Wang, Weidong Cai, and Dongnan Liu],\n[\"InMoDeGAN: Interpretable Motion Decomposing Generative Adversarial Network for Video Generation\" by Yaohui Wang, Francois Bremond,"], "TTFT": 14.07155442237854, "answer": "[\"InMoDeGAN: Interpretable Motion Decomposition Generative Adversarial Network for Video Generation\", \"LATENT IMAGE ANIMATOR: <br> LEARNING TO ANIMATE IMAGES VIA LATENT SPACE NAVIGATION\", \"Dance Any Beat: Blending Beats with Visuals in Dance Video Generation\"]", "f1 socre": 0.43548387096774194}
{"id": "49680a79-a974-49b0-b7e3-85ff515bc4f2", "pred": ["{{\"Reference\": [\"Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding\"], \"Citation\": [\"Smurfs: Leveraging Multiple Proficiency Agents for Effective Tool Utilization in LLMs\" ]}}\n\nThe paper \"EasyTool: Enhancing LLM-based Agents with Concise Tool Instruction\" references the paper \"Don't Fine-Tune, Dec"], "TTFT": 10.98206639289856, "answer": "{\"Reference\": [\"Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding\"], \"Citation\": [\"Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning\"]}", "f1 socre": 0.6521739130434783}
{"id": "9f29f552-c817-4c2a-8910-b5e8ae7c014f", "pred": ["[\"Fine-Tuning LLaMA for Multi-Stage Text Retrieval\" by Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin, 2023, cited by \"Improving Text Embeddings with Large Language Models\" by Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, Furu Wei, and Jimmy Lin,"], "TTFT": 13.682081937789917, "answer": "[\"RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large Language Models\", \"Fine-Tuning LLaMA for Multi-Stage Text Retrieval\", \"Improving Text Embeddings with Large Language Models\"]", "f1 socre": 0.3225806451612903}
{"id": "30a2535f-7e73-46ad-a402-ee80ac6e4f92", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"Quantum Annealers Chain Strengths: A Simple Heuristic to Set Them All\" by Valentin Gilbert and St\u00e9phane Louise, 2024, arXiv: 2404.05443 [39]],\n[\"Benchmarking Quantum Annealers with Near-Optimal Minor Embeddings\" by Valentin Gilbert"], "TTFT": 16.3840811252594, "answer": "[\"Scaling Advantage in Approximate Optimization with Quantum Annealing\", \"4-clique Network Minor Embedding for Quantum Annealers\", \"Quantum Annealers Chain Strengths: A Simple Heuristic to Set Them All\", \"Benchmarking Quantum Annealers with Near-Optimal Minor-Embedded Instances\"]", "f1 socre": 0.40579710144927533}
{"id": "68e88dcc-6dec-4bb4-b51b-1fe8d6cd856d", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"LGDE: Local Graph-based Dictionary Expansion\" by Schindler et al., 2023]\n[\"Query2doc: Query Expansion with Large Language Models\" by Wang, Yang, and Wei, 2023]\n[\"From free text to clusters: A graph-based approach to text mining and information retrieval\" by Raghav"], "TTFT": 10.241957187652588, "answer": "[\"Query2doc: Query Expansion with Large Language Models\", \"Query Expansion by Prompting Large Language Models\", \"LGDE: Local Graph-based Dictionary Expansion\"]", "f1 socre": 0.32989690721649484}
{"id": "d7adf528-91a9-4978-a25c-29e61562a3be", "pred": ["{{\"Reference\": [\"The Pitfalls of Defining Hallucination\", \"Hallucination is Inevitable: An Innate Limitation of Large Language Models\"],\n\"Citation\": [\"Hallucination is Inevitable: An Innate Limitation of Large Language Models\", \"The Pitfalls of Defining Hallucination\"]}}\n\nIn this paper, the authors show that hallucination is inevitable for large language models (LLMs"], "TTFT": 11.103811502456665, "answer": "{\"Reference\": [\"The Pitfalls of Defining Hallucination\"], \"Citation\": [\"An ASSESSMENT OF MODEL-ON-MODEL DECEPTION\"]}", "f1 socre": 0.29333333333333333}
{"id": "949f3450-9b30-4556-be5f-ca8afe9e072f", "pred": ["{\n\"Title\": \"LLamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource Language\",\n\"Authors\": [\"Cagri Toraman\"],\n\"Reference\": [\n{\"Title\": \"BELLE: A Large-Scale Chinese Instruction Tuning Dataset for Pretrained Language Models\"},\n{\"Title\": \"Bactrain-X: A Multilingual Instruction Tuning Dataset for Low-Resource L"], "TTFT": 12.38475251197815, "answer": "{\"Reference\": [], \"Citation\": [\"LlamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource Language\", \"InstructionCP: A fast approach to transfer Large Language Models into target language\"]}", "f1 socre": 0.43010752688172044}
{"id": "8878fb61-6996-4984-9774-5ce9fb506d37", "pred": ["{\n\"Title\": \"Extreme Compression of Large Language Models via Additive Quantization\",\n\"Authors\": [\"Vage Egiazarian\", \"* 12\", \"Andrei Panferov\", \"* 12\", \"Denis Kuznedelev\", \"* 23\", \"Elias Frantar\", \"* 4\", \"Artem Babenko\", \"* 2\"],\n\"Reference\": [\n{\"Title\": \"Mistral"], "TTFT": 11.69446086883545, "answer": "{\"Reference\": [\"Mistral 7B\"], \"Citation\": [\"Sparse Expansion and Neuronal Disentanglement\"]}", "f1 socre": 0.10526315789473684}
{"id": "17539923-dd06-4712-becd-68fb8d68614d", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"Context-Aware Learning to Rank with Self-Attention\" by Przemys\u0142aw Pobrotyn, Tomasz Bartczak, Miko\u0142aj Synowiec, Rados\u0142aw Bia\u0142obrzeski, and Jaros\u0142aw Bojar]\n\ncites\n\n[\"Learning a ranking model with delta features to study the effect of neighborhood on item preference in e"], "TTFT": 6.013138055801392, "answer": "[\"Influence of Neighborhood on the Preference of an Item in eCommerce Search\", \"Context-Aware Learning to Rank with Self-Attention\"]", "f1 socre": 0.31249999999999994}
{"id": "ad6d0a53-1f8a-46b5-9738-a34d0a3cafaf", "pred": ["Based on the provided instructions and the given papers, the longest citation chain is as follows:\n\n[\"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis\" by Mena, C. Gary et al. (2019), \"Churn prediction using deep convolutional neural networks and autoencoders\" by Wangperawong, A. et al. (2016)]\n\nExplanation:\n\nThe"], "TTFT": 3.672146797180176, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\"]", "f1 socre": 0.4000000000000001}
{"id": "b5b963fe-30ab-435f-9574-dcdfd96f3707", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Minimod: A Finite Difference solver for Seismic Modeling\" by Jie Meng, Andreas Atle, and Henri Calandra, 2020]\n[\"Massively scalable stencil algorithm for seismic modeling on the Cerebras WSE-2\" by Mathias Jacquelin, Mauricio Araya-Polo,"], "TTFT": 8.336519241333008, "answer": "[\"Minimod: A Finite Difference solver for Seismic Modeling\", \"Massively scalable stencil algorithm\"]", "f1 socre": 0.3368421052631579}
{"id": "7b104919-2fba-44d2-91dd-a56bd2f30e91", "pred": ["{{\"Reference\": [\"LLM A*: Human in the Loop Large Language Models Enabled A* Search for Robotics\"], \"Citation\": [\"FunSearch\", \"Various papers on search-based MAPF methods (e.g., CBS, ECBS, EECBS)\"]}}\n\nThe paper \"Evolution of Heuristics: Towards Efficient Automatic Algorithm Design Using Large Language Model\" references the paper \"LLM A*: Human in"], "TTFT": 12.563227891921997, "answer": "{\"Reference\": [\"LLM A*: Human in the Loop Large Language Models Enabled A* Search for Robotics\"], \"Citation\": [\"Ensembling Prioritized Hybrid Policies for Multi-agent Pathfinding\"]}", "f1 socre": 0.5}
{"id": "31ee8539-2e9e-40e2-adc3-082c872698f7", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\" by Niimi et al., 2024]\n\ncites\n\n[\"A PRE"], "TTFT": 11.793168783187866, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\", \"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\", \"An Efficient Multimodal Learning Framework to Comprehend Consumer Preferences Using BERT and Cross-Attention\"]", "f1 socre": 0.39682539682539686}
{"id": "9b982095-daad-4632-9573-b0d7037dd6a7", "pred": ["{{\"Reference\": [\"A Prompt for Investigating the Effectiveness of LLMs in Self-Critiquing for Planning: An Empirical Study\", \"Large Language Models Cannot Self-Correct Reasoning Intrinsically\"],\n\"Citation\": [\"LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks\"]}}"], "TTFT": 10.443064212799072, "answer": "{\"Reference\": [\"LARGE LANGUAGE MoDELS CANNOT SELF-CorRECT REASONING YET\", \"Can Large Language Models Really Improve by Self-critiquing Their Own Plans?\"], \"Citation\": []}", "f1 socre": 0.3943661971830986}
{"id": "ecded47a-3741-4458-ba61-c04271c33651", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"MonaCoBERT: Monotonic attention based ConvBERT for Knowledge Tracing\" by Unggi Lee, Yonghyun Park, Yujin Kim, Seongyune Choi, and Hyeoncheol Kim, 2021]\n\nThis paper cites:\n\n[\"Understanding the Instability of Transformer-based Mod"], "TTFT": 10.399053812026978, "answer": "[\"Very Deep Transformers for Neural Machine Translation\", \"Understanding the Difficulty of Training Transformers\", \"MonaCoBERT: Monotonic attention based ConvBERT for Knowledge Tracing\"]", "f1 socre": 0.29411764705882354}
{"id": "71d0fb39-d315-4e33-beec-c53271663f7f", "pred": ["{{\"Reference\": [\"LoRA-Switch: Boosting the Efficiency of Dynamic LLM Adapters via System-Algorithm Co-design\"], \"Citation\": []}}"], "TTFT": 13.256085872650146, "answer": "{\"Reference\": [\"ONLINE SPECULATIVE DECODING\"], \"Citation\": [\"LoRA-Switch: Boosting the Efficiency of Dynamic LLM Adapters via System-Algorithm Co-design\"]}", "f1 socre": 0.9019607843137255}
{"id": "5d482043-88d2-4246-8191-e603b7bf6ed4", "pred": ["{\n\"Title\": \"Scalable Pre-training of Large Autoregressive Image Models\",\n\"Authors\": [\"Alaaeldin El-Nouby\", \"Ali Farhat\", \"Mohamed Ragab\", \"Mostafa Dehghani\", \"Georg Heigold\", \"Chen Sun\", \"Mario Lu\u010di\u0107\", \"Cordelia Schmid\"],\n\"Reference\": [\n{\"Title\": \"Vivit"], "TTFT": 15.165497541427612, "answer": "{\"Reference\": [\"Data Filtering Networks\"], \"Citation\": [\"ARVideo: Autoregressive Pretraining for Self-Supervised Video Representation Learning\"]}", "f1 socre": 0.17073170731707318}
{"id": "014ae33b-22e7-4fb7-ae34-56f64160ce74", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n1. [\"DYNAP-SE2: a scalable multi-core dynamic neuromorphic asynchronous spiking neural network processor\" by Ole Richter, et al.]\n2. [\"Speck: A Smart event-based Vision Sensor with a low latency 327K Neuron Convolutional Neural Network Processing Pipeline\" by Ole Richter,"], "TTFT": 15.628410577774048, "answer": "[\"Speck: A Smart event-based Vision Sensor with a low latency 327K Neuron Convolutional Neuronal Network Processing Pipeline\", \"DYNAP-SE2: a scalable multi-core dynamic neuromorphic asynchronous spiking neural network processor\"]", "f1 socre": 0.7543859649122807}
{"id": "13a4a371-6339-4c9d-82cf-fc9ab2bb017d", "pred": ["{\n\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"],\n\"Citation\": []\n}\n\nThe paper \"OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models\" does not mention \"TinyLlama: An Open-Source Small Language Model\" in its text, so there is no citation relationship between the two papers. However, \"OpenMoE\" references \"Tiny"], "TTFT": 13.604034900665283, "answer": "{\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"], \"Citation\": [\"Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training\"]}", "f1 socre": 0.4047619047619047}
{"id": "c4a4e642-fa00-4377-a756-40f2f7ea7a74", "pred": ["{{\"Reference\": [\"A Survey of Hallucination in Large Foundation Models by Vipula Rawte, Amit Sheth, and Amitava Das\"], \"Citation\": [\"CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation by Quan Tu, Shilong Fan, Zihang Tian, Rui Yan\"]}}\n\nThis paper on CharacterEval references the survey paper on hallucination in large foundation models and"], "TTFT": 11.539960622787476, "answer": "{\"Reference\": [\"A Survey of Hallucination in \\\"Large\\\" Foundation Models\"], \"Citation\": [\"From Role-Play to Drama-Interaction: An LLM Solution\"]}", "f1 socre": 0.2745098039215686}
{"id": "835a4005-8c32-4083-b52d-4e014d1d35f5", "pred": ["Based on the provided instructions and papers, here is the longest citation chain:\n\n[\"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\" by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed, \"SUPERB: Speech processing Universal"], "TTFT": 11.292267322540283, "answer": "[\"CONTRASTIVE SEMI-SUPERVISED LEARNING FOR ASR\", \"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\", \"SUPERB: Speech processing Universal PERformance Benchmark\"]", "f1 socre": 0.38461538461538464}
{"id": "cd618a43-7176-432d-b21e-432e5c2ca4a7", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\" by Abien Fred M. Agarap, 2018, ICMLC 2018]\n\n[\"A Novel Approach Combining Recurrent Neural Network and"], "TTFT": 6.193939208984375, "answer": "[\"Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems\", \"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\"]", "f1 socre": 0.34374999999999994}
{"id": "e904afd3-5cb9-4bf7-bbee-9c8defac858c", "pred": ["Based on the provided instructions and papers, the longest citation chain can be constructed as follows:\n\n[\"On the performativity of SDG classifications in large bibliometric databases\" by Ottaviani and Stahlschmidt (2024), \"Language models are unsupervised multitask learners\" by Radford et al. (2019), \"DistilBERT: Distilling BERT: Six-Order Distillation for a Fast and Che"], "TTFT": 7.292623519897461, "answer": "[\"Contrastive Decoding: Open-ended Text Generation as Optimization\", \"An Empirical Study On Contrastive Search And Contrastive Decoding For Open-ended Text Generation\", \"On the performativity of SDG classifications in large bibliometric databases\"]", "f1 socre": 0.380952380952381}
{"id": "683f4cfd-1cfe-475d-927a-08a71d4f69e9", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\" by Niimi et al., 2024]\n\n[\"Churn Prediction with"], "TTFT": 11.674830198287964, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\", \"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\", \"An Efficient Multimodal Learning Framework to Comprehend Consumer Preferences Using BERT and Cross-Attention\"]", "f1 socre": 0.45312500000000006}
{"id": "0223faf0-f46f-459d-b1cd-45c4dab78fc2", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"A tutorial on the range variant of asymmetric numeral systems\" by James Townsend, \"Asymmetric numeral systems\" by Jarek Duda, \"Asymmetric numeral systems: entropy coding combining speed of Huffman coding with compression rate of arithmetic coding\" by Jarek Duda, Khalid Tahboub, Neeraj J. Gad"], "TTFT": 14.098109483718872, "answer": "[\"Lightweight compression with encryption based on Asymmetric Numeral Systems\", \"A tutorial on the range variant of asymmetric numeral systems\", \"ATC: an Advanced Tucker Compression library for multidimensional data\"]", "f1 socre": 0.3652173913043478}
{"id": "b7405d8e-13f0-4750-a25d-df3a67ffd39a", "pred": ["{{\"Reference\": [\"Benchmarking Large Language Models in Retrieval-Augmented Generation\", \"Retrieval-Augmented Generation in Question Answering: A Survey\"],\n\"Citation\": [\"Seven Failure Points When Engineering a Retrieval Augmented Generation System\"]}}\n\nThis paper presents the lessons learned and failure points from three case studies of implementing Retrieval-Augmented Generation (RAG) systems. It also discusses"], "TTFT": 10.79617714881897, "answer": "{\"Reference\": [\"Benchmarking Large Language Models in Retrieval-Augmented Generation\"], \"Citation\": [\"When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively\"]}", "f1 socre": 0.4186046511627907}
{"id": "150529cd-8767-477e-aa2b-28bae4970fdd", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System\" by Santos et al., \"Massively scalable stencil algorithm for the 3D wave equation on the Cerebras Wafer-Scale Engine\" by Jacquelin and Araya-Polo]\n\nThe reason for this citation chain is that the paper by Sant"], "TTFT": 15.363740921020508, "answer": "[\"Massively scalable stencil algorithm\", \"Disruptive Changes in Field Equation Modeling A Simple Interface for Wafer Scale Engines\", \"Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System\"]", "f1 socre": 0.39285714285714285}
{"id": "a8fa7aa1-2198-419b-a0de-6ba4d1c1f160", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Dance Any Beat: Blending Beats with Visuals in Dance Video Generation\" by Xuanchen Wang, Heng Wang, Weidong Cai, and Dongnan Liu],\n[\"InMoDeGAN: Interpretable Motion Decomposing Generative Adversarial Network for Video Generation\" by Yaohui Wang, Francois Bremond,"], "TTFT": 14.07155442237854, "answer": "[\"InMoDeGAN: Interpretable Motion Decomposition Generative Adversarial Network for Video Generation\", \"LATENT IMAGE ANIMATOR: <br> LEARNING TO ANIMATE IMAGES VIA LATENT SPACE NAVIGATION\", \"Dance Any Beat: Blending Beats with Visuals in Dance Video Generation\"]", "f1 socre": 0.43548387096774194}
{"id": "49680a79-a974-49b0-b7e3-85ff515bc4f2", "pred": ["{{\"Reference\": [\"Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding\"], \"Citation\": [\"Smurfs: Leveraging Multiple Proficiency Agents for Effective Tool Utilization in LLMs\" ]}}\n\nThe paper \"EasyTool: Enhancing LLM-based Agents with Concise Tool Instruction\" references the paper \"Don't Fine-Tune, Dec"], "TTFT": 10.98206639289856, "answer": "{\"Reference\": [\"Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding\"], \"Citation\": [\"Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning\"]}", "f1 socre": 0.6521739130434783}
{"id": "9f29f552-c817-4c2a-8910-b5e8ae7c014f", "pred": ["[\"Fine-Tuning LLaMA for Multi-Stage Text Retrieval\" by Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin, 2023, cited by \"Improving Text Embeddings with Large Language Models\" by Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, Furu Wei, and Jimmy Lin,"], "TTFT": 13.682081937789917, "answer": "[\"RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large Language Models\", \"Fine-Tuning LLaMA for Multi-Stage Text Retrieval\", \"Improving Text Embeddings with Large Language Models\"]", "f1 socre": 0.3225806451612903}
{"id": "30a2535f-7e73-46ad-a402-ee80ac6e4f92", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"Quantum Annealers Chain Strengths: A Simple Heuristic to Set Them All\" by Valentin Gilbert and St\u00e9phane Louise, 2024, arXiv: 2404.05443 [39]],\n[\"Benchmarking Quantum Annealers with Near-Optimal Minor Embeddings\" by Valentin Gilbert"], "TTFT": 16.3840811252594, "answer": "[\"Scaling Advantage in Approximate Optimization with Quantum Annealing\", \"4-clique Network Minor Embedding for Quantum Annealers\", \"Quantum Annealers Chain Strengths: A Simple Heuristic to Set Them All\", \"Benchmarking Quantum Annealers with Near-Optimal Minor-Embedded Instances\"]", "f1 socre": 0.40579710144927533}
{"id": "68e88dcc-6dec-4bb4-b51b-1fe8d6cd856d", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"LGDE: Local Graph-based Dictionary Expansion\" by Schindler et al., 2023]\n[\"Query2doc: Query Expansion with Large Language Models\" by Wang, Yang, and Wei, 2023]\n[\"From free text to clusters: A graph-based approach to text mining and information retrieval\" by Raghav"], "TTFT": 10.241957187652588, "answer": "[\"Query2doc: Query Expansion with Large Language Models\", \"Query Expansion by Prompting Large Language Models\", \"LGDE: Local Graph-based Dictionary Expansion\"]", "f1 socre": 0.32989690721649484}
{"id": "d7adf528-91a9-4978-a25c-29e61562a3be", "pred": ["{{\"Reference\": [\"The Pitfalls of Defining Hallucination\", \"Hallucination is Inevitable: An Innate Limitation of Large Language Models\"],\n\"Citation\": [\"Hallucination is Inevitable: An Innate Limitation of Large Language Models\", \"The Pitfalls of Defining Hallucination\"]}}\n\nIn this paper, the authors show that hallucination is inevitable for large language models (LLMs"], "TTFT": 11.103811502456665, "answer": "{\"Reference\": [\"The Pitfalls of Defining Hallucination\"], \"Citation\": [\"An ASSESSMENT OF MODEL-ON-MODEL DECEPTION\"]}", "f1 socre": 0.29333333333333333}
{"id": "949f3450-9b30-4556-be5f-ca8afe9e072f", "pred": ["{\n\"Title\": \"LLamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource Language\",\n\"Authors\": [\"Cagri Toraman\"],\n\"Reference\": [\n{\"Title\": \"BELLE: A Large-Scale Chinese Instruction Tuning Dataset for Pretrained Language Models\"},\n{\"Title\": \"Bactrain-X: A Multilingual Instruction Tuning Dataset for Low-Resource L"], "TTFT": 12.38475251197815, "answer": "{\"Reference\": [], \"Citation\": [\"LlamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource Language\", \"InstructionCP: A fast approach to transfer Large Language Models into target language\"]}", "f1 socre": 0.43010752688172044}
{"id": "8878fb61-6996-4984-9774-5ce9fb506d37", "pred": ["{\n\"Title\": \"Extreme Compression of Large Language Models via Additive Quantization\",\n\"Authors\": [\"Vage Egiazarian\", \"* 12\", \"Andrei Panferov\", \"* 12\", \"Denis Kuznedelev\", \"* 23\", \"Elias Frantar\", \"* 4\", \"Artem Babenko\", \"* 2\"],\n\"Reference\": [\n{\"Title\": \"Mistral"], "TTFT": 11.69446086883545, "answer": "{\"Reference\": [\"Mistral 7B\"], \"Citation\": [\"Sparse Expansion and Neuronal Disentanglement\"]}", "f1 socre": 0.10526315789473684}
{"id": "17539923-dd06-4712-becd-68fb8d68614d", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"Context-Aware Learning to Rank with Self-Attention\" by Przemys\u0142aw Pobrotyn, Tomasz Bartczak, Miko\u0142aj Synowiec, Rados\u0142aw Bia\u0142obrzeski, and Jaros\u0142aw Bojar]\n\ncites\n\n[\"Learning a ranking model with delta features to study the effect of neighborhood on item preference in e"], "TTFT": 6.013138055801392, "answer": "[\"Influence of Neighborhood on the Preference of an Item in eCommerce Search\", \"Context-Aware Learning to Rank with Self-Attention\"]", "f1 socre": 0.31249999999999994}
{"id": "ad6d0a53-1f8a-46b5-9738-a34d0a3cafaf", "pred": ["Based on the provided instructions and the given papers, the longest citation chain is as follows:\n\n[\"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis\" by Mena, C. Gary et al. (2019), \"Churn prediction using deep convolutional neural networks and autoencoders\" by Wangperawong, A. et al. (2016)]\n\nExplanation:\n\nThe"], "TTFT": 3.672146797180176, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\"]", "f1 socre": 0.4000000000000001}
{"id": "b5b963fe-30ab-435f-9574-dcdfd96f3707", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Minimod: A Finite Difference solver for Seismic Modeling\" by Jie Meng, Andreas Atle, and Henri Calandra, 2020]\n[\"Massively scalable stencil algorithm for seismic modeling on the Cerebras WSE-2\" by Mathias Jacquelin, Mauricio Araya-Polo,"], "TTFT": 8.336519241333008, "answer": "[\"Minimod: A Finite Difference solver for Seismic Modeling\", \"Massively scalable stencil algorithm\"]", "f1 socre": 0.3368421052631579}
{"id": "7b104919-2fba-44d2-91dd-a56bd2f30e91", "pred": ["{{\"Reference\": [\"LLM A*: Human in the Loop Large Language Models Enabled A* Search for Robotics\"], \"Citation\": [\"FunSearch\", \"Various papers on search-based MAPF methods (e.g., CBS, ECBS, EECBS)\"]}}\n\nThe paper \"Evolution of Heuristics: Towards Efficient Automatic Algorithm Design Using Large Language Model\" references the paper \"LLM A*: Human in"], "TTFT": 12.563227891921997, "answer": "{\"Reference\": [\"LLM A*: Human in the Loop Large Language Models Enabled A* Search for Robotics\"], \"Citation\": [\"Ensembling Prioritized Hybrid Policies for Multi-agent Pathfinding\"]}", "f1 socre": 0.5}
{"id": "31ee8539-2e9e-40e2-adc3-082c872698f7", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\" by Niimi et al., 2024]\n\ncites\n\n[\"A PRE"], "TTFT": 11.793168783187866, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\", \"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\", \"An Efficient Multimodal Learning Framework to Comprehend Consumer Preferences Using BERT and Cross-Attention\"]", "f1 socre": 0.39682539682539686}
{"id": "9b982095-daad-4632-9573-b0d7037dd6a7", "pred": ["{{\"Reference\": [\"A Prompt for Investigating the Effectiveness of LLMs in Self-Critiquing for Planning: An Empirical Study\", \"Large Language Models Cannot Self-Correct Reasoning Intrinsically\"],\n\"Citation\": [\"LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks\"]}}"], "TTFT": 10.443064212799072, "answer": "{\"Reference\": [\"LARGE LANGUAGE MoDELS CANNOT SELF-CorRECT REASONING YET\", \"Can Large Language Models Really Improve by Self-critiquing Their Own Plans?\"], \"Citation\": []}", "f1 socre": 0.3943661971830986}
{"id": "ecded47a-3741-4458-ba61-c04271c33651", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"MonaCoBERT: Monotonic attention based ConvBERT for Knowledge Tracing\" by Unggi Lee, Yonghyun Park, Yujin Kim, Seongyune Choi, and Hyeoncheol Kim, 2021]\n\nThis paper cites:\n\n[\"Understanding the Instability of Transformer-based Mod"], "TTFT": 10.399053812026978, "answer": "[\"Very Deep Transformers for Neural Machine Translation\", \"Understanding the Difficulty of Training Transformers\", \"MonaCoBERT: Monotonic attention based ConvBERT for Knowledge Tracing\"]", "f1 socre": 0.29411764705882354}
{"id": "71d0fb39-d315-4e33-beec-c53271663f7f", "pred": ["{{\"Reference\": [\"LoRA-Switch: Boosting the Efficiency of Dynamic LLM Adapters via System-Algorithm Co-design\"], \"Citation\": []}}"], "TTFT": 13.256085872650146, "answer": "{\"Reference\": [\"ONLINE SPECULATIVE DECODING\"], \"Citation\": [\"LoRA-Switch: Boosting the Efficiency of Dynamic LLM Adapters via System-Algorithm Co-design\"]}", "f1 socre": 0.9019607843137255}
{"id": "5d482043-88d2-4246-8191-e603b7bf6ed4", "pred": ["{\n\"Title\": \"Scalable Pre-training of Large Autoregressive Image Models\",\n\"Authors\": [\"Alaaeldin El-Nouby\", \"Ali Farhat\", \"Mohamed Ragab\", \"Mostafa Dehghani\", \"Georg Heigold\", \"Chen Sun\", \"Mario Lu\u010di\u0107\", \"Cordelia Schmid\"],\n\"Reference\": [\n{\"Title\": \"Vivit"], "TTFT": 15.165497541427612, "answer": "{\"Reference\": [\"Data Filtering Networks\"], \"Citation\": [\"ARVideo: Autoregressive Pretraining for Self-Supervised Video Representation Learning\"]}", "f1 socre": 0.17073170731707318}
{"id": "014ae33b-22e7-4fb7-ae34-56f64160ce74", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n1. [\"DYNAP-SE2: a scalable multi-core dynamic neuromorphic asynchronous spiking neural network processor\" by Ole Richter, et al.]\n2. [\"Speck: A Smart event-based Vision Sensor with a low latency 327K Neuron Convolutional Neural Network Processing Pipeline\" by Ole Richter,"], "TTFT": 15.628410577774048, "answer": "[\"Speck: A Smart event-based Vision Sensor with a low latency 327K Neuron Convolutional Neuronal Network Processing Pipeline\", \"DYNAP-SE2: a scalable multi-core dynamic neuromorphic asynchronous spiking neural network processor\"]", "f1 socre": 0.7543859649122807}
{"id": "3e4ee654-dfd8-4ffd-a825-79e0997961c9", "pred": ["{{\"Reference\": [\"Navigating the Safety Landscape: Measuring LLM Cybersecurity Risks with VISAGE Metric\", \"Purple Llama CYBERSECEval: A Secure Coding Benchmark for Large Language Models\"],\n\"Citation\": [\"Navigating the Safety Landscape: Measuring LLM Cybersecurity Risks with VISAGE Metric\": \"studied the safety landscape of LLMs and discovered"], "TTFT": 12.062817811965942, "answer": "{\"Reference\": [\"Purple Llama CYBERSECEvAL: A Secure Coding Benchmark for Language Models\"], \"Citation\": [\"Navigating the Safety Landscape: Measuring Risks in Finetuning Large Language Models\"]}", "f1 socre": 0.5238095238095238}
{"id": "13a4a371-6339-4c9d-82cf-fc9ab2bb017d", "pred": ["{\n\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"],\n\"Citation\": []\n}\n\nThe paper \"OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models\" does not mention \"TinyLlama: An Open-Source Small Language Model\" in its text, so there is no citation relationship between the two papers. However, \"OpenMoE\" references \"Tiny"], "TTFT": 13.604034900665283, "answer": "{\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"], \"Citation\": [\"Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training\"]}", "f1 socre": 0.4047619047619047}
{"id": "c4a4e642-fa00-4377-a756-40f2f7ea7a74", "pred": ["{{\"Reference\": [\"A Survey of Hallucination in Large Foundation Models by Vipula Rawte, Amit Sheth, and Amitava Das\"], \"Citation\": [\"CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation by Quan Tu, Shilong Fan, Zihang Tian, Rui Yan\"]}}\n\nThis paper on CharacterEval references the survey paper on hallucination in large foundation models and"], "TTFT": 11.539960622787476, "answer": "{\"Reference\": [\"A Survey of Hallucination in \\\"Large\\\" Foundation Models\"], \"Citation\": [\"From Role-Play to Drama-Interaction: An LLM Solution\"]}", "f1 socre": 0.2745098039215686}
{"id": "835a4005-8c32-4083-b52d-4e014d1d35f5", "pred": ["Based on the provided instructions and papers, here is the longest citation chain:\n\n[\"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\" by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed, \"SUPERB: Speech processing Universal"], "TTFT": 11.292267322540283, "answer": "[\"CONTRASTIVE SEMI-SUPERVISED LEARNING FOR ASR\", \"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\", \"SUPERB: Speech processing Universal PERformance Benchmark\"]", "f1 socre": 0.38461538461538464}
{"id": "cd618a43-7176-432d-b21e-432e5c2ca4a7", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\" by Abien Fred M. Agarap, 2018, ICMLC 2018]\n\n[\"A Novel Approach Combining Recurrent Neural Network and"], "TTFT": 6.193939208984375, "answer": "[\"Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems\", \"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\"]", "f1 socre": 0.34374999999999994}
{"id": "e904afd3-5cb9-4bf7-bbee-9c8defac858c", "pred": ["Based on the provided instructions and papers, the longest citation chain can be constructed as follows:\n\n[\"On the performativity of SDG classifications in large bibliometric databases\" by Ottaviani and Stahlschmidt (2024), \"Language models are unsupervised multitask learners\" by Radford et al. (2019), \"DistilBERT: Distilling BERT: Six-Order Distillation for a Fast and Che"], "TTFT": 7.292623519897461, "answer": "[\"Contrastive Decoding: Open-ended Text Generation as Optimization\", \"An Empirical Study On Contrastive Search And Contrastive Decoding For Open-ended Text Generation\", \"On the performativity of SDG classifications in large bibliometric databases\"]", "f1 socre": 0.380952380952381}
{"id": "683f4cfd-1cfe-475d-927a-08a71d4f69e9", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\" by Niimi et al., 2024]\n\n[\"Churn Prediction with"], "TTFT": 11.674830198287964, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\", \"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\", \"An Efficient Multimodal Learning Framework to Comprehend Consumer Preferences Using BERT and Cross-Attention\"]", "f1 socre": 0.45312500000000006}
{"id": "0223faf0-f46f-459d-b1cd-45c4dab78fc2", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"A tutorial on the range variant of asymmetric numeral systems\" by James Townsend, \"Asymmetric numeral systems\" by Jarek Duda, \"Asymmetric numeral systems: entropy coding combining speed of Huffman coding with compression rate of arithmetic coding\" by Jarek Duda, Khalid Tahboub, Neeraj J. Gad"], "TTFT": 14.098109483718872, "answer": "[\"Lightweight compression with encryption based on Asymmetric Numeral Systems\", \"A tutorial on the range variant of asymmetric numeral systems\", \"ATC: an Advanced Tucker Compression library for multidimensional data\"]", "f1 socre": 0.3652173913043478}
{"id": "b7405d8e-13f0-4750-a25d-df3a67ffd39a", "pred": ["{{\"Reference\": [\"Benchmarking Large Language Models in Retrieval-Augmented Generation\", \"Retrieval-Augmented Generation in Question Answering: A Survey\"],\n\"Citation\": [\"Seven Failure Points When Engineering a Retrieval Augmented Generation System\"]}}\n\nThis paper presents the lessons learned and failure points from three case studies of implementing Retrieval-Augmented Generation (RAG) systems. It also discusses"], "TTFT": 10.79617714881897, "answer": "{\"Reference\": [\"Benchmarking Large Language Models in Retrieval-Augmented Generation\"], \"Citation\": [\"When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively\"]}", "f1 socre": 0.4186046511627907}
{"id": "150529cd-8767-477e-aa2b-28bae4970fdd", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System\" by Santos et al., \"Massively scalable stencil algorithm for the 3D wave equation on the Cerebras Wafer-Scale Engine\" by Jacquelin and Araya-Polo]\n\nThe reason for this citation chain is that the paper by Sant"], "TTFT": 15.363740921020508, "answer": "[\"Massively scalable stencil algorithm\", \"Disruptive Changes in Field Equation Modeling A Simple Interface for Wafer Scale Engines\", \"Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System\"]", "f1 socre": 0.39285714285714285}
{"id": "a8fa7aa1-2198-419b-a0de-6ba4d1c1f160", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Dance Any Beat: Blending Beats with Visuals in Dance Video Generation\" by Xuanchen Wang, Heng Wang, Weidong Cai, and Dongnan Liu],\n[\"InMoDeGAN: Interpretable Motion Decomposing Generative Adversarial Network for Video Generation\" by Yaohui Wang, Francois Bremond,"], "TTFT": 14.07155442237854, "answer": "[\"InMoDeGAN: Interpretable Motion Decomposition Generative Adversarial Network for Video Generation\", \"LATENT IMAGE ANIMATOR: <br> LEARNING TO ANIMATE IMAGES VIA LATENT SPACE NAVIGATION\", \"Dance Any Beat: Blending Beats with Visuals in Dance Video Generation\"]", "f1 socre": 0.43548387096774194}
{"id": "49680a79-a974-49b0-b7e3-85ff515bc4f2", "pred": ["{{\"Reference\": [\"Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding\"], \"Citation\": [\"Smurfs: Leveraging Multiple Proficiency Agents for Effective Tool Utilization in LLMs\" ]}}\n\nThe paper \"EasyTool: Enhancing LLM-based Agents with Concise Tool Instruction\" references the paper \"Don't Fine-Tune, Dec"], "TTFT": 10.98206639289856, "answer": "{\"Reference\": [\"Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding\"], \"Citation\": [\"Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning\"]}", "f1 socre": 0.6521739130434783}
{"id": "9f29f552-c817-4c2a-8910-b5e8ae7c014f", "pred": ["[\"Fine-Tuning LLaMA for Multi-Stage Text Retrieval\" by Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin, 2023, cited by \"Improving Text Embeddings with Large Language Models\" by Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, Furu Wei, and Jimmy Lin,"], "TTFT": 13.682081937789917, "answer": "[\"RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large Language Models\", \"Fine-Tuning LLaMA for Multi-Stage Text Retrieval\", \"Improving Text Embeddings with Large Language Models\"]", "f1 socre": 0.3225806451612903}
{"id": "30a2535f-7e73-46ad-a402-ee80ac6e4f92", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"Quantum Annealers Chain Strengths: A Simple Heuristic to Set Them All\" by Valentin Gilbert and St\u00e9phane Louise, 2024, arXiv: 2404.05443 [39]],\n[\"Benchmarking Quantum Annealers with Near-Optimal Minor Embeddings\" by Valentin Gilbert"], "TTFT": 16.3840811252594, "answer": "[\"Scaling Advantage in Approximate Optimization with Quantum Annealing\", \"4-clique Network Minor Embedding for Quantum Annealers\", \"Quantum Annealers Chain Strengths: A Simple Heuristic to Set Them All\", \"Benchmarking Quantum Annealers with Near-Optimal Minor-Embedded Instances\"]", "f1 socre": 0.40579710144927533}
{"id": "68e88dcc-6dec-4bb4-b51b-1fe8d6cd856d", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"LGDE: Local Graph-based Dictionary Expansion\" by Schindler et al., 2023]\n[\"Query2doc: Query Expansion with Large Language Models\" by Wang, Yang, and Wei, 2023]\n[\"From free text to clusters: A graph-based approach to text mining and information retrieval\" by Raghav"], "TTFT": 10.241957187652588, "answer": "[\"Query2doc: Query Expansion with Large Language Models\", \"Query Expansion by Prompting Large Language Models\", \"LGDE: Local Graph-based Dictionary Expansion\"]", "f1 socre": 0.32989690721649484}
{"id": "d7adf528-91a9-4978-a25c-29e61562a3be", "pred": ["{{\"Reference\": [\"The Pitfalls of Defining Hallucination\", \"Hallucination is Inevitable: An Innate Limitation of Large Language Models\"],\n\"Citation\": [\"Hallucination is Inevitable: An Innate Limitation of Large Language Models\", \"The Pitfalls of Defining Hallucination\"]}}\n\nIn this paper, the authors show that hallucination is inevitable for large language models (LLMs"], "TTFT": 11.103811502456665, "answer": "{\"Reference\": [\"The Pitfalls of Defining Hallucination\"], \"Citation\": [\"An ASSESSMENT OF MODEL-ON-MODEL DECEPTION\"]}", "f1 socre": 0.29333333333333333}
{"id": "949f3450-9b30-4556-be5f-ca8afe9e072f", "pred": ["{\n\"Title\": \"LLamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource Language\",\n\"Authors\": [\"Cagri Toraman\"],\n\"Reference\": [\n{\"Title\": \"BELLE: A Large-Scale Chinese Instruction Tuning Dataset for Pretrained Language Models\"},\n{\"Title\": \"Bactrain-X: A Multilingual Instruction Tuning Dataset for Low-Resource L"], "TTFT": 12.38475251197815, "answer": "{\"Reference\": [], \"Citation\": [\"LlamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource Language\", \"InstructionCP: A fast approach to transfer Large Language Models into target language\"]}", "f1 socre": 0.43010752688172044}
{"id": "8878fb61-6996-4984-9774-5ce9fb506d37", "pred": ["{\n\"Title\": \"Extreme Compression of Large Language Models via Additive Quantization\",\n\"Authors\": [\"Vage Egiazarian\", \"* 12\", \"Andrei Panferov\", \"* 12\", \"Denis Kuznedelev\", \"* 23\", \"Elias Frantar\", \"* 4\", \"Artem Babenko\", \"* 2\"],\n\"Reference\": [\n{\"Title\": \"Mistral"], "TTFT": 11.69446086883545, "answer": "{\"Reference\": [\"Mistral 7B\"], \"Citation\": [\"Sparse Expansion and Neuronal Disentanglement\"]}", "f1 socre": 0.10526315789473684}
{"id": "17539923-dd06-4712-becd-68fb8d68614d", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"Context-Aware Learning to Rank with Self-Attention\" by Przemys\u0142aw Pobrotyn, Tomasz Bartczak, Miko\u0142aj Synowiec, Rados\u0142aw Bia\u0142obrzeski, and Jaros\u0142aw Bojar]\n\ncites\n\n[\"Learning a ranking model with delta features to study the effect of neighborhood on item preference in e"], "TTFT": 6.013138055801392, "answer": "[\"Influence of Neighborhood on the Preference of an Item in eCommerce Search\", \"Context-Aware Learning to Rank with Self-Attention\"]", "f1 socre": 0.31249999999999994}
{"id": "ad6d0a53-1f8a-46b5-9738-a34d0a3cafaf", "pred": ["Based on the provided instructions and the given papers, the longest citation chain is as follows:\n\n[\"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis\" by Mena, C. Gary et al. (2019), \"Churn prediction using deep convolutional neural networks and autoencoders\" by Wangperawong, A. et al. (2016)]\n\nExplanation:\n\nThe"], "TTFT": 3.672146797180176, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\"]", "f1 socre": 0.4000000000000001}
{"id": "b5b963fe-30ab-435f-9574-dcdfd96f3707", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Minimod: A Finite Difference solver for Seismic Modeling\" by Jie Meng, Andreas Atle, and Henri Calandra, 2020]\n[\"Massively scalable stencil algorithm for seismic modeling on the Cerebras WSE-2\" by Mathias Jacquelin, Mauricio Araya-Polo,"], "TTFT": 8.336519241333008, "answer": "[\"Minimod: A Finite Difference solver for Seismic Modeling\", \"Massively scalable stencil algorithm\"]", "f1 socre": 0.3368421052631579}
{"id": "7b104919-2fba-44d2-91dd-a56bd2f30e91", "pred": ["{{\"Reference\": [\"LLM A*: Human in the Loop Large Language Models Enabled A* Search for Robotics\"], \"Citation\": [\"FunSearch\", \"Various papers on search-based MAPF methods (e.g., CBS, ECBS, EECBS)\"]}}\n\nThe paper \"Evolution of Heuristics: Towards Efficient Automatic Algorithm Design Using Large Language Model\" references the paper \"LLM A*: Human in"], "TTFT": 12.563227891921997, "answer": "{\"Reference\": [\"LLM A*: Human in the Loop Large Language Models Enabled A* Search for Robotics\"], \"Citation\": [\"Ensembling Prioritized Hybrid Policies for Multi-agent Pathfinding\"]}", "f1 socre": 0.5}
{"id": "31ee8539-2e9e-40e2-adc3-082c872698f7", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\" by Niimi et al., 2024]\n\ncites\n\n[\"A PRE"], "TTFT": 11.793168783187866, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\", \"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\", \"An Efficient Multimodal Learning Framework to Comprehend Consumer Preferences Using BERT and Cross-Attention\"]", "f1 socre": 0.39682539682539686}
{"id": "9b982095-daad-4632-9573-b0d7037dd6a7", "pred": ["{{\"Reference\": [\"A Prompt for Investigating the Effectiveness of LLMs in Self-Critiquing for Planning: An Empirical Study\", \"Large Language Models Cannot Self-Correct Reasoning Intrinsically\"],\n\"Citation\": [\"LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks\"]}}"], "TTFT": 10.443064212799072, "answer": "{\"Reference\": [\"LARGE LANGUAGE MoDELS CANNOT SELF-CorRECT REASONING YET\", \"Can Large Language Models Really Improve by Self-critiquing Their Own Plans?\"], \"Citation\": []}", "f1 socre": 0.3943661971830986}
{"id": "ecded47a-3741-4458-ba61-c04271c33651", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"MonaCoBERT: Monotonic attention based ConvBERT for Knowledge Tracing\" by Unggi Lee, Yonghyun Park, Yujin Kim, Seongyune Choi, and Hyeoncheol Kim, 2021]\n\nThis paper cites:\n\n[\"Understanding the Instability of Transformer-based Mod"], "TTFT": 10.399053812026978, "answer": "[\"Very Deep Transformers for Neural Machine Translation\", \"Understanding the Difficulty of Training Transformers\", \"MonaCoBERT: Monotonic attention based ConvBERT for Knowledge Tracing\"]", "f1 socre": 0.29411764705882354}
{"id": "71d0fb39-d315-4e33-beec-c53271663f7f", "pred": ["{{\"Reference\": [\"LoRA-Switch: Boosting the Efficiency of Dynamic LLM Adapters via System-Algorithm Co-design\"], \"Citation\": []}}"], "TTFT": 13.256085872650146, "answer": "{\"Reference\": [\"ONLINE SPECULATIVE DECODING\"], \"Citation\": [\"LoRA-Switch: Boosting the Efficiency of Dynamic LLM Adapters via System-Algorithm Co-design\"]}", "f1 socre": 0.9019607843137255}
{"id": "5d482043-88d2-4246-8191-e603b7bf6ed4", "pred": ["{\n\"Title\": \"Scalable Pre-training of Large Autoregressive Image Models\",\n\"Authors\": [\"Alaaeldin El-Nouby\", \"Ali Farhat\", \"Mohamed Ragab\", \"Mostafa Dehghani\", \"Georg Heigold\", \"Chen Sun\", \"Mario Lu\u010di\u0107\", \"Cordelia Schmid\"],\n\"Reference\": [\n{\"Title\": \"Vivit"], "TTFT": 15.165497541427612, "answer": "{\"Reference\": [\"Data Filtering Networks\"], \"Citation\": [\"ARVideo: Autoregressive Pretraining for Self-Supervised Video Representation Learning\"]}", "f1 socre": 0.17073170731707318}
{"id": "014ae33b-22e7-4fb7-ae34-56f64160ce74", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n1. [\"DYNAP-SE2: a scalable multi-core dynamic neuromorphic asynchronous spiking neural network processor\" by Ole Richter, et al.]\n2. [\"Speck: A Smart event-based Vision Sensor with a low latency 327K Neuron Convolutional Neural Network Processing Pipeline\" by Ole Richter,"], "TTFT": 15.628410577774048, "answer": "[\"Speck: A Smart event-based Vision Sensor with a low latency 327K Neuron Convolutional Neuronal Network Processing Pipeline\", \"DYNAP-SE2: a scalable multi-core dynamic neuromorphic asynchronous spiking neural network processor\"]", "f1 socre": 0.7543859649122807}
{"id": "3e4ee654-dfd8-4ffd-a825-79e0997961c9", "pred": ["{{\"Reference\": [\"Navigating the Safety Landscape: Measuring LLM Cybersecurity Risks with VISAGE Metric\", \"Purple Llama CYBERSECEval: A Secure Coding Benchmark for Large Language Models\"],\n\"Citation\": [\"Navigating the Safety Landscape: Measuring LLM Cybersecurity Risks with VISAGE Metric\": \"studied the safety landscape of LLMs and discovered"], "TTFT": 12.062817811965942, "answer": "{\"Reference\": [\"Purple Llama CYBERSECEvAL: A Secure Coding Benchmark for Language Models\"], \"Citation\": [\"Navigating the Safety Landscape: Measuring Risks in Finetuning Large Language Models\"]}", "f1 socre": 0.5238095238095238}
{"id": "f359acf5-39a1-4490-ba4a-95d38de35fa7", "pred": ["{\n\"Title\": \"Soaring from 4K to 400K: Extending LLM's Context with Activation Beacon\",\n\"Authors\": [\"Peitian Zhang\", \"Zheng Liu\", \"Shitao Xiao\", \"Ninglu Shao\", \"Hongjin Qian\", \"Qiwei Ye\", \"Zhicheng Dou\"],\n\"Affiliations\": [\"Beijing Academy of Artificial Intelligence\","], "TTFT": 10.695560455322266, "answer": "{\"Reference\": [\"PoSE: EFFICIENT CONTEXT WINDOW EXTENSION OF LLMS VIA POSITIONAL SKIP-WISE TRAINING\"], \"Citation\": [\"Extending Llama-3's Context Ten-Fold Overnight\"]}", "f1 socre": 0.10752688172043011}
{"id": "13a4a371-6339-4c9d-82cf-fc9ab2bb017d", "pred": ["{\n\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"],\n\"Citation\": []\n}\n\nThe paper \"OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models\" does not mention \"TinyLlama: An Open-Source Small Language Model\" in its text, so there is no citation relationship between the two papers. However, \"OpenMoE\" references \"Tiny"], "TTFT": 13.604034900665283, "answer": "{\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"], \"Citation\": [\"Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training\"]}", "f1 socre": 0.4047619047619047}
{"id": "c4a4e642-fa00-4377-a756-40f2f7ea7a74", "pred": ["{{\"Reference\": [\"A Survey of Hallucination in Large Foundation Models by Vipula Rawte, Amit Sheth, and Amitava Das\"], \"Citation\": [\"CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation by Quan Tu, Shilong Fan, Zihang Tian, Rui Yan\"]}}\n\nThis paper on CharacterEval references the survey paper on hallucination in large foundation models and"], "TTFT": 11.539960622787476, "answer": "{\"Reference\": [\"A Survey of Hallucination in \\\"Large\\\" Foundation Models\"], \"Citation\": [\"From Role-Play to Drama-Interaction: An LLM Solution\"]}", "f1 socre": 0.2745098039215686}
{"id": "835a4005-8c32-4083-b52d-4e014d1d35f5", "pred": ["Based on the provided instructions and papers, here is the longest citation chain:\n\n[\"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\" by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed, \"SUPERB: Speech processing Universal"], "TTFT": 11.292267322540283, "answer": "[\"CONTRASTIVE SEMI-SUPERVISED LEARNING FOR ASR\", \"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\", \"SUPERB: Speech processing Universal PERformance Benchmark\"]", "f1 socre": 0.38461538461538464}
{"id": "cd618a43-7176-432d-b21e-432e5c2ca4a7", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\" by Abien Fred M. Agarap, 2018, ICMLC 2018]\n\n[\"A Novel Approach Combining Recurrent Neural Network and"], "TTFT": 6.193939208984375, "answer": "[\"Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems\", \"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\"]", "f1 socre": 0.34374999999999994}
{"id": "e904afd3-5cb9-4bf7-bbee-9c8defac858c", "pred": ["Based on the provided instructions and papers, the longest citation chain can be constructed as follows:\n\n[\"On the performativity of SDG classifications in large bibliometric databases\" by Ottaviani and Stahlschmidt (2024), \"Language models are unsupervised multitask learners\" by Radford et al. (2019), \"DistilBERT: Distilling BERT: Six-Order Distillation for a Fast and Che"], "TTFT": 7.292623519897461, "answer": "[\"Contrastive Decoding: Open-ended Text Generation as Optimization\", \"An Empirical Study On Contrastive Search And Contrastive Decoding For Open-ended Text Generation\", \"On the performativity of SDG classifications in large bibliometric databases\"]", "f1 socre": 0.380952380952381}
{"id": "683f4cfd-1cfe-475d-927a-08a71d4f69e9", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\" by Niimi et al., 2024]\n\n[\"Churn Prediction with"], "TTFT": 11.674830198287964, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\", \"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\", \"An Efficient Multimodal Learning Framework to Comprehend Consumer Preferences Using BERT and Cross-Attention\"]", "f1 socre": 0.45312500000000006}
{"id": "0223faf0-f46f-459d-b1cd-45c4dab78fc2", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"A tutorial on the range variant of asymmetric numeral systems\" by James Townsend, \"Asymmetric numeral systems\" by Jarek Duda, \"Asymmetric numeral systems: entropy coding combining speed of Huffman coding with compression rate of arithmetic coding\" by Jarek Duda, Khalid Tahboub, Neeraj J. Gad"], "TTFT": 14.098109483718872, "answer": "[\"Lightweight compression with encryption based on Asymmetric Numeral Systems\", \"A tutorial on the range variant of asymmetric numeral systems\", \"ATC: an Advanced Tucker Compression library for multidimensional data\"]", "f1 socre": 0.3652173913043478}
{"id": "b7405d8e-13f0-4750-a25d-df3a67ffd39a", "pred": ["{{\"Reference\": [\"Benchmarking Large Language Models in Retrieval-Augmented Generation\", \"Retrieval-Augmented Generation in Question Answering: A Survey\"],\n\"Citation\": [\"Seven Failure Points When Engineering a Retrieval Augmented Generation System\"]}}\n\nThis paper presents the lessons learned and failure points from three case studies of implementing Retrieval-Augmented Generation (RAG) systems. It also discusses"], "TTFT": 10.79617714881897, "answer": "{\"Reference\": [\"Benchmarking Large Language Models in Retrieval-Augmented Generation\"], \"Citation\": [\"When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively\"]}", "f1 socre": 0.4186046511627907}
{"id": "150529cd-8767-477e-aa2b-28bae4970fdd", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System\" by Santos et al., \"Massively scalable stencil algorithm for the 3D wave equation on the Cerebras Wafer-Scale Engine\" by Jacquelin and Araya-Polo]\n\nThe reason for this citation chain is that the paper by Sant"], "TTFT": 15.363740921020508, "answer": "[\"Massively scalable stencil algorithm\", \"Disruptive Changes in Field Equation Modeling A Simple Interface for Wafer Scale Engines\", \"Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System\"]", "f1 socre": 0.39285714285714285}
{"id": "a8fa7aa1-2198-419b-a0de-6ba4d1c1f160", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Dance Any Beat: Blending Beats with Visuals in Dance Video Generation\" by Xuanchen Wang, Heng Wang, Weidong Cai, and Dongnan Liu],\n[\"InMoDeGAN: Interpretable Motion Decomposing Generative Adversarial Network for Video Generation\" by Yaohui Wang, Francois Bremond,"], "TTFT": 14.07155442237854, "answer": "[\"InMoDeGAN: Interpretable Motion Decomposition Generative Adversarial Network for Video Generation\", \"LATENT IMAGE ANIMATOR: <br> LEARNING TO ANIMATE IMAGES VIA LATENT SPACE NAVIGATION\", \"Dance Any Beat: Blending Beats with Visuals in Dance Video Generation\"]", "f1 socre": 0.43548387096774194}
{"id": "49680a79-a974-49b0-b7e3-85ff515bc4f2", "pred": ["{{\"Reference\": [\"Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding\"], \"Citation\": [\"Smurfs: Leveraging Multiple Proficiency Agents for Effective Tool Utilization in LLMs\" ]}}\n\nThe paper \"EasyTool: Enhancing LLM-based Agents with Concise Tool Instruction\" references the paper \"Don't Fine-Tune, Dec"], "TTFT": 10.98206639289856, "answer": "{\"Reference\": [\"Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding\"], \"Citation\": [\"Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning\"]}", "f1 socre": 0.6521739130434783}
{"id": "9f29f552-c817-4c2a-8910-b5e8ae7c014f", "pred": ["[\"Fine-Tuning LLaMA for Multi-Stage Text Retrieval\" by Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin, 2023, cited by \"Improving Text Embeddings with Large Language Models\" by Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, Furu Wei, and Jimmy Lin,"], "TTFT": 13.682081937789917, "answer": "[\"RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large Language Models\", \"Fine-Tuning LLaMA for Multi-Stage Text Retrieval\", \"Improving Text Embeddings with Large Language Models\"]", "f1 socre": 0.3225806451612903}
{"id": "30a2535f-7e73-46ad-a402-ee80ac6e4f92", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"Quantum Annealers Chain Strengths: A Simple Heuristic to Set Them All\" by Valentin Gilbert and St\u00e9phane Louise, 2024, arXiv: 2404.05443 [39]],\n[\"Benchmarking Quantum Annealers with Near-Optimal Minor Embeddings\" by Valentin Gilbert"], "TTFT": 16.3840811252594, "answer": "[\"Scaling Advantage in Approximate Optimization with Quantum Annealing\", \"4-clique Network Minor Embedding for Quantum Annealers\", \"Quantum Annealers Chain Strengths: A Simple Heuristic to Set Them All\", \"Benchmarking Quantum Annealers with Near-Optimal Minor-Embedded Instances\"]", "f1 socre": 0.40579710144927533}
{"id": "68e88dcc-6dec-4bb4-b51b-1fe8d6cd856d", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"LGDE: Local Graph-based Dictionary Expansion\" by Schindler et al., 2023]\n[\"Query2doc: Query Expansion with Large Language Models\" by Wang, Yang, and Wei, 2023]\n[\"From free text to clusters: A graph-based approach to text mining and information retrieval\" by Raghav"], "TTFT": 10.241957187652588, "answer": "[\"Query2doc: Query Expansion with Large Language Models\", \"Query Expansion by Prompting Large Language Models\", \"LGDE: Local Graph-based Dictionary Expansion\"]", "f1 socre": 0.32989690721649484}
{"id": "d7adf528-91a9-4978-a25c-29e61562a3be", "pred": ["{{\"Reference\": [\"The Pitfalls of Defining Hallucination\", \"Hallucination is Inevitable: An Innate Limitation of Large Language Models\"],\n\"Citation\": [\"Hallucination is Inevitable: An Innate Limitation of Large Language Models\", \"The Pitfalls of Defining Hallucination\"]}}\n\nIn this paper, the authors show that hallucination is inevitable for large language models (LLMs"], "TTFT": 11.103811502456665, "answer": "{\"Reference\": [\"The Pitfalls of Defining Hallucination\"], \"Citation\": [\"An ASSESSMENT OF MODEL-ON-MODEL DECEPTION\"]}", "f1 socre": 0.29333333333333333}
{"id": "949f3450-9b30-4556-be5f-ca8afe9e072f", "pred": ["{\n\"Title\": \"LLamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource Language\",\n\"Authors\": [\"Cagri Toraman\"],\n\"Reference\": [\n{\"Title\": \"BELLE: A Large-Scale Chinese Instruction Tuning Dataset for Pretrained Language Models\"},\n{\"Title\": \"Bactrain-X: A Multilingual Instruction Tuning Dataset for Low-Resource L"], "TTFT": 12.38475251197815, "answer": "{\"Reference\": [], \"Citation\": [\"LlamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource Language\", \"InstructionCP: A fast approach to transfer Large Language Models into target language\"]}", "f1 socre": 0.43010752688172044}
{"id": "8878fb61-6996-4984-9774-5ce9fb506d37", "pred": ["{\n\"Title\": \"Extreme Compression of Large Language Models via Additive Quantization\",\n\"Authors\": [\"Vage Egiazarian\", \"* 12\", \"Andrei Panferov\", \"* 12\", \"Denis Kuznedelev\", \"* 23\", \"Elias Frantar\", \"* 4\", \"Artem Babenko\", \"* 2\"],\n\"Reference\": [\n{\"Title\": \"Mistral"], "TTFT": 11.69446086883545, "answer": "{\"Reference\": [\"Mistral 7B\"], \"Citation\": [\"Sparse Expansion and Neuronal Disentanglement\"]}", "f1 socre": 0.10526315789473684}
{"id": "17539923-dd06-4712-becd-68fb8d68614d", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"Context-Aware Learning to Rank with Self-Attention\" by Przemys\u0142aw Pobrotyn, Tomasz Bartczak, Miko\u0142aj Synowiec, Rados\u0142aw Bia\u0142obrzeski, and Jaros\u0142aw Bojar]\n\ncites\n\n[\"Learning a ranking model with delta features to study the effect of neighborhood on item preference in e"], "TTFT": 6.013138055801392, "answer": "[\"Influence of Neighborhood on the Preference of an Item in eCommerce Search\", \"Context-Aware Learning to Rank with Self-Attention\"]", "f1 socre": 0.31249999999999994}
{"id": "ad6d0a53-1f8a-46b5-9738-a34d0a3cafaf", "pred": ["Based on the provided instructions and the given papers, the longest citation chain is as follows:\n\n[\"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis\" by Mena, C. Gary et al. (2019), \"Churn prediction using deep convolutional neural networks and autoencoders\" by Wangperawong, A. et al. (2016)]\n\nExplanation:\n\nThe"], "TTFT": 3.672146797180176, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\"]", "f1 socre": 0.4000000000000001}
{"id": "b5b963fe-30ab-435f-9574-dcdfd96f3707", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Minimod: A Finite Difference solver for Seismic Modeling\" by Jie Meng, Andreas Atle, and Henri Calandra, 2020]\n[\"Massively scalable stencil algorithm for seismic modeling on the Cerebras WSE-2\" by Mathias Jacquelin, Mauricio Araya-Polo,"], "TTFT": 8.336519241333008, "answer": "[\"Minimod: A Finite Difference solver for Seismic Modeling\", \"Massively scalable stencil algorithm\"]", "f1 socre": 0.3368421052631579}
{"id": "7b104919-2fba-44d2-91dd-a56bd2f30e91", "pred": ["{{\"Reference\": [\"LLM A*: Human in the Loop Large Language Models Enabled A* Search for Robotics\"], \"Citation\": [\"FunSearch\", \"Various papers on search-based MAPF methods (e.g., CBS, ECBS, EECBS)\"]}}\n\nThe paper \"Evolution of Heuristics: Towards Efficient Automatic Algorithm Design Using Large Language Model\" references the paper \"LLM A*: Human in"], "TTFT": 12.563227891921997, "answer": "{\"Reference\": [\"LLM A*: Human in the Loop Large Language Models Enabled A* Search for Robotics\"], \"Citation\": [\"Ensembling Prioritized Hybrid Policies for Multi-agent Pathfinding\"]}", "f1 socre": 0.5}
{"id": "31ee8539-2e9e-40e2-adc3-082c872698f7", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\" by Niimi et al., 2024]\n\ncites\n\n[\"A PRE"], "TTFT": 11.793168783187866, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\", \"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\", \"An Efficient Multimodal Learning Framework to Comprehend Consumer Preferences Using BERT and Cross-Attention\"]", "f1 socre": 0.39682539682539686}
{"id": "9b982095-daad-4632-9573-b0d7037dd6a7", "pred": ["{{\"Reference\": [\"A Prompt for Investigating the Effectiveness of LLMs in Self-Critiquing for Planning: An Empirical Study\", \"Large Language Models Cannot Self-Correct Reasoning Intrinsically\"],\n\"Citation\": [\"LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks\"]}}"], "TTFT": 10.443064212799072, "answer": "{\"Reference\": [\"LARGE LANGUAGE MoDELS CANNOT SELF-CorRECT REASONING YET\", \"Can Large Language Models Really Improve by Self-critiquing Their Own Plans?\"], \"Citation\": []}", "f1 socre": 0.3943661971830986}
{"id": "ecded47a-3741-4458-ba61-c04271c33651", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"MonaCoBERT: Monotonic attention based ConvBERT for Knowledge Tracing\" by Unggi Lee, Yonghyun Park, Yujin Kim, Seongyune Choi, and Hyeoncheol Kim, 2021]\n\nThis paper cites:\n\n[\"Understanding the Instability of Transformer-based Mod"], "TTFT": 10.399053812026978, "answer": "[\"Very Deep Transformers for Neural Machine Translation\", \"Understanding the Difficulty of Training Transformers\", \"MonaCoBERT: Monotonic attention based ConvBERT for Knowledge Tracing\"]", "f1 socre": 0.29411764705882354}
{"id": "71d0fb39-d315-4e33-beec-c53271663f7f", "pred": ["{{\"Reference\": [\"LoRA-Switch: Boosting the Efficiency of Dynamic LLM Adapters via System-Algorithm Co-design\"], \"Citation\": []}}"], "TTFT": 13.256085872650146, "answer": "{\"Reference\": [\"ONLINE SPECULATIVE DECODING\"], \"Citation\": [\"LoRA-Switch: Boosting the Efficiency of Dynamic LLM Adapters via System-Algorithm Co-design\"]}", "f1 socre": 0.9019607843137255}
{"id": "5d482043-88d2-4246-8191-e603b7bf6ed4", "pred": ["{\n\"Title\": \"Scalable Pre-training of Large Autoregressive Image Models\",\n\"Authors\": [\"Alaaeldin El-Nouby\", \"Ali Farhat\", \"Mohamed Ragab\", \"Mostafa Dehghani\", \"Georg Heigold\", \"Chen Sun\", \"Mario Lu\u010di\u0107\", \"Cordelia Schmid\"],\n\"Reference\": [\n{\"Title\": \"Vivit"], "TTFT": 15.165497541427612, "answer": "{\"Reference\": [\"Data Filtering Networks\"], \"Citation\": [\"ARVideo: Autoregressive Pretraining for Self-Supervised Video Representation Learning\"]}", "f1 socre": 0.17073170731707318}
{"id": "014ae33b-22e7-4fb7-ae34-56f64160ce74", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n1. [\"DYNAP-SE2: a scalable multi-core dynamic neuromorphic asynchronous spiking neural network processor\" by Ole Richter, et al.]\n2. [\"Speck: A Smart event-based Vision Sensor with a low latency 327K Neuron Convolutional Neural Network Processing Pipeline\" by Ole Richter,"], "TTFT": 15.628410577774048, "answer": "[\"Speck: A Smart event-based Vision Sensor with a low latency 327K Neuron Convolutional Neuronal Network Processing Pipeline\", \"DYNAP-SE2: a scalable multi-core dynamic neuromorphic asynchronous spiking neural network processor\"]", "f1 socre": 0.7543859649122807}
{"id": "3e4ee654-dfd8-4ffd-a825-79e0997961c9", "pred": ["{{\"Reference\": [\"Navigating the Safety Landscape: Measuring LLM Cybersecurity Risks with VISAGE Metric\", \"Purple Llama CYBERSECEval: A Secure Coding Benchmark for Large Language Models\"],\n\"Citation\": [\"Navigating the Safety Landscape: Measuring LLM Cybersecurity Risks with VISAGE Metric\": \"studied the safety landscape of LLMs and discovered"], "TTFT": 12.062817811965942, "answer": "{\"Reference\": [\"Purple Llama CYBERSECEvAL: A Secure Coding Benchmark for Language Models\"], \"Citation\": [\"Navigating the Safety Landscape: Measuring Risks in Finetuning Large Language Models\"]}", "f1 socre": 0.5238095238095238}
{"id": "f359acf5-39a1-4490-ba4a-95d38de35fa7", "pred": ["{\n\"Title\": \"Soaring from 4K to 400K: Extending LLM's Context with Activation Beacon\",\n\"Authors\": [\"Peitian Zhang\", \"Zheng Liu\", \"Shitao Xiao\", \"Ninglu Shao\", \"Hongjin Qian\", \"Qiwei Ye\", \"Zhicheng Dou\"],\n\"Affiliations\": [\"Beijing Academy of Artificial Intelligence\","], "TTFT": 10.695560455322266, "answer": "{\"Reference\": [\"PoSE: EFFICIENT CONTEXT WINDOW EXTENSION OF LLMS VIA POSITIONAL SKIP-WISE TRAINING\"], \"Citation\": [\"Extending Llama-3's Context Ten-Fold Overnight\"]}", "f1 socre": 0.10752688172043011}
{"id": "2357f6df-fed9-4508-a80c-800fe6995caa", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"GTBENCH: Uncovering Game-Theoretic Characteristics of LLMs\" by Jinhao Duan, Renming Zhang, James Diffenderfer, Bhavya Kailkhura, Lichao Sun, Elias Stengel-Eskin, Mohit Bansal, Tianlong Chen, and Kaidi Xu]\n\n[\"Exploring"], "TTFT": 10.72509765625, "answer": "[\"![](https://cdn.mathpix.com/cropped/2024_06_04_791d7ec1c70f2f7bcd44g-01.jpg?height=135&width=174&top_left_y=320&top_left_x=260)\", \"Generative AI for Game Theory-based Mobile Networking\", \"Exploring Equilibrium Strategies in Network Games with Generative AI\"]", "f1 socre": 0.025477707006369425}
{"id": "13a4a371-6339-4c9d-82cf-fc9ab2bb017d", "pred": ["{\n\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"],\n\"Citation\": []\n}\n\nThe paper \"OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models\" does not mention \"TinyLlama: An Open-Source Small Language Model\" in its text, so there is no citation relationship between the two papers. However, \"OpenMoE\" references \"Tiny"], "TTFT": 13.604034900665283, "answer": "{\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"], \"Citation\": [\"Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training\"]}", "f1 socre": 0.4047619047619047}
{"id": "c4a4e642-fa00-4377-a756-40f2f7ea7a74", "pred": ["{{\"Reference\": [\"A Survey of Hallucination in Large Foundation Models by Vipula Rawte, Amit Sheth, and Amitava Das\"], \"Citation\": [\"CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation by Quan Tu, Shilong Fan, Zihang Tian, Rui Yan\"]}}\n\nThis paper on CharacterEval references the survey paper on hallucination in large foundation models and"], "TTFT": 11.539960622787476, "answer": "{\"Reference\": [\"A Survey of Hallucination in \\\"Large\\\" Foundation Models\"], \"Citation\": [\"From Role-Play to Drama-Interaction: An LLM Solution\"]}", "f1 socre": 0.2745098039215686}
{"id": "835a4005-8c32-4083-b52d-4e014d1d35f5", "pred": ["Based on the provided instructions and papers, here is the longest citation chain:\n\n[\"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\" by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed, \"SUPERB: Speech processing Universal"], "TTFT": 11.292267322540283, "answer": "[\"CONTRASTIVE SEMI-SUPERVISED LEARNING FOR ASR\", \"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\", \"SUPERB: Speech processing Universal PERformance Benchmark\"]", "f1 socre": 0.38461538461538464}
{"id": "cd618a43-7176-432d-b21e-432e5c2ca4a7", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\" by Abien Fred M. Agarap, 2018, ICMLC 2018]\n\n[\"A Novel Approach Combining Recurrent Neural Network and"], "TTFT": 6.193939208984375, "answer": "[\"Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems\", \"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\"]", "f1 socre": 0.34374999999999994}
{"id": "e904afd3-5cb9-4bf7-bbee-9c8defac858c", "pred": ["Based on the provided instructions and papers, the longest citation chain can be constructed as follows:\n\n[\"On the performativity of SDG classifications in large bibliometric databases\" by Ottaviani and Stahlschmidt (2024), \"Language models are unsupervised multitask learners\" by Radford et al. (2019), \"DistilBERT: Distilling BERT: Six-Order Distillation for a Fast and Che"], "TTFT": 7.292623519897461, "answer": "[\"Contrastive Decoding: Open-ended Text Generation as Optimization\", \"An Empirical Study On Contrastive Search And Contrastive Decoding For Open-ended Text Generation\", \"On the performativity of SDG classifications in large bibliometric databases\"]", "f1 socre": 0.380952380952381}
{"id": "683f4cfd-1cfe-475d-927a-08a71d4f69e9", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\" by Niimi et al., 2024]\n\n[\"Churn Prediction with"], "TTFT": 11.674830198287964, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\", \"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\", \"An Efficient Multimodal Learning Framework to Comprehend Consumer Preferences Using BERT and Cross-Attention\"]", "f1 socre": 0.45312500000000006}
{"id": "0223faf0-f46f-459d-b1cd-45c4dab78fc2", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"A tutorial on the range variant of asymmetric numeral systems\" by James Townsend, \"Asymmetric numeral systems\" by Jarek Duda, \"Asymmetric numeral systems: entropy coding combining speed of Huffman coding with compression rate of arithmetic coding\" by Jarek Duda, Khalid Tahboub, Neeraj J. Gad"], "TTFT": 14.098109483718872, "answer": "[\"Lightweight compression with encryption based on Asymmetric Numeral Systems\", \"A tutorial on the range variant of asymmetric numeral systems\", \"ATC: an Advanced Tucker Compression library for multidimensional data\"]", "f1 socre": 0.3652173913043478}
{"id": "b7405d8e-13f0-4750-a25d-df3a67ffd39a", "pred": ["{{\"Reference\": [\"Benchmarking Large Language Models in Retrieval-Augmented Generation\", \"Retrieval-Augmented Generation in Question Answering: A Survey\"],\n\"Citation\": [\"Seven Failure Points When Engineering a Retrieval Augmented Generation System\"]}}\n\nThis paper presents the lessons learned and failure points from three case studies of implementing Retrieval-Augmented Generation (RAG) systems. It also discusses"], "TTFT": 10.79617714881897, "answer": "{\"Reference\": [\"Benchmarking Large Language Models in Retrieval-Augmented Generation\"], \"Citation\": [\"When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively\"]}", "f1 socre": 0.4186046511627907}
{"id": "150529cd-8767-477e-aa2b-28bae4970fdd", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System\" by Santos et al., \"Massively scalable stencil algorithm for the 3D wave equation on the Cerebras Wafer-Scale Engine\" by Jacquelin and Araya-Polo]\n\nThe reason for this citation chain is that the paper by Sant"], "TTFT": 15.363740921020508, "answer": "[\"Massively scalable stencil algorithm\", \"Disruptive Changes in Field Equation Modeling A Simple Interface for Wafer Scale Engines\", \"Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System\"]", "f1 socre": 0.39285714285714285}
{"id": "a8fa7aa1-2198-419b-a0de-6ba4d1c1f160", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Dance Any Beat: Blending Beats with Visuals in Dance Video Generation\" by Xuanchen Wang, Heng Wang, Weidong Cai, and Dongnan Liu],\n[\"InMoDeGAN: Interpretable Motion Decomposing Generative Adversarial Network for Video Generation\" by Yaohui Wang, Francois Bremond,"], "TTFT": 14.07155442237854, "answer": "[\"InMoDeGAN: Interpretable Motion Decomposition Generative Adversarial Network for Video Generation\", \"LATENT IMAGE ANIMATOR: <br> LEARNING TO ANIMATE IMAGES VIA LATENT SPACE NAVIGATION\", \"Dance Any Beat: Blending Beats with Visuals in Dance Video Generation\"]", "f1 socre": 0.43548387096774194}
{"id": "49680a79-a974-49b0-b7e3-85ff515bc4f2", "pred": ["{{\"Reference\": [\"Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding\"], \"Citation\": [\"Smurfs: Leveraging Multiple Proficiency Agents for Effective Tool Utilization in LLMs\" ]}}\n\nThe paper \"EasyTool: Enhancing LLM-based Agents with Concise Tool Instruction\" references the paper \"Don't Fine-Tune, Dec"], "TTFT": 10.98206639289856, "answer": "{\"Reference\": [\"Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding\"], \"Citation\": [\"Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning\"]}", "f1 socre": 0.6521739130434783}
{"id": "9f29f552-c817-4c2a-8910-b5e8ae7c014f", "pred": ["[\"Fine-Tuning LLaMA for Multi-Stage Text Retrieval\" by Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin, 2023, cited by \"Improving Text Embeddings with Large Language Models\" by Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, Furu Wei, and Jimmy Lin,"], "TTFT": 13.682081937789917, "answer": "[\"RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large Language Models\", \"Fine-Tuning LLaMA for Multi-Stage Text Retrieval\", \"Improving Text Embeddings with Large Language Models\"]", "f1 socre": 0.3225806451612903}
{"id": "30a2535f-7e73-46ad-a402-ee80ac6e4f92", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"Quantum Annealers Chain Strengths: A Simple Heuristic to Set Them All\" by Valentin Gilbert and St\u00e9phane Louise, 2024, arXiv: 2404.05443 [39]],\n[\"Benchmarking Quantum Annealers with Near-Optimal Minor Embeddings\" by Valentin Gilbert"], "TTFT": 16.3840811252594, "answer": "[\"Scaling Advantage in Approximate Optimization with Quantum Annealing\", \"4-clique Network Minor Embedding for Quantum Annealers\", \"Quantum Annealers Chain Strengths: A Simple Heuristic to Set Them All\", \"Benchmarking Quantum Annealers with Near-Optimal Minor-Embedded Instances\"]", "f1 socre": 0.40579710144927533}
{"id": "68e88dcc-6dec-4bb4-b51b-1fe8d6cd856d", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"LGDE: Local Graph-based Dictionary Expansion\" by Schindler et al., 2023]\n[\"Query2doc: Query Expansion with Large Language Models\" by Wang, Yang, and Wei, 2023]\n[\"From free text to clusters: A graph-based approach to text mining and information retrieval\" by Raghav"], "TTFT": 10.241957187652588, "answer": "[\"Query2doc: Query Expansion with Large Language Models\", \"Query Expansion by Prompting Large Language Models\", \"LGDE: Local Graph-based Dictionary Expansion\"]", "f1 socre": 0.32989690721649484}
{"id": "d7adf528-91a9-4978-a25c-29e61562a3be", "pred": ["{{\"Reference\": [\"The Pitfalls of Defining Hallucination\", \"Hallucination is Inevitable: An Innate Limitation of Large Language Models\"],\n\"Citation\": [\"Hallucination is Inevitable: An Innate Limitation of Large Language Models\", \"The Pitfalls of Defining Hallucination\"]}}\n\nIn this paper, the authors show that hallucination is inevitable for large language models (LLMs"], "TTFT": 11.103811502456665, "answer": "{\"Reference\": [\"The Pitfalls of Defining Hallucination\"], \"Citation\": [\"An ASSESSMENT OF MODEL-ON-MODEL DECEPTION\"]}", "f1 socre": 0.29333333333333333}
{"id": "949f3450-9b30-4556-be5f-ca8afe9e072f", "pred": ["{\n\"Title\": \"LLamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource Language\",\n\"Authors\": [\"Cagri Toraman\"],\n\"Reference\": [\n{\"Title\": \"BELLE: A Large-Scale Chinese Instruction Tuning Dataset for Pretrained Language Models\"},\n{\"Title\": \"Bactrain-X: A Multilingual Instruction Tuning Dataset for Low-Resource L"], "TTFT": 12.38475251197815, "answer": "{\"Reference\": [], \"Citation\": [\"LlamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource Language\", \"InstructionCP: A fast approach to transfer Large Language Models into target language\"]}", "f1 socre": 0.43010752688172044}
{"id": "8878fb61-6996-4984-9774-5ce9fb506d37", "pred": ["{\n\"Title\": \"Extreme Compression of Large Language Models via Additive Quantization\",\n\"Authors\": [\"Vage Egiazarian\", \"* 12\", \"Andrei Panferov\", \"* 12\", \"Denis Kuznedelev\", \"* 23\", \"Elias Frantar\", \"* 4\", \"Artem Babenko\", \"* 2\"],\n\"Reference\": [\n{\"Title\": \"Mistral"], "TTFT": 11.69446086883545, "answer": "{\"Reference\": [\"Mistral 7B\"], \"Citation\": [\"Sparse Expansion and Neuronal Disentanglement\"]}", "f1 socre": 0.10526315789473684}
{"id": "17539923-dd06-4712-becd-68fb8d68614d", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"Context-Aware Learning to Rank with Self-Attention\" by Przemys\u0142aw Pobrotyn, Tomasz Bartczak, Miko\u0142aj Synowiec, Rados\u0142aw Bia\u0142obrzeski, and Jaros\u0142aw Bojar]\n\ncites\n\n[\"Learning a ranking model with delta features to study the effect of neighborhood on item preference in e"], "TTFT": 6.013138055801392, "answer": "[\"Influence of Neighborhood on the Preference of an Item in eCommerce Search\", \"Context-Aware Learning to Rank with Self-Attention\"]", "f1 socre": 0.31249999999999994}
{"id": "ad6d0a53-1f8a-46b5-9738-a34d0a3cafaf", "pred": ["Based on the provided instructions and the given papers, the longest citation chain is as follows:\n\n[\"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis\" by Mena, C. Gary et al. (2019), \"Churn prediction using deep convolutional neural networks and autoencoders\" by Wangperawong, A. et al. (2016)]\n\nExplanation:\n\nThe"], "TTFT": 3.672146797180176, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\"]", "f1 socre": 0.4000000000000001}
{"id": "b5b963fe-30ab-435f-9574-dcdfd96f3707", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Minimod: A Finite Difference solver for Seismic Modeling\" by Jie Meng, Andreas Atle, and Henri Calandra, 2020]\n[\"Massively scalable stencil algorithm for seismic modeling on the Cerebras WSE-2\" by Mathias Jacquelin, Mauricio Araya-Polo,"], "TTFT": 8.336519241333008, "answer": "[\"Minimod: A Finite Difference solver for Seismic Modeling\", \"Massively scalable stencil algorithm\"]", "f1 socre": 0.3368421052631579}
{"id": "7b104919-2fba-44d2-91dd-a56bd2f30e91", "pred": ["{{\"Reference\": [\"LLM A*: Human in the Loop Large Language Models Enabled A* Search for Robotics\"], \"Citation\": [\"FunSearch\", \"Various papers on search-based MAPF methods (e.g., CBS, ECBS, EECBS)\"]}}\n\nThe paper \"Evolution of Heuristics: Towards Efficient Automatic Algorithm Design Using Large Language Model\" references the paper \"LLM A*: Human in"], "TTFT": 12.563227891921997, "answer": "{\"Reference\": [\"LLM A*: Human in the Loop Large Language Models Enabled A* Search for Robotics\"], \"Citation\": [\"Ensembling Prioritized Hybrid Policies for Multi-agent Pathfinding\"]}", "f1 socre": 0.5}
{"id": "31ee8539-2e9e-40e2-adc3-082c872698f7", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\" by Niimi et al., 2024]\n\ncites\n\n[\"A PRE"], "TTFT": 11.793168783187866, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\", \"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\", \"An Efficient Multimodal Learning Framework to Comprehend Consumer Preferences Using BERT and Cross-Attention\"]", "f1 socre": 0.39682539682539686}
{"id": "9b982095-daad-4632-9573-b0d7037dd6a7", "pred": ["{{\"Reference\": [\"A Prompt for Investigating the Effectiveness of LLMs in Self-Critiquing for Planning: An Empirical Study\", \"Large Language Models Cannot Self-Correct Reasoning Intrinsically\"],\n\"Citation\": [\"LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks\"]}}"], "TTFT": 10.443064212799072, "answer": "{\"Reference\": [\"LARGE LANGUAGE MoDELS CANNOT SELF-CorRECT REASONING YET\", \"Can Large Language Models Really Improve by Self-critiquing Their Own Plans?\"], \"Citation\": []}", "f1 socre": 0.3943661971830986}
{"id": "ecded47a-3741-4458-ba61-c04271c33651", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"MonaCoBERT: Monotonic attention based ConvBERT for Knowledge Tracing\" by Unggi Lee, Yonghyun Park, Yujin Kim, Seongyune Choi, and Hyeoncheol Kim, 2021]\n\nThis paper cites:\n\n[\"Understanding the Instability of Transformer-based Mod"], "TTFT": 10.399053812026978, "answer": "[\"Very Deep Transformers for Neural Machine Translation\", \"Understanding the Difficulty of Training Transformers\", \"MonaCoBERT: Monotonic attention based ConvBERT for Knowledge Tracing\"]", "f1 socre": 0.29411764705882354}
{"id": "71d0fb39-d315-4e33-beec-c53271663f7f", "pred": ["{{\"Reference\": [\"LoRA-Switch: Boosting the Efficiency of Dynamic LLM Adapters via System-Algorithm Co-design\"], \"Citation\": []}}"], "TTFT": 13.256085872650146, "answer": "{\"Reference\": [\"ONLINE SPECULATIVE DECODING\"], \"Citation\": [\"LoRA-Switch: Boosting the Efficiency of Dynamic LLM Adapters via System-Algorithm Co-design\"]}", "f1 socre": 0.9019607843137255}
{"id": "5d482043-88d2-4246-8191-e603b7bf6ed4", "pred": ["{\n\"Title\": \"Scalable Pre-training of Large Autoregressive Image Models\",\n\"Authors\": [\"Alaaeldin El-Nouby\", \"Ali Farhat\", \"Mohamed Ragab\", \"Mostafa Dehghani\", \"Georg Heigold\", \"Chen Sun\", \"Mario Lu\u010di\u0107\", \"Cordelia Schmid\"],\n\"Reference\": [\n{\"Title\": \"Vivit"], "TTFT": 15.165497541427612, "answer": "{\"Reference\": [\"Data Filtering Networks\"], \"Citation\": [\"ARVideo: Autoregressive Pretraining for Self-Supervised Video Representation Learning\"]}", "f1 socre": 0.17073170731707318}
{"id": "014ae33b-22e7-4fb7-ae34-56f64160ce74", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n1. [\"DYNAP-SE2: a scalable multi-core dynamic neuromorphic asynchronous spiking neural network processor\" by Ole Richter, et al.]\n2. [\"Speck: A Smart event-based Vision Sensor with a low latency 327K Neuron Convolutional Neural Network Processing Pipeline\" by Ole Richter,"], "TTFT": 15.628410577774048, "answer": "[\"Speck: A Smart event-based Vision Sensor with a low latency 327K Neuron Convolutional Neuronal Network Processing Pipeline\", \"DYNAP-SE2: a scalable multi-core dynamic neuromorphic asynchronous spiking neural network processor\"]", "f1 socre": 0.7543859649122807}
{"id": "3e4ee654-dfd8-4ffd-a825-79e0997961c9", "pred": ["{{\"Reference\": [\"Navigating the Safety Landscape: Measuring LLM Cybersecurity Risks with VISAGE Metric\", \"Purple Llama CYBERSECEval: A Secure Coding Benchmark for Large Language Models\"],\n\"Citation\": [\"Navigating the Safety Landscape: Measuring LLM Cybersecurity Risks with VISAGE Metric\": \"studied the safety landscape of LLMs and discovered"], "TTFT": 12.062817811965942, "answer": "{\"Reference\": [\"Purple Llama CYBERSECEvAL: A Secure Coding Benchmark for Language Models\"], \"Citation\": [\"Navigating the Safety Landscape: Measuring Risks in Finetuning Large Language Models\"]}", "f1 socre": 0.5238095238095238}
{"id": "f359acf5-39a1-4490-ba4a-95d38de35fa7", "pred": ["{\n\"Title\": \"Soaring from 4K to 400K: Extending LLM's Context with Activation Beacon\",\n\"Authors\": [\"Peitian Zhang\", \"Zheng Liu\", \"Shitao Xiao\", \"Ninglu Shao\", \"Hongjin Qian\", \"Qiwei Ye\", \"Zhicheng Dou\"],\n\"Affiliations\": [\"Beijing Academy of Artificial Intelligence\","], "TTFT": 10.695560455322266, "answer": "{\"Reference\": [\"PoSE: EFFICIENT CONTEXT WINDOW EXTENSION OF LLMS VIA POSITIONAL SKIP-WISE TRAINING\"], \"Citation\": [\"Extending Llama-3's Context Ten-Fold Overnight\"]}", "f1 socre": 0.10752688172043011}
{"id": "2357f6df-fed9-4508-a80c-800fe6995caa", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"GTBENCH: Uncovering Game-Theoretic Characteristics of LLMs\" by Jinhao Duan, Renming Zhang, James Diffenderfer, Bhavya Kailkhura, Lichao Sun, Elias Stengel-Eskin, Mohit Bansal, Tianlong Chen, and Kaidi Xu]\n\n[\"Exploring"], "TTFT": 10.72509765625, "answer": "[\"![](https://cdn.mathpix.com/cropped/2024_06_04_791d7ec1c70f2f7bcd44g-01.jpg?height=135&width=174&top_left_y=320&top_left_x=260)\", \"Generative AI for Game Theory-based Mobile Networking\", \"Exploring Equilibrium Strategies in Network Games with Generative AI\"]", "f1 socre": 0.025477707006369425}
{"id": "aa9707c4-9506-4e51-899d-9213ad10a324", "pred": ["Based on the provided instructions, the longest citation chain from the given papers is as follows:\n\n[\"PromptMind Team at EHRSQL-2024: Improving Reliability of SQL Generation using Ensemble LLMs\", \"Angel-Optimized Text Embeddings\", \"Dolan and Brockett (2005): Building a question answering test collection\"]\n\nExplanation:\n\n1. The first paper, \"Prompt"], "TTFT": 11.649495124816895, "answer": "[\"Scaling Sentence Embeddings with Large Language Models\", \"ANGLE-OPTIMIZED TEXT EMBEDDINGS\", \"PromptMind Team at EHRSQL-2024: Improving Reliability of SQL Generation using Ensemble LLMs\"]", "f1 socre": 0.5617977528089888}
{"id": "13a4a371-6339-4c9d-82cf-fc9ab2bb017d", "pred": ["{\n\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"],\n\"Citation\": []\n}\n\nThe paper \"OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models\" does not mention \"TinyLlama: An Open-Source Small Language Model\" in its text, so there is no citation relationship between the two papers. However, \"OpenMoE\" references \"Tiny"], "TTFT": 13.604034900665283, "answer": "{\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"], \"Citation\": [\"Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training\"]}", "f1 socre": 0.4047619047619047}
{"id": "c4a4e642-fa00-4377-a756-40f2f7ea7a74", "pred": ["{{\"Reference\": [\"A Survey of Hallucination in Large Foundation Models by Vipula Rawte, Amit Sheth, and Amitava Das\"], \"Citation\": [\"CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation by Quan Tu, Shilong Fan, Zihang Tian, Rui Yan\"]}}\n\nThis paper on CharacterEval references the survey paper on hallucination in large foundation models and"], "TTFT": 11.539960622787476, "answer": "{\"Reference\": [\"A Survey of Hallucination in \\\"Large\\\" Foundation Models\"], \"Citation\": [\"From Role-Play to Drama-Interaction: An LLM Solution\"]}", "f1 socre": 0.2745098039215686}
{"id": "835a4005-8c32-4083-b52d-4e014d1d35f5", "pred": ["Based on the provided instructions and papers, here is the longest citation chain:\n\n[\"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\" by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed, \"SUPERB: Speech processing Universal"], "TTFT": 11.292267322540283, "answer": "[\"CONTRASTIVE SEMI-SUPERVISED LEARNING FOR ASR\", \"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\", \"SUPERB: Speech processing Universal PERformance Benchmark\"]", "f1 socre": 0.38461538461538464}
{"id": "cd618a43-7176-432d-b21e-432e5c2ca4a7", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\" by Abien Fred M. Agarap, 2018, ICMLC 2018]\n\n[\"A Novel Approach Combining Recurrent Neural Network and"], "TTFT": 6.193939208984375, "answer": "[\"Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems\", \"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\"]", "f1 socre": 0.34374999999999994}
{"id": "e904afd3-5cb9-4bf7-bbee-9c8defac858c", "pred": ["Based on the provided instructions and papers, the longest citation chain can be constructed as follows:\n\n[\"On the performativity of SDG classifications in large bibliometric databases\" by Ottaviani and Stahlschmidt (2024), \"Language models are unsupervised multitask learners\" by Radford et al. (2019), \"DistilBERT: Distilling BERT: Six-Order Distillation for a Fast and Che"], "TTFT": 7.292623519897461, "answer": "[\"Contrastive Decoding: Open-ended Text Generation as Optimization\", \"An Empirical Study On Contrastive Search And Contrastive Decoding For Open-ended Text Generation\", \"On the performativity of SDG classifications in large bibliometric databases\"]", "f1 socre": 0.380952380952381}
{"id": "683f4cfd-1cfe-475d-927a-08a71d4f69e9", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\" by Niimi et al., 2024]\n\n[\"Churn Prediction with"], "TTFT": 11.674830198287964, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\", \"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\", \"An Efficient Multimodal Learning Framework to Comprehend Consumer Preferences Using BERT and Cross-Attention\"]", "f1 socre": 0.45312500000000006}
{"id": "0223faf0-f46f-459d-b1cd-45c4dab78fc2", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"A tutorial on the range variant of asymmetric numeral systems\" by James Townsend, \"Asymmetric numeral systems\" by Jarek Duda, \"Asymmetric numeral systems: entropy coding combining speed of Huffman coding with compression rate of arithmetic coding\" by Jarek Duda, Khalid Tahboub, Neeraj J. Gad"], "TTFT": 14.098109483718872, "answer": "[\"Lightweight compression with encryption based on Asymmetric Numeral Systems\", \"A tutorial on the range variant of asymmetric numeral systems\", \"ATC: an Advanced Tucker Compression library for multidimensional data\"]", "f1 socre": 0.3652173913043478}
{"id": "b7405d8e-13f0-4750-a25d-df3a67ffd39a", "pred": ["{{\"Reference\": [\"Benchmarking Large Language Models in Retrieval-Augmented Generation\", \"Retrieval-Augmented Generation in Question Answering: A Survey\"],\n\"Citation\": [\"Seven Failure Points When Engineering a Retrieval Augmented Generation System\"]}}\n\nThis paper presents the lessons learned and failure points from three case studies of implementing Retrieval-Augmented Generation (RAG) systems. It also discusses"], "TTFT": 10.79617714881897, "answer": "{\"Reference\": [\"Benchmarking Large Language Models in Retrieval-Augmented Generation\"], \"Citation\": [\"When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively\"]}", "f1 socre": 0.4186046511627907}
{"id": "150529cd-8767-477e-aa2b-28bae4970fdd", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System\" by Santos et al., \"Massively scalable stencil algorithm for the 3D wave equation on the Cerebras Wafer-Scale Engine\" by Jacquelin and Araya-Polo]\n\nThe reason for this citation chain is that the paper by Sant"], "TTFT": 15.363740921020508, "answer": "[\"Massively scalable stencil algorithm\", \"Disruptive Changes in Field Equation Modeling A Simple Interface for Wafer Scale Engines\", \"Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System\"]", "f1 socre": 0.39285714285714285}
{"id": "a8fa7aa1-2198-419b-a0de-6ba4d1c1f160", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Dance Any Beat: Blending Beats with Visuals in Dance Video Generation\" by Xuanchen Wang, Heng Wang, Weidong Cai, and Dongnan Liu],\n[\"InMoDeGAN: Interpretable Motion Decomposing Generative Adversarial Network for Video Generation\" by Yaohui Wang, Francois Bremond,"], "TTFT": 14.07155442237854, "answer": "[\"InMoDeGAN: Interpretable Motion Decomposition Generative Adversarial Network for Video Generation\", \"LATENT IMAGE ANIMATOR: <br> LEARNING TO ANIMATE IMAGES VIA LATENT SPACE NAVIGATION\", \"Dance Any Beat: Blending Beats with Visuals in Dance Video Generation\"]", "f1 socre": 0.43548387096774194}
{"id": "49680a79-a974-49b0-b7e3-85ff515bc4f2", "pred": ["{{\"Reference\": [\"Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding\"], \"Citation\": [\"Smurfs: Leveraging Multiple Proficiency Agents for Effective Tool Utilization in LLMs\" ]}}\n\nThe paper \"EasyTool: Enhancing LLM-based Agents with Concise Tool Instruction\" references the paper \"Don't Fine-Tune, Dec"], "TTFT": 10.98206639289856, "answer": "{\"Reference\": [\"Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding\"], \"Citation\": [\"Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning\"]}", "f1 socre": 0.6521739130434783}
{"id": "9f29f552-c817-4c2a-8910-b5e8ae7c014f", "pred": ["[\"Fine-Tuning LLaMA for Multi-Stage Text Retrieval\" by Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin, 2023, cited by \"Improving Text Embeddings with Large Language Models\" by Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, Furu Wei, and Jimmy Lin,"], "TTFT": 13.682081937789917, "answer": "[\"RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large Language Models\", \"Fine-Tuning LLaMA for Multi-Stage Text Retrieval\", \"Improving Text Embeddings with Large Language Models\"]", "f1 socre": 0.3225806451612903}
{"id": "30a2535f-7e73-46ad-a402-ee80ac6e4f92", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"Quantum Annealers Chain Strengths: A Simple Heuristic to Set Them All\" by Valentin Gilbert and St\u00e9phane Louise, 2024, arXiv: 2404.05443 [39]],\n[\"Benchmarking Quantum Annealers with Near-Optimal Minor Embeddings\" by Valentin Gilbert"], "TTFT": 16.3840811252594, "answer": "[\"Scaling Advantage in Approximate Optimization with Quantum Annealing\", \"4-clique Network Minor Embedding for Quantum Annealers\", \"Quantum Annealers Chain Strengths: A Simple Heuristic to Set Them All\", \"Benchmarking Quantum Annealers with Near-Optimal Minor-Embedded Instances\"]", "f1 socre": 0.40579710144927533}
{"id": "68e88dcc-6dec-4bb4-b51b-1fe8d6cd856d", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"LGDE: Local Graph-based Dictionary Expansion\" by Schindler et al., 2023]\n[\"Query2doc: Query Expansion with Large Language Models\" by Wang, Yang, and Wei, 2023]\n[\"From free text to clusters: A graph-based approach to text mining and information retrieval\" by Raghav"], "TTFT": 10.241957187652588, "answer": "[\"Query2doc: Query Expansion with Large Language Models\", \"Query Expansion by Prompting Large Language Models\", \"LGDE: Local Graph-based Dictionary Expansion\"]", "f1 socre": 0.32989690721649484}
{"id": "d7adf528-91a9-4978-a25c-29e61562a3be", "pred": ["{{\"Reference\": [\"The Pitfalls of Defining Hallucination\", \"Hallucination is Inevitable: An Innate Limitation of Large Language Models\"],\n\"Citation\": [\"Hallucination is Inevitable: An Innate Limitation of Large Language Models\", \"The Pitfalls of Defining Hallucination\"]}}\n\nIn this paper, the authors show that hallucination is inevitable for large language models (LLMs"], "TTFT": 11.103811502456665, "answer": "{\"Reference\": [\"The Pitfalls of Defining Hallucination\"], \"Citation\": [\"An ASSESSMENT OF MODEL-ON-MODEL DECEPTION\"]}", "f1 socre": 0.29333333333333333}
{"id": "949f3450-9b30-4556-be5f-ca8afe9e072f", "pred": ["{\n\"Title\": \"LLamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource Language\",\n\"Authors\": [\"Cagri Toraman\"],\n\"Reference\": [\n{\"Title\": \"BELLE: A Large-Scale Chinese Instruction Tuning Dataset for Pretrained Language Models\"},\n{\"Title\": \"Bactrain-X: A Multilingual Instruction Tuning Dataset for Low-Resource L"], "TTFT": 12.38475251197815, "answer": "{\"Reference\": [], \"Citation\": [\"LlamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource Language\", \"InstructionCP: A fast approach to transfer Large Language Models into target language\"]}", "f1 socre": 0.43010752688172044}
{"id": "8878fb61-6996-4984-9774-5ce9fb506d37", "pred": ["{\n\"Title\": \"Extreme Compression of Large Language Models via Additive Quantization\",\n\"Authors\": [\"Vage Egiazarian\", \"* 12\", \"Andrei Panferov\", \"* 12\", \"Denis Kuznedelev\", \"* 23\", \"Elias Frantar\", \"* 4\", \"Artem Babenko\", \"* 2\"],\n\"Reference\": [\n{\"Title\": \"Mistral"], "TTFT": 11.69446086883545, "answer": "{\"Reference\": [\"Mistral 7B\"], \"Citation\": [\"Sparse Expansion and Neuronal Disentanglement\"]}", "f1 socre": 0.10526315789473684}
{"id": "17539923-dd06-4712-becd-68fb8d68614d", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"Context-Aware Learning to Rank with Self-Attention\" by Przemys\u0142aw Pobrotyn, Tomasz Bartczak, Miko\u0142aj Synowiec, Rados\u0142aw Bia\u0142obrzeski, and Jaros\u0142aw Bojar]\n\ncites\n\n[\"Learning a ranking model with delta features to study the effect of neighborhood on item preference in e"], "TTFT": 6.013138055801392, "answer": "[\"Influence of Neighborhood on the Preference of an Item in eCommerce Search\", \"Context-Aware Learning to Rank with Self-Attention\"]", "f1 socre": 0.31249999999999994}
{"id": "ad6d0a53-1f8a-46b5-9738-a34d0a3cafaf", "pred": ["Based on the provided instructions and the given papers, the longest citation chain is as follows:\n\n[\"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis\" by Mena, C. Gary et al. (2019), \"Churn prediction using deep convolutional neural networks and autoencoders\" by Wangperawong, A. et al. (2016)]\n\nExplanation:\n\nThe"], "TTFT": 3.672146797180176, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\"]", "f1 socre": 0.4000000000000001}
{"id": "b5b963fe-30ab-435f-9574-dcdfd96f3707", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Minimod: A Finite Difference solver for Seismic Modeling\" by Jie Meng, Andreas Atle, and Henri Calandra, 2020]\n[\"Massively scalable stencil algorithm for seismic modeling on the Cerebras WSE-2\" by Mathias Jacquelin, Mauricio Araya-Polo,"], "TTFT": 8.336519241333008, "answer": "[\"Minimod: A Finite Difference solver for Seismic Modeling\", \"Massively scalable stencil algorithm\"]", "f1 socre": 0.3368421052631579}
{"id": "7b104919-2fba-44d2-91dd-a56bd2f30e91", "pred": ["{{\"Reference\": [\"LLM A*: Human in the Loop Large Language Models Enabled A* Search for Robotics\"], \"Citation\": [\"FunSearch\", \"Various papers on search-based MAPF methods (e.g., CBS, ECBS, EECBS)\"]}}\n\nThe paper \"Evolution of Heuristics: Towards Efficient Automatic Algorithm Design Using Large Language Model\" references the paper \"LLM A*: Human in"], "TTFT": 12.563227891921997, "answer": "{\"Reference\": [\"LLM A*: Human in the Loop Large Language Models Enabled A* Search for Robotics\"], \"Citation\": [\"Ensembling Prioritized Hybrid Policies for Multi-agent Pathfinding\"]}", "f1 socre": 0.5}
{"id": "31ee8539-2e9e-40e2-adc3-082c872698f7", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\" by Niimi et al., 2024]\n\ncites\n\n[\"A PRE"], "TTFT": 11.793168783187866, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\", \"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\", \"An Efficient Multimodal Learning Framework to Comprehend Consumer Preferences Using BERT and Cross-Attention\"]", "f1 socre": 0.39682539682539686}
{"id": "9b982095-daad-4632-9573-b0d7037dd6a7", "pred": ["{{\"Reference\": [\"A Prompt for Investigating the Effectiveness of LLMs in Self-Critiquing for Planning: An Empirical Study\", \"Large Language Models Cannot Self-Correct Reasoning Intrinsically\"],\n\"Citation\": [\"LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks\"]}}"], "TTFT": 10.443064212799072, "answer": "{\"Reference\": [\"LARGE LANGUAGE MoDELS CANNOT SELF-CorRECT REASONING YET\", \"Can Large Language Models Really Improve by Self-critiquing Their Own Plans?\"], \"Citation\": []}", "f1 socre": 0.3943661971830986}
{"id": "ecded47a-3741-4458-ba61-c04271c33651", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"MonaCoBERT: Monotonic attention based ConvBERT for Knowledge Tracing\" by Unggi Lee, Yonghyun Park, Yujin Kim, Seongyune Choi, and Hyeoncheol Kim, 2021]\n\nThis paper cites:\n\n[\"Understanding the Instability of Transformer-based Mod"], "TTFT": 10.399053812026978, "answer": "[\"Very Deep Transformers for Neural Machine Translation\", \"Understanding the Difficulty of Training Transformers\", \"MonaCoBERT: Monotonic attention based ConvBERT for Knowledge Tracing\"]", "f1 socre": 0.29411764705882354}
{"id": "71d0fb39-d315-4e33-beec-c53271663f7f", "pred": ["{{\"Reference\": [\"LoRA-Switch: Boosting the Efficiency of Dynamic LLM Adapters via System-Algorithm Co-design\"], \"Citation\": []}}"], "TTFT": 13.256085872650146, "answer": "{\"Reference\": [\"ONLINE SPECULATIVE DECODING\"], \"Citation\": [\"LoRA-Switch: Boosting the Efficiency of Dynamic LLM Adapters via System-Algorithm Co-design\"]}", "f1 socre": 0.9019607843137255}
{"id": "5d482043-88d2-4246-8191-e603b7bf6ed4", "pred": ["{\n\"Title\": \"Scalable Pre-training of Large Autoregressive Image Models\",\n\"Authors\": [\"Alaaeldin El-Nouby\", \"Ali Farhat\", \"Mohamed Ragab\", \"Mostafa Dehghani\", \"Georg Heigold\", \"Chen Sun\", \"Mario Lu\u010di\u0107\", \"Cordelia Schmid\"],\n\"Reference\": [\n{\"Title\": \"Vivit"], "TTFT": 15.165497541427612, "answer": "{\"Reference\": [\"Data Filtering Networks\"], \"Citation\": [\"ARVideo: Autoregressive Pretraining for Self-Supervised Video Representation Learning\"]}", "f1 socre": 0.17073170731707318}
{"id": "014ae33b-22e7-4fb7-ae34-56f64160ce74", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n1. [\"DYNAP-SE2: a scalable multi-core dynamic neuromorphic asynchronous spiking neural network processor\" by Ole Richter, et al.]\n2. [\"Speck: A Smart event-based Vision Sensor with a low latency 327K Neuron Convolutional Neural Network Processing Pipeline\" by Ole Richter,"], "TTFT": 15.628410577774048, "answer": "[\"Speck: A Smart event-based Vision Sensor with a low latency 327K Neuron Convolutional Neuronal Network Processing Pipeline\", \"DYNAP-SE2: a scalable multi-core dynamic neuromorphic asynchronous spiking neural network processor\"]", "f1 socre": 0.7543859649122807}
{"id": "3e4ee654-dfd8-4ffd-a825-79e0997961c9", "pred": ["{{\"Reference\": [\"Navigating the Safety Landscape: Measuring LLM Cybersecurity Risks with VISAGE Metric\", \"Purple Llama CYBERSECEval: A Secure Coding Benchmark for Large Language Models\"],\n\"Citation\": [\"Navigating the Safety Landscape: Measuring LLM Cybersecurity Risks with VISAGE Metric\": \"studied the safety landscape of LLMs and discovered"], "TTFT": 12.062817811965942, "answer": "{\"Reference\": [\"Purple Llama CYBERSECEvAL: A Secure Coding Benchmark for Language Models\"], \"Citation\": [\"Navigating the Safety Landscape: Measuring Risks in Finetuning Large Language Models\"]}", "f1 socre": 0.5238095238095238}
{"id": "f359acf5-39a1-4490-ba4a-95d38de35fa7", "pred": ["{\n\"Title\": \"Soaring from 4K to 400K: Extending LLM's Context with Activation Beacon\",\n\"Authors\": [\"Peitian Zhang\", \"Zheng Liu\", \"Shitao Xiao\", \"Ninglu Shao\", \"Hongjin Qian\", \"Qiwei Ye\", \"Zhicheng Dou\"],\n\"Affiliations\": [\"Beijing Academy of Artificial Intelligence\","], "TTFT": 10.695560455322266, "answer": "{\"Reference\": [\"PoSE: EFFICIENT CONTEXT WINDOW EXTENSION OF LLMS VIA POSITIONAL SKIP-WISE TRAINING\"], \"Citation\": [\"Extending Llama-3's Context Ten-Fold Overnight\"]}", "f1 socre": 0.10752688172043011}
{"id": "2357f6df-fed9-4508-a80c-800fe6995caa", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"GTBENCH: Uncovering Game-Theoretic Characteristics of LLMs\" by Jinhao Duan, Renming Zhang, James Diffenderfer, Bhavya Kailkhura, Lichao Sun, Elias Stengel-Eskin, Mohit Bansal, Tianlong Chen, and Kaidi Xu]\n\n[\"Exploring"], "TTFT": 10.72509765625, "answer": "[\"![](https://cdn.mathpix.com/cropped/2024_06_04_791d7ec1c70f2f7bcd44g-01.jpg?height=135&width=174&top_left_y=320&top_left_x=260)\", \"Generative AI for Game Theory-based Mobile Networking\", \"Exploring Equilibrium Strategies in Network Games with Generative AI\"]", "f1 socre": 0.025477707006369425}
{"id": "aa9707c4-9506-4e51-899d-9213ad10a324", "pred": ["Based on the provided instructions, the longest citation chain from the given papers is as follows:\n\n[\"PromptMind Team at EHRSQL-2024: Improving Reliability of SQL Generation using Ensemble LLMs\", \"Angel-Optimized Text Embeddings\", \"Dolan and Brockett (2005): Building a question answering test collection\"]\n\nExplanation:\n\n1. The first paper, \"Prompt"], "TTFT": 11.649495124816895, "answer": "[\"Scaling Sentence Embeddings with Large Language Models\", \"ANGLE-OPTIMIZED TEXT EMBEDDINGS\", \"PromptMind Team at EHRSQL-2024: Improving Reliability of SQL Generation using Ensemble LLMs\"]", "f1 socre": 0.5617977528089888}
{"id": "edc6d95f-02ba-4835-bff6-853cb7d61f6b", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"The unreasonable effectiveness of optimal transport in economics\" by Galichon, 2021, \"Optimal transport in a nutshell\" by Galichon, 2021, \"Exploring the connection between optimal transport and economic applications\" by Galichon, 2016, \"Optimal Transport for Applied Mathematicians\" by Sant"], "TTFT": 13.675036668777466, "answer": "[\"Water is a radiation protection agent for ionised pyrrole\", \"The unreasonable effectiveness of optimal transport in economics\"]", "f1 socre": 0.23157894736842105}
{"id": "13a4a371-6339-4c9d-82cf-fc9ab2bb017d", "pred": ["{\n\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"],\n\"Citation\": []\n}\n\nThe paper \"OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models\" does not mention \"TinyLlama: An Open-Source Small Language Model\" in its text, so there is no citation relationship between the two papers. However, \"OpenMoE\" references \"Tiny"], "TTFT": 13.604034900665283, "answer": "{\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"], \"Citation\": [\"Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training\"]}", "f1 socre": 0.4047619047619047}
{"id": "c4a4e642-fa00-4377-a756-40f2f7ea7a74", "pred": ["{{\"Reference\": [\"A Survey of Hallucination in Large Foundation Models by Vipula Rawte, Amit Sheth, and Amitava Das\"], \"Citation\": [\"CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation by Quan Tu, Shilong Fan, Zihang Tian, Rui Yan\"]}}\n\nThis paper on CharacterEval references the survey paper on hallucination in large foundation models and"], "TTFT": 11.539960622787476, "answer": "{\"Reference\": [\"A Survey of Hallucination in \\\"Large\\\" Foundation Models\"], \"Citation\": [\"From Role-Play to Drama-Interaction: An LLM Solution\"]}", "f1 socre": 0.2745098039215686}
{"id": "835a4005-8c32-4083-b52d-4e014d1d35f5", "pred": ["Based on the provided instructions and papers, here is the longest citation chain:\n\n[\"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\" by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed, \"SUPERB: Speech processing Universal"], "TTFT": 11.292267322540283, "answer": "[\"CONTRASTIVE SEMI-SUPERVISED LEARNING FOR ASR\", \"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\", \"SUPERB: Speech processing Universal PERformance Benchmark\"]", "f1 socre": 0.38461538461538464}
{"id": "cd618a43-7176-432d-b21e-432e5c2ca4a7", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\" by Abien Fred M. Agarap, 2018, ICMLC 2018]\n\n[\"A Novel Approach Combining Recurrent Neural Network and"], "TTFT": 6.193939208984375, "answer": "[\"Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems\", \"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\"]", "f1 socre": 0.34374999999999994}
{"id": "e904afd3-5cb9-4bf7-bbee-9c8defac858c", "pred": ["Based on the provided instructions and papers, the longest citation chain can be constructed as follows:\n\n[\"On the performativity of SDG classifications in large bibliometric databases\" by Ottaviani and Stahlschmidt (2024), \"Language models are unsupervised multitask learners\" by Radford et al. (2019), \"DistilBERT: Distilling BERT: Six-Order Distillation for a Fast and Che"], "TTFT": 7.292623519897461, "answer": "[\"Contrastive Decoding: Open-ended Text Generation as Optimization\", \"An Empirical Study On Contrastive Search And Contrastive Decoding For Open-ended Text Generation\", \"On the performativity of SDG classifications in large bibliometric databases\"]", "f1 socre": 0.380952380952381}
{"id": "683f4cfd-1cfe-475d-927a-08a71d4f69e9", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\" by Niimi et al., 2024]\n\n[\"Churn Prediction with"], "TTFT": 11.674830198287964, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\", \"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\", \"An Efficient Multimodal Learning Framework to Comprehend Consumer Preferences Using BERT and Cross-Attention\"]", "f1 socre": 0.45312500000000006}
{"id": "0223faf0-f46f-459d-b1cd-45c4dab78fc2", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"A tutorial on the range variant of asymmetric numeral systems\" by James Townsend, \"Asymmetric numeral systems\" by Jarek Duda, \"Asymmetric numeral systems: entropy coding combining speed of Huffman coding with compression rate of arithmetic coding\" by Jarek Duda, Khalid Tahboub, Neeraj J. Gad"], "TTFT": 14.098109483718872, "answer": "[\"Lightweight compression with encryption based on Asymmetric Numeral Systems\", \"A tutorial on the range variant of asymmetric numeral systems\", \"ATC: an Advanced Tucker Compression library for multidimensional data\"]", "f1 socre": 0.3652173913043478}
{"id": "b7405d8e-13f0-4750-a25d-df3a67ffd39a", "pred": ["{{\"Reference\": [\"Benchmarking Large Language Models in Retrieval-Augmented Generation\", \"Retrieval-Augmented Generation in Question Answering: A Survey\"],\n\"Citation\": [\"Seven Failure Points When Engineering a Retrieval Augmented Generation System\"]}}\n\nThis paper presents the lessons learned and failure points from three case studies of implementing Retrieval-Augmented Generation (RAG) systems. It also discusses"], "TTFT": 10.79617714881897, "answer": "{\"Reference\": [\"Benchmarking Large Language Models in Retrieval-Augmented Generation\"], \"Citation\": [\"When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively\"]}", "f1 socre": 0.4186046511627907}
{"id": "150529cd-8767-477e-aa2b-28bae4970fdd", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System\" by Santos et al., \"Massively scalable stencil algorithm for the 3D wave equation on the Cerebras Wafer-Scale Engine\" by Jacquelin and Araya-Polo]\n\nThe reason for this citation chain is that the paper by Sant"], "TTFT": 15.363740921020508, "answer": "[\"Massively scalable stencil algorithm\", \"Disruptive Changes in Field Equation Modeling A Simple Interface for Wafer Scale Engines\", \"Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System\"]", "f1 socre": 0.39285714285714285}
{"id": "a8fa7aa1-2198-419b-a0de-6ba4d1c1f160", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Dance Any Beat: Blending Beats with Visuals in Dance Video Generation\" by Xuanchen Wang, Heng Wang, Weidong Cai, and Dongnan Liu],\n[\"InMoDeGAN: Interpretable Motion Decomposing Generative Adversarial Network for Video Generation\" by Yaohui Wang, Francois Bremond,"], "TTFT": 14.07155442237854, "answer": "[\"InMoDeGAN: Interpretable Motion Decomposition Generative Adversarial Network for Video Generation\", \"LATENT IMAGE ANIMATOR: <br> LEARNING TO ANIMATE IMAGES VIA LATENT SPACE NAVIGATION\", \"Dance Any Beat: Blending Beats with Visuals in Dance Video Generation\"]", "f1 socre": 0.43548387096774194}
{"id": "49680a79-a974-49b0-b7e3-85ff515bc4f2", "pred": ["{{\"Reference\": [\"Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding\"], \"Citation\": [\"Smurfs: Leveraging Multiple Proficiency Agents for Effective Tool Utilization in LLMs\" ]}}\n\nThe paper \"EasyTool: Enhancing LLM-based Agents with Concise Tool Instruction\" references the paper \"Don't Fine-Tune, Dec"], "TTFT": 10.98206639289856, "answer": "{\"Reference\": [\"Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding\"], \"Citation\": [\"Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning\"]}", "f1 socre": 0.6521739130434783}
{"id": "9f29f552-c817-4c2a-8910-b5e8ae7c014f", "pred": ["[\"Fine-Tuning LLaMA for Multi-Stage Text Retrieval\" by Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin, 2023, cited by \"Improving Text Embeddings with Large Language Models\" by Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, Furu Wei, and Jimmy Lin,"], "TTFT": 13.682081937789917, "answer": "[\"RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large Language Models\", \"Fine-Tuning LLaMA for Multi-Stage Text Retrieval\", \"Improving Text Embeddings with Large Language Models\"]", "f1 socre": 0.3225806451612903}
{"id": "30a2535f-7e73-46ad-a402-ee80ac6e4f92", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"Quantum Annealers Chain Strengths: A Simple Heuristic to Set Them All\" by Valentin Gilbert and St\u00e9phane Louise, 2024, arXiv: 2404.05443 [39]],\n[\"Benchmarking Quantum Annealers with Near-Optimal Minor Embeddings\" by Valentin Gilbert"], "TTFT": 16.3840811252594, "answer": "[\"Scaling Advantage in Approximate Optimization with Quantum Annealing\", \"4-clique Network Minor Embedding for Quantum Annealers\", \"Quantum Annealers Chain Strengths: A Simple Heuristic to Set Them All\", \"Benchmarking Quantum Annealers with Near-Optimal Minor-Embedded Instances\"]", "f1 socre": 0.40579710144927533}
{"id": "68e88dcc-6dec-4bb4-b51b-1fe8d6cd856d", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"LGDE: Local Graph-based Dictionary Expansion\" by Schindler et al., 2023]\n[\"Query2doc: Query Expansion with Large Language Models\" by Wang, Yang, and Wei, 2023]\n[\"From free text to clusters: A graph-based approach to text mining and information retrieval\" by Raghav"], "TTFT": 10.241957187652588, "answer": "[\"Query2doc: Query Expansion with Large Language Models\", \"Query Expansion by Prompting Large Language Models\", \"LGDE: Local Graph-based Dictionary Expansion\"]", "f1 socre": 0.32989690721649484}
{"id": "d7adf528-91a9-4978-a25c-29e61562a3be", "pred": ["{{\"Reference\": [\"The Pitfalls of Defining Hallucination\", \"Hallucination is Inevitable: An Innate Limitation of Large Language Models\"],\n\"Citation\": [\"Hallucination is Inevitable: An Innate Limitation of Large Language Models\", \"The Pitfalls of Defining Hallucination\"]}}\n\nIn this paper, the authors show that hallucination is inevitable for large language models (LLMs"], "TTFT": 11.103811502456665, "answer": "{\"Reference\": [\"The Pitfalls of Defining Hallucination\"], \"Citation\": [\"An ASSESSMENT OF MODEL-ON-MODEL DECEPTION\"]}", "f1 socre": 0.29333333333333333}
{"id": "949f3450-9b30-4556-be5f-ca8afe9e072f", "pred": ["{\n\"Title\": \"LLamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource Language\",\n\"Authors\": [\"Cagri Toraman\"],\n\"Reference\": [\n{\"Title\": \"BELLE: A Large-Scale Chinese Instruction Tuning Dataset for Pretrained Language Models\"},\n{\"Title\": \"Bactrain-X: A Multilingual Instruction Tuning Dataset for Low-Resource L"], "TTFT": 12.38475251197815, "answer": "{\"Reference\": [], \"Citation\": [\"LlamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource Language\", \"InstructionCP: A fast approach to transfer Large Language Models into target language\"]}", "f1 socre": 0.43010752688172044}
{"id": "8878fb61-6996-4984-9774-5ce9fb506d37", "pred": ["{\n\"Title\": \"Extreme Compression of Large Language Models via Additive Quantization\",\n\"Authors\": [\"Vage Egiazarian\", \"* 12\", \"Andrei Panferov\", \"* 12\", \"Denis Kuznedelev\", \"* 23\", \"Elias Frantar\", \"* 4\", \"Artem Babenko\", \"* 2\"],\n\"Reference\": [\n{\"Title\": \"Mistral"], "TTFT": 11.69446086883545, "answer": "{\"Reference\": [\"Mistral 7B\"], \"Citation\": [\"Sparse Expansion and Neuronal Disentanglement\"]}", "f1 socre": 0.10526315789473684}
{"id": "17539923-dd06-4712-becd-68fb8d68614d", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"Context-Aware Learning to Rank with Self-Attention\" by Przemys\u0142aw Pobrotyn, Tomasz Bartczak, Miko\u0142aj Synowiec, Rados\u0142aw Bia\u0142obrzeski, and Jaros\u0142aw Bojar]\n\ncites\n\n[\"Learning a ranking model with delta features to study the effect of neighborhood on item preference in e"], "TTFT": 6.013138055801392, "answer": "[\"Influence of Neighborhood on the Preference of an Item in eCommerce Search\", \"Context-Aware Learning to Rank with Self-Attention\"]", "f1 socre": 0.31249999999999994}
{"id": "ad6d0a53-1f8a-46b5-9738-a34d0a3cafaf", "pred": ["Based on the provided instructions and the given papers, the longest citation chain is as follows:\n\n[\"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis\" by Mena, C. Gary et al. (2019), \"Churn prediction using deep convolutional neural networks and autoencoders\" by Wangperawong, A. et al. (2016)]\n\nExplanation:\n\nThe"], "TTFT": 3.672146797180176, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\"]", "f1 socre": 0.4000000000000001}
{"id": "b5b963fe-30ab-435f-9574-dcdfd96f3707", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Minimod: A Finite Difference solver for Seismic Modeling\" by Jie Meng, Andreas Atle, and Henri Calandra, 2020]\n[\"Massively scalable stencil algorithm for seismic modeling on the Cerebras WSE-2\" by Mathias Jacquelin, Mauricio Araya-Polo,"], "TTFT": 8.336519241333008, "answer": "[\"Minimod: A Finite Difference solver for Seismic Modeling\", \"Massively scalable stencil algorithm\"]", "f1 socre": 0.3368421052631579}
{"id": "7b104919-2fba-44d2-91dd-a56bd2f30e91", "pred": ["{{\"Reference\": [\"LLM A*: Human in the Loop Large Language Models Enabled A* Search for Robotics\"], \"Citation\": [\"FunSearch\", \"Various papers on search-based MAPF methods (e.g., CBS, ECBS, EECBS)\"]}}\n\nThe paper \"Evolution of Heuristics: Towards Efficient Automatic Algorithm Design Using Large Language Model\" references the paper \"LLM A*: Human in"], "TTFT": 12.563227891921997, "answer": "{\"Reference\": [\"LLM A*: Human in the Loop Large Language Models Enabled A* Search for Robotics\"], \"Citation\": [\"Ensembling Prioritized Hybrid Policies for Multi-agent Pathfinding\"]}", "f1 socre": 0.5}
{"id": "31ee8539-2e9e-40e2-adc3-082c872698f7", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\" by Niimi et al., 2024]\n\ncites\n\n[\"A PRE"], "TTFT": 11.793168783187866, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\", \"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\", \"An Efficient Multimodal Learning Framework to Comprehend Consumer Preferences Using BERT and Cross-Attention\"]", "f1 socre": 0.39682539682539686}
{"id": "9b982095-daad-4632-9573-b0d7037dd6a7", "pred": ["{{\"Reference\": [\"A Prompt for Investigating the Effectiveness of LLMs in Self-Critiquing for Planning: An Empirical Study\", \"Large Language Models Cannot Self-Correct Reasoning Intrinsically\"],\n\"Citation\": [\"LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks\"]}}"], "TTFT": 10.443064212799072, "answer": "{\"Reference\": [\"LARGE LANGUAGE MoDELS CANNOT SELF-CorRECT REASONING YET\", \"Can Large Language Models Really Improve by Self-critiquing Their Own Plans?\"], \"Citation\": []}", "f1 socre": 0.3943661971830986}
{"id": "ecded47a-3741-4458-ba61-c04271c33651", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"MonaCoBERT: Monotonic attention based ConvBERT for Knowledge Tracing\" by Unggi Lee, Yonghyun Park, Yujin Kim, Seongyune Choi, and Hyeoncheol Kim, 2021]\n\nThis paper cites:\n\n[\"Understanding the Instability of Transformer-based Mod"], "TTFT": 10.399053812026978, "answer": "[\"Very Deep Transformers for Neural Machine Translation\", \"Understanding the Difficulty of Training Transformers\", \"MonaCoBERT: Monotonic attention based ConvBERT for Knowledge Tracing\"]", "f1 socre": 0.29411764705882354}
{"id": "71d0fb39-d315-4e33-beec-c53271663f7f", "pred": ["{{\"Reference\": [\"LoRA-Switch: Boosting the Efficiency of Dynamic LLM Adapters via System-Algorithm Co-design\"], \"Citation\": []}}"], "TTFT": 13.256085872650146, "answer": "{\"Reference\": [\"ONLINE SPECULATIVE DECODING\"], \"Citation\": [\"LoRA-Switch: Boosting the Efficiency of Dynamic LLM Adapters via System-Algorithm Co-design\"]}", "f1 socre": 0.9019607843137255}
{"id": "5d482043-88d2-4246-8191-e603b7bf6ed4", "pred": ["{\n\"Title\": \"Scalable Pre-training of Large Autoregressive Image Models\",\n\"Authors\": [\"Alaaeldin El-Nouby\", \"Ali Farhat\", \"Mohamed Ragab\", \"Mostafa Dehghani\", \"Georg Heigold\", \"Chen Sun\", \"Mario Lu\u010di\u0107\", \"Cordelia Schmid\"],\n\"Reference\": [\n{\"Title\": \"Vivit"], "TTFT": 15.165497541427612, "answer": "{\"Reference\": [\"Data Filtering Networks\"], \"Citation\": [\"ARVideo: Autoregressive Pretraining for Self-Supervised Video Representation Learning\"]}", "f1 socre": 0.17073170731707318}
{"id": "014ae33b-22e7-4fb7-ae34-56f64160ce74", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n1. [\"DYNAP-SE2: a scalable multi-core dynamic neuromorphic asynchronous spiking neural network processor\" by Ole Richter, et al.]\n2. [\"Speck: A Smart event-based Vision Sensor with a low latency 327K Neuron Convolutional Neural Network Processing Pipeline\" by Ole Richter,"], "TTFT": 15.628410577774048, "answer": "[\"Speck: A Smart event-based Vision Sensor with a low latency 327K Neuron Convolutional Neuronal Network Processing Pipeline\", \"DYNAP-SE2: a scalable multi-core dynamic neuromorphic asynchronous spiking neural network processor\"]", "f1 socre": 0.7543859649122807}
{"id": "3e4ee654-dfd8-4ffd-a825-79e0997961c9", "pred": ["{{\"Reference\": [\"Navigating the Safety Landscape: Measuring LLM Cybersecurity Risks with VISAGE Metric\", \"Purple Llama CYBERSECEval: A Secure Coding Benchmark for Large Language Models\"],\n\"Citation\": [\"Navigating the Safety Landscape: Measuring LLM Cybersecurity Risks with VISAGE Metric\": \"studied the safety landscape of LLMs and discovered"], "TTFT": 12.062817811965942, "answer": "{\"Reference\": [\"Purple Llama CYBERSECEvAL: A Secure Coding Benchmark for Language Models\"], \"Citation\": [\"Navigating the Safety Landscape: Measuring Risks in Finetuning Large Language Models\"]}", "f1 socre": 0.5238095238095238}
{"id": "f359acf5-39a1-4490-ba4a-95d38de35fa7", "pred": ["{\n\"Title\": \"Soaring from 4K to 400K: Extending LLM's Context with Activation Beacon\",\n\"Authors\": [\"Peitian Zhang\", \"Zheng Liu\", \"Shitao Xiao\", \"Ninglu Shao\", \"Hongjin Qian\", \"Qiwei Ye\", \"Zhicheng Dou\"],\n\"Affiliations\": [\"Beijing Academy of Artificial Intelligence\","], "TTFT": 10.695560455322266, "answer": "{\"Reference\": [\"PoSE: EFFICIENT CONTEXT WINDOW EXTENSION OF LLMS VIA POSITIONAL SKIP-WISE TRAINING\"], \"Citation\": [\"Extending Llama-3's Context Ten-Fold Overnight\"]}", "f1 socre": 0.10752688172043011}
{"id": "2357f6df-fed9-4508-a80c-800fe6995caa", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"GTBENCH: Uncovering Game-Theoretic Characteristics of LLMs\" by Jinhao Duan, Renming Zhang, James Diffenderfer, Bhavya Kailkhura, Lichao Sun, Elias Stengel-Eskin, Mohit Bansal, Tianlong Chen, and Kaidi Xu]\n\n[\"Exploring"], "TTFT": 10.72509765625, "answer": "[\"![](https://cdn.mathpix.com/cropped/2024_06_04_791d7ec1c70f2f7bcd44g-01.jpg?height=135&width=174&top_left_y=320&top_left_x=260)\", \"Generative AI for Game Theory-based Mobile Networking\", \"Exploring Equilibrium Strategies in Network Games with Generative AI\"]", "f1 socre": 0.025477707006369425}
{"id": "aa9707c4-9506-4e51-899d-9213ad10a324", "pred": ["Based on the provided instructions, the longest citation chain from the given papers is as follows:\n\n[\"PromptMind Team at EHRSQL-2024: Improving Reliability of SQL Generation using Ensemble LLMs\", \"Angel-Optimized Text Embeddings\", \"Dolan and Brockett (2005): Building a question answering test collection\"]\n\nExplanation:\n\n1. The first paper, \"Prompt"], "TTFT": 11.649495124816895, "answer": "[\"Scaling Sentence Embeddings with Large Language Models\", \"ANGLE-OPTIMIZED TEXT EMBEDDINGS\", \"PromptMind Team at EHRSQL-2024: Improving Reliability of SQL Generation using Ensemble LLMs\"]", "f1 socre": 0.5617977528089888}
{"id": "edc6d95f-02ba-4835-bff6-853cb7d61f6b", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"The unreasonable effectiveness of optimal transport in economics\" by Galichon, 2021, \"Optimal transport in a nutshell\" by Galichon, 2021, \"Exploring the connection between optimal transport and economic applications\" by Galichon, 2016, \"Optimal Transport for Applied Mathematicians\" by Sant"], "TTFT": 13.675036668777466, "answer": "[\"Water is a radiation protection agent for ionised pyrrole\", \"The unreasonable effectiveness of optimal transport in economics\"]", "f1 socre": 0.23157894736842105}
{"id": "a780dc25-81d8-4e0f-b344-5086b8055e04", "pred": ["Based on the provided instructions, the longest citation chain can be constructed as follows:\n\n[\"Beyond Traditional Magnetic Resonance Processing with Artificial Intelligence\" by Amir Jahangiri and Vladislav Orekhov, 2024],\n[\"Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems\" by Tsung-Hsien Wen, Milica Ga\u0161i\u0107, Nikola Mrk\u0161"], "TTFT": 11.49069619178772, "answer": "[\"Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems\", \"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\", \"Deep Learning using Rectified Linear Units (ReLU)\", \"BEYOND TRADITIONAL MAGNETIC RESONANCE PROCESSING WITH ARTIFICIAL INTELLIGENCE *\"]", "f1 socre": 0.35294117647058826}
{"id": "13a4a371-6339-4c9d-82cf-fc9ab2bb017d", "pred": ["{\n\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"],\n\"Citation\": []\n}\n\nThe paper \"OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models\" does not mention \"TinyLlama: An Open-Source Small Language Model\" in its text, so there is no citation relationship between the two papers. However, \"OpenMoE\" references \"Tiny"], "TTFT": 13.604034900665283, "answer": "{\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"], \"Citation\": [\"Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training\"]}", "f1 socre": 0.4047619047619047}
{"id": "c4a4e642-fa00-4377-a756-40f2f7ea7a74", "pred": ["{{\"Reference\": [\"A Survey of Hallucination in Large Foundation Models by Vipula Rawte, Amit Sheth, and Amitava Das\"], \"Citation\": [\"CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation by Quan Tu, Shilong Fan, Zihang Tian, Rui Yan\"]}}\n\nThis paper on CharacterEval references the survey paper on hallucination in large foundation models and"], "TTFT": 11.539960622787476, "answer": "{\"Reference\": [\"A Survey of Hallucination in \\\"Large\\\" Foundation Models\"], \"Citation\": [\"From Role-Play to Drama-Interaction: An LLM Solution\"]}", "f1 socre": 0.2745098039215686}
{"id": "835a4005-8c32-4083-b52d-4e014d1d35f5", "pred": ["Based on the provided instructions and papers, here is the longest citation chain:\n\n[\"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\" by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed, \"SUPERB: Speech processing Universal"], "TTFT": 11.292267322540283, "answer": "[\"CONTRASTIVE SEMI-SUPERVISED LEARNING FOR ASR\", \"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\", \"SUPERB: Speech processing Universal PERformance Benchmark\"]", "f1 socre": 0.38461538461538464}
{"id": "cd618a43-7176-432d-b21e-432e5c2ca4a7", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\" by Abien Fred M. Agarap, 2018, ICMLC 2018]\n\n[\"A Novel Approach Combining Recurrent Neural Network and"], "TTFT": 6.193939208984375, "answer": "[\"Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems\", \"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\"]", "f1 socre": 0.34374999999999994}
{"id": "e904afd3-5cb9-4bf7-bbee-9c8defac858c", "pred": ["Based on the provided instructions and papers, the longest citation chain can be constructed as follows:\n\n[\"On the performativity of SDG classifications in large bibliometric databases\" by Ottaviani and Stahlschmidt (2024), \"Language models are unsupervised multitask learners\" by Radford et al. (2019), \"DistilBERT: Distilling BERT: Six-Order Distillation for a Fast and Che"], "TTFT": 7.292623519897461, "answer": "[\"Contrastive Decoding: Open-ended Text Generation as Optimization\", \"An Empirical Study On Contrastive Search And Contrastive Decoding For Open-ended Text Generation\", \"On the performativity of SDG classifications in large bibliometric databases\"]", "f1 socre": 0.380952380952381}
{"id": "683f4cfd-1cfe-475d-927a-08a71d4f69e9", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\" by Niimi et al., 2024]\n\n[\"Churn Prediction with"], "TTFT": 11.674830198287964, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\", \"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\", \"An Efficient Multimodal Learning Framework to Comprehend Consumer Preferences Using BERT and Cross-Attention\"]", "f1 socre": 0.45312500000000006}
{"id": "0223faf0-f46f-459d-b1cd-45c4dab78fc2", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"A tutorial on the range variant of asymmetric numeral systems\" by James Townsend, \"Asymmetric numeral systems\" by Jarek Duda, \"Asymmetric numeral systems: entropy coding combining speed of Huffman coding with compression rate of arithmetic coding\" by Jarek Duda, Khalid Tahboub, Neeraj J. Gad"], "TTFT": 14.098109483718872, "answer": "[\"Lightweight compression with encryption based on Asymmetric Numeral Systems\", \"A tutorial on the range variant of asymmetric numeral systems\", \"ATC: an Advanced Tucker Compression library for multidimensional data\"]", "f1 socre": 0.3652173913043478}
{"id": "b7405d8e-13f0-4750-a25d-df3a67ffd39a", "pred": ["{{\"Reference\": [\"Benchmarking Large Language Models in Retrieval-Augmented Generation\", \"Retrieval-Augmented Generation in Question Answering: A Survey\"],\n\"Citation\": [\"Seven Failure Points When Engineering a Retrieval Augmented Generation System\"]}}\n\nThis paper presents the lessons learned and failure points from three case studies of implementing Retrieval-Augmented Generation (RAG) systems. It also discusses"], "TTFT": 10.79617714881897, "answer": "{\"Reference\": [\"Benchmarking Large Language Models in Retrieval-Augmented Generation\"], \"Citation\": [\"When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively\"]}", "f1 socre": 0.4186046511627907}
{"id": "150529cd-8767-477e-aa2b-28bae4970fdd", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System\" by Santos et al., \"Massively scalable stencil algorithm for the 3D wave equation on the Cerebras Wafer-Scale Engine\" by Jacquelin and Araya-Polo]\n\nThe reason for this citation chain is that the paper by Sant"], "TTFT": 15.363740921020508, "answer": "[\"Massively scalable stencil algorithm\", \"Disruptive Changes in Field Equation Modeling A Simple Interface for Wafer Scale Engines\", \"Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System\"]", "f1 socre": 0.39285714285714285}
{"id": "a8fa7aa1-2198-419b-a0de-6ba4d1c1f160", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Dance Any Beat: Blending Beats with Visuals in Dance Video Generation\" by Xuanchen Wang, Heng Wang, Weidong Cai, and Dongnan Liu],\n[\"InMoDeGAN: Interpretable Motion Decomposing Generative Adversarial Network for Video Generation\" by Yaohui Wang, Francois Bremond,"], "TTFT": 14.07155442237854, "answer": "[\"InMoDeGAN: Interpretable Motion Decomposition Generative Adversarial Network for Video Generation\", \"LATENT IMAGE ANIMATOR: <br> LEARNING TO ANIMATE IMAGES VIA LATENT SPACE NAVIGATION\", \"Dance Any Beat: Blending Beats with Visuals in Dance Video Generation\"]", "f1 socre": 0.43548387096774194}
{"id": "49680a79-a974-49b0-b7e3-85ff515bc4f2", "pred": ["{{\"Reference\": [\"Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding\"], \"Citation\": [\"Smurfs: Leveraging Multiple Proficiency Agents for Effective Tool Utilization in LLMs\" ]}}\n\nThe paper \"EasyTool: Enhancing LLM-based Agents with Concise Tool Instruction\" references the paper \"Don't Fine-Tune, Dec"], "TTFT": 10.98206639289856, "answer": "{\"Reference\": [\"Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding\"], \"Citation\": [\"Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning\"]}", "f1 socre": 0.6521739130434783}
{"id": "9f29f552-c817-4c2a-8910-b5e8ae7c014f", "pred": ["[\"Fine-Tuning LLaMA for Multi-Stage Text Retrieval\" by Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin, 2023, cited by \"Improving Text Embeddings with Large Language Models\" by Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, Furu Wei, and Jimmy Lin,"], "TTFT": 13.682081937789917, "answer": "[\"RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large Language Models\", \"Fine-Tuning LLaMA for Multi-Stage Text Retrieval\", \"Improving Text Embeddings with Large Language Models\"]", "f1 socre": 0.3225806451612903}
{"id": "30a2535f-7e73-46ad-a402-ee80ac6e4f92", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"Quantum Annealers Chain Strengths: A Simple Heuristic to Set Them All\" by Valentin Gilbert and St\u00e9phane Louise, 2024, arXiv: 2404.05443 [39]],\n[\"Benchmarking Quantum Annealers with Near-Optimal Minor Embeddings\" by Valentin Gilbert"], "TTFT": 16.3840811252594, "answer": "[\"Scaling Advantage in Approximate Optimization with Quantum Annealing\", \"4-clique Network Minor Embedding for Quantum Annealers\", \"Quantum Annealers Chain Strengths: A Simple Heuristic to Set Them All\", \"Benchmarking Quantum Annealers with Near-Optimal Minor-Embedded Instances\"]", "f1 socre": 0.40579710144927533}
{"id": "68e88dcc-6dec-4bb4-b51b-1fe8d6cd856d", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"LGDE: Local Graph-based Dictionary Expansion\" by Schindler et al., 2023]\n[\"Query2doc: Query Expansion with Large Language Models\" by Wang, Yang, and Wei, 2023]\n[\"From free text to clusters: A graph-based approach to text mining and information retrieval\" by Raghav"], "TTFT": 10.241957187652588, "answer": "[\"Query2doc: Query Expansion with Large Language Models\", \"Query Expansion by Prompting Large Language Models\", \"LGDE: Local Graph-based Dictionary Expansion\"]", "f1 socre": 0.32989690721649484}
{"id": "d7adf528-91a9-4978-a25c-29e61562a3be", "pred": ["{{\"Reference\": [\"The Pitfalls of Defining Hallucination\", \"Hallucination is Inevitable: An Innate Limitation of Large Language Models\"],\n\"Citation\": [\"Hallucination is Inevitable: An Innate Limitation of Large Language Models\", \"The Pitfalls of Defining Hallucination\"]}}\n\nIn this paper, the authors show that hallucination is inevitable for large language models (LLMs"], "TTFT": 11.103811502456665, "answer": "{\"Reference\": [\"The Pitfalls of Defining Hallucination\"], \"Citation\": [\"An ASSESSMENT OF MODEL-ON-MODEL DECEPTION\"]}", "f1 socre": 0.29333333333333333}
{"id": "949f3450-9b30-4556-be5f-ca8afe9e072f", "pred": ["{\n\"Title\": \"LLamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource Language\",\n\"Authors\": [\"Cagri Toraman\"],\n\"Reference\": [\n{\"Title\": \"BELLE: A Large-Scale Chinese Instruction Tuning Dataset for Pretrained Language Models\"},\n{\"Title\": \"Bactrain-X: A Multilingual Instruction Tuning Dataset for Low-Resource L"], "TTFT": 12.38475251197815, "answer": "{\"Reference\": [], \"Citation\": [\"LlamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource Language\", \"InstructionCP: A fast approach to transfer Large Language Models into target language\"]}", "f1 socre": 0.43010752688172044}
{"id": "8878fb61-6996-4984-9774-5ce9fb506d37", "pred": ["{\n\"Title\": \"Extreme Compression of Large Language Models via Additive Quantization\",\n\"Authors\": [\"Vage Egiazarian\", \"* 12\", \"Andrei Panferov\", \"* 12\", \"Denis Kuznedelev\", \"* 23\", \"Elias Frantar\", \"* 4\", \"Artem Babenko\", \"* 2\"],\n\"Reference\": [\n{\"Title\": \"Mistral"], "TTFT": 11.69446086883545, "answer": "{\"Reference\": [\"Mistral 7B\"], \"Citation\": [\"Sparse Expansion and Neuronal Disentanglement\"]}", "f1 socre": 0.10526315789473684}
{"id": "17539923-dd06-4712-becd-68fb8d68614d", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"Context-Aware Learning to Rank with Self-Attention\" by Przemys\u0142aw Pobrotyn, Tomasz Bartczak, Miko\u0142aj Synowiec, Rados\u0142aw Bia\u0142obrzeski, and Jaros\u0142aw Bojar]\n\ncites\n\n[\"Learning a ranking model with delta features to study the effect of neighborhood on item preference in e"], "TTFT": 6.013138055801392, "answer": "[\"Influence of Neighborhood on the Preference of an Item in eCommerce Search\", \"Context-Aware Learning to Rank with Self-Attention\"]", "f1 socre": 0.31249999999999994}
{"id": "ad6d0a53-1f8a-46b5-9738-a34d0a3cafaf", "pred": ["Based on the provided instructions and the given papers, the longest citation chain is as follows:\n\n[\"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis\" by Mena, C. Gary et al. (2019), \"Churn prediction using deep convolutional neural networks and autoencoders\" by Wangperawong, A. et al. (2016)]\n\nExplanation:\n\nThe"], "TTFT": 3.672146797180176, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\"]", "f1 socre": 0.4000000000000001}
{"id": "b5b963fe-30ab-435f-9574-dcdfd96f3707", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Minimod: A Finite Difference solver for Seismic Modeling\" by Jie Meng, Andreas Atle, and Henri Calandra, 2020]\n[\"Massively scalable stencil algorithm for seismic modeling on the Cerebras WSE-2\" by Mathias Jacquelin, Mauricio Araya-Polo,"], "TTFT": 8.336519241333008, "answer": "[\"Minimod: A Finite Difference solver for Seismic Modeling\", \"Massively scalable stencil algorithm\"]", "f1 socre": 0.3368421052631579}
{"id": "7b104919-2fba-44d2-91dd-a56bd2f30e91", "pred": ["{{\"Reference\": [\"LLM A*: Human in the Loop Large Language Models Enabled A* Search for Robotics\"], \"Citation\": [\"FunSearch\", \"Various papers on search-based MAPF methods (e.g., CBS, ECBS, EECBS)\"]}}\n\nThe paper \"Evolution of Heuristics: Towards Efficient Automatic Algorithm Design Using Large Language Model\" references the paper \"LLM A*: Human in"], "TTFT": 12.563227891921997, "answer": "{\"Reference\": [\"LLM A*: Human in the Loop Large Language Models Enabled A* Search for Robotics\"], \"Citation\": [\"Ensembling Prioritized Hybrid Policies for Multi-agent Pathfinding\"]}", "f1 socre": 0.5}
{"id": "31ee8539-2e9e-40e2-adc3-082c872698f7", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\" by Niimi et al., 2024]\n\ncites\n\n[\"A PRE"], "TTFT": 11.793168783187866, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\", \"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\", \"An Efficient Multimodal Learning Framework to Comprehend Consumer Preferences Using BERT and Cross-Attention\"]", "f1 socre": 0.39682539682539686}
{"id": "9b982095-daad-4632-9573-b0d7037dd6a7", "pred": ["{{\"Reference\": [\"A Prompt for Investigating the Effectiveness of LLMs in Self-Critiquing for Planning: An Empirical Study\", \"Large Language Models Cannot Self-Correct Reasoning Intrinsically\"],\n\"Citation\": [\"LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks\"]}}"], "TTFT": 10.443064212799072, "answer": "{\"Reference\": [\"LARGE LANGUAGE MoDELS CANNOT SELF-CorRECT REASONING YET\", \"Can Large Language Models Really Improve by Self-critiquing Their Own Plans?\"], \"Citation\": []}", "f1 socre": 0.3943661971830986}
{"id": "ecded47a-3741-4458-ba61-c04271c33651", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"MonaCoBERT: Monotonic attention based ConvBERT for Knowledge Tracing\" by Unggi Lee, Yonghyun Park, Yujin Kim, Seongyune Choi, and Hyeoncheol Kim, 2021]\n\nThis paper cites:\n\n[\"Understanding the Instability of Transformer-based Mod"], "TTFT": 10.399053812026978, "answer": "[\"Very Deep Transformers for Neural Machine Translation\", \"Understanding the Difficulty of Training Transformers\", \"MonaCoBERT: Monotonic attention based ConvBERT for Knowledge Tracing\"]", "f1 socre": 0.29411764705882354}
{"id": "71d0fb39-d315-4e33-beec-c53271663f7f", "pred": ["{{\"Reference\": [\"LoRA-Switch: Boosting the Efficiency of Dynamic LLM Adapters via System-Algorithm Co-design\"], \"Citation\": []}}"], "TTFT": 13.256085872650146, "answer": "{\"Reference\": [\"ONLINE SPECULATIVE DECODING\"], \"Citation\": [\"LoRA-Switch: Boosting the Efficiency of Dynamic LLM Adapters via System-Algorithm Co-design\"]}", "f1 socre": 0.9019607843137255}
{"id": "5d482043-88d2-4246-8191-e603b7bf6ed4", "pred": ["{\n\"Title\": \"Scalable Pre-training of Large Autoregressive Image Models\",\n\"Authors\": [\"Alaaeldin El-Nouby\", \"Ali Farhat\", \"Mohamed Ragab\", \"Mostafa Dehghani\", \"Georg Heigold\", \"Chen Sun\", \"Mario Lu\u010di\u0107\", \"Cordelia Schmid\"],\n\"Reference\": [\n{\"Title\": \"Vivit"], "TTFT": 15.165497541427612, "answer": "{\"Reference\": [\"Data Filtering Networks\"], \"Citation\": [\"ARVideo: Autoregressive Pretraining for Self-Supervised Video Representation Learning\"]}", "f1 socre": 0.17073170731707318}
{"id": "014ae33b-22e7-4fb7-ae34-56f64160ce74", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n1. [\"DYNAP-SE2: a scalable multi-core dynamic neuromorphic asynchronous spiking neural network processor\" by Ole Richter, et al.]\n2. [\"Speck: A Smart event-based Vision Sensor with a low latency 327K Neuron Convolutional Neural Network Processing Pipeline\" by Ole Richter,"], "TTFT": 15.628410577774048, "answer": "[\"Speck: A Smart event-based Vision Sensor with a low latency 327K Neuron Convolutional Neuronal Network Processing Pipeline\", \"DYNAP-SE2: a scalable multi-core dynamic neuromorphic asynchronous spiking neural network processor\"]", "f1 socre": 0.7543859649122807}
{"id": "3e4ee654-dfd8-4ffd-a825-79e0997961c9", "pred": ["{{\"Reference\": [\"Navigating the Safety Landscape: Measuring LLM Cybersecurity Risks with VISAGE Metric\", \"Purple Llama CYBERSECEval: A Secure Coding Benchmark for Large Language Models\"],\n\"Citation\": [\"Navigating the Safety Landscape: Measuring LLM Cybersecurity Risks with VISAGE Metric\": \"studied the safety landscape of LLMs and discovered"], "TTFT": 12.062817811965942, "answer": "{\"Reference\": [\"Purple Llama CYBERSECEvAL: A Secure Coding Benchmark for Language Models\"], \"Citation\": [\"Navigating the Safety Landscape: Measuring Risks in Finetuning Large Language Models\"]}", "f1 socre": 0.5238095238095238}
{"id": "f359acf5-39a1-4490-ba4a-95d38de35fa7", "pred": ["{\n\"Title\": \"Soaring from 4K to 400K: Extending LLM's Context with Activation Beacon\",\n\"Authors\": [\"Peitian Zhang\", \"Zheng Liu\", \"Shitao Xiao\", \"Ninglu Shao\", \"Hongjin Qian\", \"Qiwei Ye\", \"Zhicheng Dou\"],\n\"Affiliations\": [\"Beijing Academy of Artificial Intelligence\","], "TTFT": 10.695560455322266, "answer": "{\"Reference\": [\"PoSE: EFFICIENT CONTEXT WINDOW EXTENSION OF LLMS VIA POSITIONAL SKIP-WISE TRAINING\"], \"Citation\": [\"Extending Llama-3's Context Ten-Fold Overnight\"]}", "f1 socre": 0.10752688172043011}
{"id": "2357f6df-fed9-4508-a80c-800fe6995caa", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"GTBENCH: Uncovering Game-Theoretic Characteristics of LLMs\" by Jinhao Duan, Renming Zhang, James Diffenderfer, Bhavya Kailkhura, Lichao Sun, Elias Stengel-Eskin, Mohit Bansal, Tianlong Chen, and Kaidi Xu]\n\n[\"Exploring"], "TTFT": 10.72509765625, "answer": "[\"![](https://cdn.mathpix.com/cropped/2024_06_04_791d7ec1c70f2f7bcd44g-01.jpg?height=135&width=174&top_left_y=320&top_left_x=260)\", \"Generative AI for Game Theory-based Mobile Networking\", \"Exploring Equilibrium Strategies in Network Games with Generative AI\"]", "f1 socre": 0.025477707006369425}
{"id": "aa9707c4-9506-4e51-899d-9213ad10a324", "pred": ["Based on the provided instructions, the longest citation chain from the given papers is as follows:\n\n[\"PromptMind Team at EHRSQL-2024: Improving Reliability of SQL Generation using Ensemble LLMs\", \"Angel-Optimized Text Embeddings\", \"Dolan and Brockett (2005): Building a question answering test collection\"]\n\nExplanation:\n\n1. The first paper, \"Prompt"], "TTFT": 11.649495124816895, "answer": "[\"Scaling Sentence Embeddings with Large Language Models\", \"ANGLE-OPTIMIZED TEXT EMBEDDINGS\", \"PromptMind Team at EHRSQL-2024: Improving Reliability of SQL Generation using Ensemble LLMs\"]", "f1 socre": 0.5617977528089888}
{"id": "edc6d95f-02ba-4835-bff6-853cb7d61f6b", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"The unreasonable effectiveness of optimal transport in economics\" by Galichon, 2021, \"Optimal transport in a nutshell\" by Galichon, 2021, \"Exploring the connection between optimal transport and economic applications\" by Galichon, 2016, \"Optimal Transport for Applied Mathematicians\" by Sant"], "TTFT": 13.675036668777466, "answer": "[\"Water is a radiation protection agent for ionised pyrrole\", \"The unreasonable effectiveness of optimal transport in economics\"]", "f1 socre": 0.23157894736842105}
{"id": "a780dc25-81d8-4e0f-b344-5086b8055e04", "pred": ["Based on the provided instructions, the longest citation chain can be constructed as follows:\n\n[\"Beyond Traditional Magnetic Resonance Processing with Artificial Intelligence\" by Amir Jahangiri and Vladislav Orekhov, 2024],\n[\"Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems\" by Tsung-Hsien Wen, Milica Ga\u0161i\u0107, Nikola Mrk\u0161"], "TTFT": 11.49069619178772, "answer": "[\"Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems\", \"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\", \"Deep Learning using Rectified Linear Units (ReLU)\", \"BEYOND TRADITIONAL MAGNETIC RESONANCE PROCESSING WITH ARTIFICIAL INTELLIGENCE *\"]", "f1 socre": 0.35294117647058826}
{"id": "1df3da1b-87c8-4692-9ec7-83d1b0e9caa3", "pred": ["{\n\"Title\": \"Break the Sequential Dependency of LLM Inference Using LOOKAHEAD DECODING\",\n\"Authors\": [\"Yichao Fu\", \"Peter Bailis\", \"Ion Stoica\", \"Hao Zhang\"],\n\"Reference\": [\n{\"Title\": \"Attention is all you need\"},\n{\"Title\": \"Gqa: Training generalized multi-query transformer models from multi-head checkpoints\"},\n"], "TTFT": 9.91625428199768, "answer": "{\"Reference\": [\"Mistral 7B\"], \"Citation\": [\"Kangaroo: Lossless Self-Speculative Decoding via Double Early Exiting\"]}", "f1 socre": 0.04938271604938271}
{"id": "13a4a371-6339-4c9d-82cf-fc9ab2bb017d", "pred": ["{\n\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"],\n\"Citation\": []\n}\n\nThe paper \"OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models\" does not mention \"TinyLlama: An Open-Source Small Language Model\" in its text, so there is no citation relationship between the two papers. However, \"OpenMoE\" references \"Tiny"], "TTFT": 13.604034900665283, "answer": "{\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"], \"Citation\": [\"Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training\"]}", "f1 socre": 0.4047619047619047}
{"id": "c4a4e642-fa00-4377-a756-40f2f7ea7a74", "pred": ["{{\"Reference\": [\"A Survey of Hallucination in Large Foundation Models by Vipula Rawte, Amit Sheth, and Amitava Das\"], \"Citation\": [\"CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation by Quan Tu, Shilong Fan, Zihang Tian, Rui Yan\"]}}\n\nThis paper on CharacterEval references the survey paper on hallucination in large foundation models and"], "TTFT": 11.539960622787476, "answer": "{\"Reference\": [\"A Survey of Hallucination in \\\"Large\\\" Foundation Models\"], \"Citation\": [\"From Role-Play to Drama-Interaction: An LLM Solution\"]}", "f1 socre": 0.2745098039215686}
{"id": "835a4005-8c32-4083-b52d-4e014d1d35f5", "pred": ["Based on the provided instructions and papers, here is the longest citation chain:\n\n[\"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\" by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed, \"SUPERB: Speech processing Universal"], "TTFT": 11.292267322540283, "answer": "[\"CONTRASTIVE SEMI-SUPERVISED LEARNING FOR ASR\", \"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\", \"SUPERB: Speech processing Universal PERformance Benchmark\"]", "f1 socre": 0.38461538461538464}
{"id": "cd618a43-7176-432d-b21e-432e5c2ca4a7", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\" by Abien Fred M. Agarap, 2018, ICMLC 2018]\n\n[\"A Novel Approach Combining Recurrent Neural Network and"], "TTFT": 6.193939208984375, "answer": "[\"Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems\", \"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\"]", "f1 socre": 0.34374999999999994}
{"id": "e904afd3-5cb9-4bf7-bbee-9c8defac858c", "pred": ["Based on the provided instructions and papers, the longest citation chain can be constructed as follows:\n\n[\"On the performativity of SDG classifications in large bibliometric databases\" by Ottaviani and Stahlschmidt (2024), \"Language models are unsupervised multitask learners\" by Radford et al. (2019), \"DistilBERT: Distilling BERT: Six-Order Distillation for a Fast and Che"], "TTFT": 7.292623519897461, "answer": "[\"Contrastive Decoding: Open-ended Text Generation as Optimization\", \"An Empirical Study On Contrastive Search And Contrastive Decoding For Open-ended Text Generation\", \"On the performativity of SDG classifications in large bibliometric databases\"]", "f1 socre": 0.380952380952381}
{"id": "683f4cfd-1cfe-475d-927a-08a71d4f69e9", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\" by Niimi et al., 2024]\n\n[\"Churn Prediction with"], "TTFT": 11.674830198287964, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\", \"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\", \"An Efficient Multimodal Learning Framework to Comprehend Consumer Preferences Using BERT and Cross-Attention\"]", "f1 socre": 0.45312500000000006}
{"id": "0223faf0-f46f-459d-b1cd-45c4dab78fc2", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"A tutorial on the range variant of asymmetric numeral systems\" by James Townsend, \"Asymmetric numeral systems\" by Jarek Duda, \"Asymmetric numeral systems: entropy coding combining speed of Huffman coding with compression rate of arithmetic coding\" by Jarek Duda, Khalid Tahboub, Neeraj J. Gad"], "TTFT": 14.098109483718872, "answer": "[\"Lightweight compression with encryption based on Asymmetric Numeral Systems\", \"A tutorial on the range variant of asymmetric numeral systems\", \"ATC: an Advanced Tucker Compression library for multidimensional data\"]", "f1 socre": 0.3652173913043478}
{"id": "b7405d8e-13f0-4750-a25d-df3a67ffd39a", "pred": ["{{\"Reference\": [\"Benchmarking Large Language Models in Retrieval-Augmented Generation\", \"Retrieval-Augmented Generation in Question Answering: A Survey\"],\n\"Citation\": [\"Seven Failure Points When Engineering a Retrieval Augmented Generation System\"]}}\n\nThis paper presents the lessons learned and failure points from three case studies of implementing Retrieval-Augmented Generation (RAG) systems. It also discusses"], "TTFT": 10.79617714881897, "answer": "{\"Reference\": [\"Benchmarking Large Language Models in Retrieval-Augmented Generation\"], \"Citation\": [\"When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively\"]}", "f1 socre": 0.4186046511627907}
{"id": "150529cd-8767-477e-aa2b-28bae4970fdd", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System\" by Santos et al., \"Massively scalable stencil algorithm for the 3D wave equation on the Cerebras Wafer-Scale Engine\" by Jacquelin and Araya-Polo]\n\nThe reason for this citation chain is that the paper by Sant"], "TTFT": 15.363740921020508, "answer": "[\"Massively scalable stencil algorithm\", \"Disruptive Changes in Field Equation Modeling A Simple Interface for Wafer Scale Engines\", \"Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System\"]", "f1 socre": 0.39285714285714285}
{"id": "a8fa7aa1-2198-419b-a0de-6ba4d1c1f160", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Dance Any Beat: Blending Beats with Visuals in Dance Video Generation\" by Xuanchen Wang, Heng Wang, Weidong Cai, and Dongnan Liu],\n[\"InMoDeGAN: Interpretable Motion Decomposing Generative Adversarial Network for Video Generation\" by Yaohui Wang, Francois Bremond,"], "TTFT": 14.07155442237854, "answer": "[\"InMoDeGAN: Interpretable Motion Decomposition Generative Adversarial Network for Video Generation\", \"LATENT IMAGE ANIMATOR: <br> LEARNING TO ANIMATE IMAGES VIA LATENT SPACE NAVIGATION\", \"Dance Any Beat: Blending Beats with Visuals in Dance Video Generation\"]", "f1 socre": 0.43548387096774194}
{"id": "49680a79-a974-49b0-b7e3-85ff515bc4f2", "pred": ["{{\"Reference\": [\"Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding\"], \"Citation\": [\"Smurfs: Leveraging Multiple Proficiency Agents for Effective Tool Utilization in LLMs\" ]}}\n\nThe paper \"EasyTool: Enhancing LLM-based Agents with Concise Tool Instruction\" references the paper \"Don't Fine-Tune, Dec"], "TTFT": 10.98206639289856, "answer": "{\"Reference\": [\"Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding\"], \"Citation\": [\"Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning\"]}", "f1 socre": 0.6521739130434783}
{"id": "9f29f552-c817-4c2a-8910-b5e8ae7c014f", "pred": ["[\"Fine-Tuning LLaMA for Multi-Stage Text Retrieval\" by Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin, 2023, cited by \"Improving Text Embeddings with Large Language Models\" by Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, Furu Wei, and Jimmy Lin,"], "TTFT": 13.682081937789917, "answer": "[\"RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large Language Models\", \"Fine-Tuning LLaMA for Multi-Stage Text Retrieval\", \"Improving Text Embeddings with Large Language Models\"]", "f1 socre": 0.3225806451612903}
{"id": "30a2535f-7e73-46ad-a402-ee80ac6e4f92", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"Quantum Annealers Chain Strengths: A Simple Heuristic to Set Them All\" by Valentin Gilbert and St\u00e9phane Louise, 2024, arXiv: 2404.05443 [39]],\n[\"Benchmarking Quantum Annealers with Near-Optimal Minor Embeddings\" by Valentin Gilbert"], "TTFT": 16.3840811252594, "answer": "[\"Scaling Advantage in Approximate Optimization with Quantum Annealing\", \"4-clique Network Minor Embedding for Quantum Annealers\", \"Quantum Annealers Chain Strengths: A Simple Heuristic to Set Them All\", \"Benchmarking Quantum Annealers with Near-Optimal Minor-Embedded Instances\"]", "f1 socre": 0.40579710144927533}
{"id": "68e88dcc-6dec-4bb4-b51b-1fe8d6cd856d", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"LGDE: Local Graph-based Dictionary Expansion\" by Schindler et al., 2023]\n[\"Query2doc: Query Expansion with Large Language Models\" by Wang, Yang, and Wei, 2023]\n[\"From free text to clusters: A graph-based approach to text mining and information retrieval\" by Raghav"], "TTFT": 10.241957187652588, "answer": "[\"Query2doc: Query Expansion with Large Language Models\", \"Query Expansion by Prompting Large Language Models\", \"LGDE: Local Graph-based Dictionary Expansion\"]", "f1 socre": 0.32989690721649484}
{"id": "d7adf528-91a9-4978-a25c-29e61562a3be", "pred": ["{{\"Reference\": [\"The Pitfalls of Defining Hallucination\", \"Hallucination is Inevitable: An Innate Limitation of Large Language Models\"],\n\"Citation\": [\"Hallucination is Inevitable: An Innate Limitation of Large Language Models\", \"The Pitfalls of Defining Hallucination\"]}}\n\nIn this paper, the authors show that hallucination is inevitable for large language models (LLMs"], "TTFT": 11.103811502456665, "answer": "{\"Reference\": [\"The Pitfalls of Defining Hallucination\"], \"Citation\": [\"An ASSESSMENT OF MODEL-ON-MODEL DECEPTION\"]}", "f1 socre": 0.29333333333333333}
{"id": "949f3450-9b30-4556-be5f-ca8afe9e072f", "pred": ["{\n\"Title\": \"LLamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource Language\",\n\"Authors\": [\"Cagri Toraman\"],\n\"Reference\": [\n{\"Title\": \"BELLE: A Large-Scale Chinese Instruction Tuning Dataset for Pretrained Language Models\"},\n{\"Title\": \"Bactrain-X: A Multilingual Instruction Tuning Dataset for Low-Resource L"], "TTFT": 12.38475251197815, "answer": "{\"Reference\": [], \"Citation\": [\"LlamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource Language\", \"InstructionCP: A fast approach to transfer Large Language Models into target language\"]}", "f1 socre": 0.43010752688172044}
{"id": "8878fb61-6996-4984-9774-5ce9fb506d37", "pred": ["{\n\"Title\": \"Extreme Compression of Large Language Models via Additive Quantization\",\n\"Authors\": [\"Vage Egiazarian\", \"* 12\", \"Andrei Panferov\", \"* 12\", \"Denis Kuznedelev\", \"* 23\", \"Elias Frantar\", \"* 4\", \"Artem Babenko\", \"* 2\"],\n\"Reference\": [\n{\"Title\": \"Mistral"], "TTFT": 11.69446086883545, "answer": "{\"Reference\": [\"Mistral 7B\"], \"Citation\": [\"Sparse Expansion and Neuronal Disentanglement\"]}", "f1 socre": 0.10526315789473684}
{"id": "17539923-dd06-4712-becd-68fb8d68614d", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"Context-Aware Learning to Rank with Self-Attention\" by Przemys\u0142aw Pobrotyn, Tomasz Bartczak, Miko\u0142aj Synowiec, Rados\u0142aw Bia\u0142obrzeski, and Jaros\u0142aw Bojar]\n\ncites\n\n[\"Learning a ranking model with delta features to study the effect of neighborhood on item preference in e"], "TTFT": 6.013138055801392, "answer": "[\"Influence of Neighborhood on the Preference of an Item in eCommerce Search\", \"Context-Aware Learning to Rank with Self-Attention\"]", "f1 socre": 0.31249999999999994}
{"id": "ad6d0a53-1f8a-46b5-9738-a34d0a3cafaf", "pred": ["Based on the provided instructions and the given papers, the longest citation chain is as follows:\n\n[\"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis\" by Mena, C. Gary et al. (2019), \"Churn prediction using deep convolutional neural networks and autoencoders\" by Wangperawong, A. et al. (2016)]\n\nExplanation:\n\nThe"], "TTFT": 3.672146797180176, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\"]", "f1 socre": 0.4000000000000001}
{"id": "b5b963fe-30ab-435f-9574-dcdfd96f3707", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Minimod: A Finite Difference solver for Seismic Modeling\" by Jie Meng, Andreas Atle, and Henri Calandra, 2020]\n[\"Massively scalable stencil algorithm for seismic modeling on the Cerebras WSE-2\" by Mathias Jacquelin, Mauricio Araya-Polo,"], "TTFT": 8.336519241333008, "answer": "[\"Minimod: A Finite Difference solver for Seismic Modeling\", \"Massively scalable stencil algorithm\"]", "f1 socre": 0.3368421052631579}
{"id": "7b104919-2fba-44d2-91dd-a56bd2f30e91", "pred": ["{{\"Reference\": [\"LLM A*: Human in the Loop Large Language Models Enabled A* Search for Robotics\"], \"Citation\": [\"FunSearch\", \"Various papers on search-based MAPF methods (e.g., CBS, ECBS, EECBS)\"]}}\n\nThe paper \"Evolution of Heuristics: Towards Efficient Automatic Algorithm Design Using Large Language Model\" references the paper \"LLM A*: Human in"], "TTFT": 12.563227891921997, "answer": "{\"Reference\": [\"LLM A*: Human in the Loop Large Language Models Enabled A* Search for Robotics\"], \"Citation\": [\"Ensembling Prioritized Hybrid Policies for Multi-agent Pathfinding\"]}", "f1 socre": 0.5}
{"id": "31ee8539-2e9e-40e2-adc3-082c872698f7", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\" by Niimi et al., 2024]\n\ncites\n\n[\"A PRE"], "TTFT": 11.793168783187866, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\", \"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\", \"An Efficient Multimodal Learning Framework to Comprehend Consumer Preferences Using BERT and Cross-Attention\"]", "f1 socre": 0.39682539682539686}
{"id": "9b982095-daad-4632-9573-b0d7037dd6a7", "pred": ["{{\"Reference\": [\"A Prompt for Investigating the Effectiveness of LLMs in Self-Critiquing for Planning: An Empirical Study\", \"Large Language Models Cannot Self-Correct Reasoning Intrinsically\"],\n\"Citation\": [\"LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks\"]}}"], "TTFT": 10.443064212799072, "answer": "{\"Reference\": [\"LARGE LANGUAGE MoDELS CANNOT SELF-CorRECT REASONING YET\", \"Can Large Language Models Really Improve by Self-critiquing Their Own Plans?\"], \"Citation\": []}", "f1 socre": 0.3943661971830986}
{"id": "ecded47a-3741-4458-ba61-c04271c33651", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"MonaCoBERT: Monotonic attention based ConvBERT for Knowledge Tracing\" by Unggi Lee, Yonghyun Park, Yujin Kim, Seongyune Choi, and Hyeoncheol Kim, 2021]\n\nThis paper cites:\n\n[\"Understanding the Instability of Transformer-based Mod"], "TTFT": 10.399053812026978, "answer": "[\"Very Deep Transformers for Neural Machine Translation\", \"Understanding the Difficulty of Training Transformers\", \"MonaCoBERT: Monotonic attention based ConvBERT for Knowledge Tracing\"]", "f1 socre": 0.29411764705882354}
{"id": "71d0fb39-d315-4e33-beec-c53271663f7f", "pred": ["{{\"Reference\": [\"LoRA-Switch: Boosting the Efficiency of Dynamic LLM Adapters via System-Algorithm Co-design\"], \"Citation\": []}}"], "TTFT": 13.256085872650146, "answer": "{\"Reference\": [\"ONLINE SPECULATIVE DECODING\"], \"Citation\": [\"LoRA-Switch: Boosting the Efficiency of Dynamic LLM Adapters via System-Algorithm Co-design\"]}", "f1 socre": 0.9019607843137255}
{"id": "5d482043-88d2-4246-8191-e603b7bf6ed4", "pred": ["{\n\"Title\": \"Scalable Pre-training of Large Autoregressive Image Models\",\n\"Authors\": [\"Alaaeldin El-Nouby\", \"Ali Farhat\", \"Mohamed Ragab\", \"Mostafa Dehghani\", \"Georg Heigold\", \"Chen Sun\", \"Mario Lu\u010di\u0107\", \"Cordelia Schmid\"],\n\"Reference\": [\n{\"Title\": \"Vivit"], "TTFT": 15.165497541427612, "answer": "{\"Reference\": [\"Data Filtering Networks\"], \"Citation\": [\"ARVideo: Autoregressive Pretraining for Self-Supervised Video Representation Learning\"]}", "f1 socre": 0.17073170731707318}
{"id": "014ae33b-22e7-4fb7-ae34-56f64160ce74", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n1. [\"DYNAP-SE2: a scalable multi-core dynamic neuromorphic asynchronous spiking neural network processor\" by Ole Richter, et al.]\n2. [\"Speck: A Smart event-based Vision Sensor with a low latency 327K Neuron Convolutional Neural Network Processing Pipeline\" by Ole Richter,"], "TTFT": 15.628410577774048, "answer": "[\"Speck: A Smart event-based Vision Sensor with a low latency 327K Neuron Convolutional Neuronal Network Processing Pipeline\", \"DYNAP-SE2: a scalable multi-core dynamic neuromorphic asynchronous spiking neural network processor\"]", "f1 socre": 0.7543859649122807}
{"id": "3e4ee654-dfd8-4ffd-a825-79e0997961c9", "pred": ["{{\"Reference\": [\"Navigating the Safety Landscape: Measuring LLM Cybersecurity Risks with VISAGE Metric\", \"Purple Llama CYBERSECEval: A Secure Coding Benchmark for Large Language Models\"],\n\"Citation\": [\"Navigating the Safety Landscape: Measuring LLM Cybersecurity Risks with VISAGE Metric\": \"studied the safety landscape of LLMs and discovered"], "TTFT": 12.062817811965942, "answer": "{\"Reference\": [\"Purple Llama CYBERSECEvAL: A Secure Coding Benchmark for Language Models\"], \"Citation\": [\"Navigating the Safety Landscape: Measuring Risks in Finetuning Large Language Models\"]}", "f1 socre": 0.5238095238095238}
{"id": "f359acf5-39a1-4490-ba4a-95d38de35fa7", "pred": ["{\n\"Title\": \"Soaring from 4K to 400K: Extending LLM's Context with Activation Beacon\",\n\"Authors\": [\"Peitian Zhang\", \"Zheng Liu\", \"Shitao Xiao\", \"Ninglu Shao\", \"Hongjin Qian\", \"Qiwei Ye\", \"Zhicheng Dou\"],\n\"Affiliations\": [\"Beijing Academy of Artificial Intelligence\","], "TTFT": 10.695560455322266, "answer": "{\"Reference\": [\"PoSE: EFFICIENT CONTEXT WINDOW EXTENSION OF LLMS VIA POSITIONAL SKIP-WISE TRAINING\"], \"Citation\": [\"Extending Llama-3's Context Ten-Fold Overnight\"]}", "f1 socre": 0.10752688172043011}
{"id": "2357f6df-fed9-4508-a80c-800fe6995caa", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"GTBENCH: Uncovering Game-Theoretic Characteristics of LLMs\" by Jinhao Duan, Renming Zhang, James Diffenderfer, Bhavya Kailkhura, Lichao Sun, Elias Stengel-Eskin, Mohit Bansal, Tianlong Chen, and Kaidi Xu]\n\n[\"Exploring"], "TTFT": 10.72509765625, "answer": "[\"![](https://cdn.mathpix.com/cropped/2024_06_04_791d7ec1c70f2f7bcd44g-01.jpg?height=135&width=174&top_left_y=320&top_left_x=260)\", \"Generative AI for Game Theory-based Mobile Networking\", \"Exploring Equilibrium Strategies in Network Games with Generative AI\"]", "f1 socre": 0.025477707006369425}
{"id": "aa9707c4-9506-4e51-899d-9213ad10a324", "pred": ["Based on the provided instructions, the longest citation chain from the given papers is as follows:\n\n[\"PromptMind Team at EHRSQL-2024: Improving Reliability of SQL Generation using Ensemble LLMs\", \"Angel-Optimized Text Embeddings\", \"Dolan and Brockett (2005): Building a question answering test collection\"]\n\nExplanation:\n\n1. The first paper, \"Prompt"], "TTFT": 11.649495124816895, "answer": "[\"Scaling Sentence Embeddings with Large Language Models\", \"ANGLE-OPTIMIZED TEXT EMBEDDINGS\", \"PromptMind Team at EHRSQL-2024: Improving Reliability of SQL Generation using Ensemble LLMs\"]", "f1 socre": 0.5617977528089888}
{"id": "edc6d95f-02ba-4835-bff6-853cb7d61f6b", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"The unreasonable effectiveness of optimal transport in economics\" by Galichon, 2021, \"Optimal transport in a nutshell\" by Galichon, 2021, \"Exploring the connection between optimal transport and economic applications\" by Galichon, 2016, \"Optimal Transport for Applied Mathematicians\" by Sant"], "TTFT": 13.675036668777466, "answer": "[\"Water is a radiation protection agent for ionised pyrrole\", \"The unreasonable effectiveness of optimal transport in economics\"]", "f1 socre": 0.23157894736842105}
{"id": "a780dc25-81d8-4e0f-b344-5086b8055e04", "pred": ["Based on the provided instructions, the longest citation chain can be constructed as follows:\n\n[\"Beyond Traditional Magnetic Resonance Processing with Artificial Intelligence\" by Amir Jahangiri and Vladislav Orekhov, 2024],\n[\"Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems\" by Tsung-Hsien Wen, Milica Ga\u0161i\u0107, Nikola Mrk\u0161"], "TTFT": 11.49069619178772, "answer": "[\"Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems\", \"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\", \"Deep Learning using Rectified Linear Units (ReLU)\", \"BEYOND TRADITIONAL MAGNETIC RESONANCE PROCESSING WITH ARTIFICIAL INTELLIGENCE *\"]", "f1 socre": 0.35294117647058826}
{"id": "1df3da1b-87c8-4692-9ec7-83d1b0e9caa3", "pred": ["{\n\"Title\": \"Break the Sequential Dependency of LLM Inference Using LOOKAHEAD DECODING\",\n\"Authors\": [\"Yichao Fu\", \"Peter Bailis\", \"Ion Stoica\", \"Hao Zhang\"],\n\"Reference\": [\n{\"Title\": \"Attention is all you need\"},\n{\"Title\": \"Gqa: Training generalized multi-query transformer models from multi-head checkpoints\"},\n"], "TTFT": 9.91625428199768, "answer": "{\"Reference\": [\"Mistral 7B\"], \"Citation\": [\"Kangaroo: Lossless Self-Speculative Decoding via Double Early Exiting\"]}", "f1 socre": 0.04938271604938271}
{"id": "f7116f92-a07c-4278-9333-b63ed5965907", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Beyond Traditional Magnetic Resonance Processing with Artificial Intelligence\" by Amir Jahangiri and Vladislav Orekhov, 2024],\n[\"Deep Learning using Rectified Linear Units (ReLU)\" by Abien Fred M. Agarap, 2018],\n[\"A Neural Network Architecture Combining"], "TTFT": 11.509602546691895, "answer": "[\"Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems\", \"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\", \"Deep Learning using Rectified Linear Units (ReLU)\", \"BEYOND TRADITIONAL MAGNETIC RESONANCE PROCESSING WITH ARTIFICIAL INTELLIGENCE *\"]", "f1 socre": 0.3090909090909091}
{"id": "13a4a371-6339-4c9d-82cf-fc9ab2bb017d", "pred": ["{\n\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"],\n\"Citation\": []\n}\n\nThe paper \"OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models\" does not mention \"TinyLlama: An Open-Source Small Language Model\" in its text, so there is no citation relationship between the two papers. However, \"OpenMoE\" references \"Tiny"], "TTFT": 13.604034900665283, "answer": "{\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"], \"Citation\": [\"Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training\"]}", "f1 socre": 0.4047619047619047}
{"id": "c4a4e642-fa00-4377-a756-40f2f7ea7a74", "pred": ["{{\"Reference\": [\"A Survey of Hallucination in Large Foundation Models by Vipula Rawte, Amit Sheth, and Amitava Das\"], \"Citation\": [\"CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation by Quan Tu, Shilong Fan, Zihang Tian, Rui Yan\"]}}\n\nThis paper on CharacterEval references the survey paper on hallucination in large foundation models and"], "TTFT": 11.539960622787476, "answer": "{\"Reference\": [\"A Survey of Hallucination in \\\"Large\\\" Foundation Models\"], \"Citation\": [\"From Role-Play to Drama-Interaction: An LLM Solution\"]}", "f1 socre": 0.2745098039215686}
{"id": "835a4005-8c32-4083-b52d-4e014d1d35f5", "pred": ["Based on the provided instructions and papers, here is the longest citation chain:\n\n[\"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\" by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed, \"SUPERB: Speech processing Universal"], "TTFT": 11.292267322540283, "answer": "[\"CONTRASTIVE SEMI-SUPERVISED LEARNING FOR ASR\", \"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\", \"SUPERB: Speech processing Universal PERformance Benchmark\"]", "f1 socre": 0.38461538461538464}
{"id": "cd618a43-7176-432d-b21e-432e5c2ca4a7", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\" by Abien Fred M. Agarap, 2018, ICMLC 2018]\n\n[\"A Novel Approach Combining Recurrent Neural Network and"], "TTFT": 6.193939208984375, "answer": "[\"Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems\", \"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\"]", "f1 socre": 0.34374999999999994}
{"id": "e904afd3-5cb9-4bf7-bbee-9c8defac858c", "pred": ["Based on the provided instructions and papers, the longest citation chain can be constructed as follows:\n\n[\"On the performativity of SDG classifications in large bibliometric databases\" by Ottaviani and Stahlschmidt (2024), \"Language models are unsupervised multitask learners\" by Radford et al. (2019), \"DistilBERT: Distilling BERT: Six-Order Distillation for a Fast and Che"], "TTFT": 7.292623519897461, "answer": "[\"Contrastive Decoding: Open-ended Text Generation as Optimization\", \"An Empirical Study On Contrastive Search And Contrastive Decoding For Open-ended Text Generation\", \"On the performativity of SDG classifications in large bibliometric databases\"]", "f1 socre": 0.380952380952381}
{"id": "683f4cfd-1cfe-475d-927a-08a71d4f69e9", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\" by Niimi et al., 2024]\n\n[\"Churn Prediction with"], "TTFT": 11.674830198287964, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\", \"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\", \"An Efficient Multimodal Learning Framework to Comprehend Consumer Preferences Using BERT and Cross-Attention\"]", "f1 socre": 0.45312500000000006}
{"id": "0223faf0-f46f-459d-b1cd-45c4dab78fc2", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"A tutorial on the range variant of asymmetric numeral systems\" by James Townsend, \"Asymmetric numeral systems\" by Jarek Duda, \"Asymmetric numeral systems: entropy coding combining speed of Huffman coding with compression rate of arithmetic coding\" by Jarek Duda, Khalid Tahboub, Neeraj J. Gad"], "TTFT": 14.098109483718872, "answer": "[\"Lightweight compression with encryption based on Asymmetric Numeral Systems\", \"A tutorial on the range variant of asymmetric numeral systems\", \"ATC: an Advanced Tucker Compression library for multidimensional data\"]", "f1 socre": 0.3652173913043478}
{"id": "b7405d8e-13f0-4750-a25d-df3a67ffd39a", "pred": ["{{\"Reference\": [\"Benchmarking Large Language Models in Retrieval-Augmented Generation\", \"Retrieval-Augmented Generation in Question Answering: A Survey\"],\n\"Citation\": [\"Seven Failure Points When Engineering a Retrieval Augmented Generation System\"]}}\n\nThis paper presents the lessons learned and failure points from three case studies of implementing Retrieval-Augmented Generation (RAG) systems. It also discusses"], "TTFT": 10.79617714881897, "answer": "{\"Reference\": [\"Benchmarking Large Language Models in Retrieval-Augmented Generation\"], \"Citation\": [\"When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively\"]}", "f1 socre": 0.4186046511627907}
{"id": "150529cd-8767-477e-aa2b-28bae4970fdd", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System\" by Santos et al., \"Massively scalable stencil algorithm for the 3D wave equation on the Cerebras Wafer-Scale Engine\" by Jacquelin and Araya-Polo]\n\nThe reason for this citation chain is that the paper by Sant"], "TTFT": 15.363740921020508, "answer": "[\"Massively scalable stencil algorithm\", \"Disruptive Changes in Field Equation Modeling A Simple Interface for Wafer Scale Engines\", \"Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System\"]", "f1 socre": 0.39285714285714285}
{"id": "a8fa7aa1-2198-419b-a0de-6ba4d1c1f160", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Dance Any Beat: Blending Beats with Visuals in Dance Video Generation\" by Xuanchen Wang, Heng Wang, Weidong Cai, and Dongnan Liu],\n[\"InMoDeGAN: Interpretable Motion Decomposing Generative Adversarial Network for Video Generation\" by Yaohui Wang, Francois Bremond,"], "TTFT": 14.07155442237854, "answer": "[\"InMoDeGAN: Interpretable Motion Decomposition Generative Adversarial Network for Video Generation\", \"LATENT IMAGE ANIMATOR: <br> LEARNING TO ANIMATE IMAGES VIA LATENT SPACE NAVIGATION\", \"Dance Any Beat: Blending Beats with Visuals in Dance Video Generation\"]", "f1 socre": 0.43548387096774194}
{"id": "49680a79-a974-49b0-b7e3-85ff515bc4f2", "pred": ["{{\"Reference\": [\"Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding\"], \"Citation\": [\"Smurfs: Leveraging Multiple Proficiency Agents for Effective Tool Utilization in LLMs\" ]}}\n\nThe paper \"EasyTool: Enhancing LLM-based Agents with Concise Tool Instruction\" references the paper \"Don't Fine-Tune, Dec"], "TTFT": 10.98206639289856, "answer": "{\"Reference\": [\"Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding\"], \"Citation\": [\"Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning\"]}", "f1 socre": 0.6521739130434783}
{"id": "9f29f552-c817-4c2a-8910-b5e8ae7c014f", "pred": ["[\"Fine-Tuning LLaMA for Multi-Stage Text Retrieval\" by Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin, 2023, cited by \"Improving Text Embeddings with Large Language Models\" by Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, Furu Wei, and Jimmy Lin,"], "TTFT": 13.682081937789917, "answer": "[\"RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large Language Models\", \"Fine-Tuning LLaMA for Multi-Stage Text Retrieval\", \"Improving Text Embeddings with Large Language Models\"]", "f1 socre": 0.3225806451612903}
{"id": "30a2535f-7e73-46ad-a402-ee80ac6e4f92", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"Quantum Annealers Chain Strengths: A Simple Heuristic to Set Them All\" by Valentin Gilbert and St\u00e9phane Louise, 2024, arXiv: 2404.05443 [39]],\n[\"Benchmarking Quantum Annealers with Near-Optimal Minor Embeddings\" by Valentin Gilbert"], "TTFT": 16.3840811252594, "answer": "[\"Scaling Advantage in Approximate Optimization with Quantum Annealing\", \"4-clique Network Minor Embedding for Quantum Annealers\", \"Quantum Annealers Chain Strengths: A Simple Heuristic to Set Them All\", \"Benchmarking Quantum Annealers with Near-Optimal Minor-Embedded Instances\"]", "f1 socre": 0.40579710144927533}
{"id": "68e88dcc-6dec-4bb4-b51b-1fe8d6cd856d", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"LGDE: Local Graph-based Dictionary Expansion\" by Schindler et al., 2023]\n[\"Query2doc: Query Expansion with Large Language Models\" by Wang, Yang, and Wei, 2023]\n[\"From free text to clusters: A graph-based approach to text mining and information retrieval\" by Raghav"], "TTFT": 10.241957187652588, "answer": "[\"Query2doc: Query Expansion with Large Language Models\", \"Query Expansion by Prompting Large Language Models\", \"LGDE: Local Graph-based Dictionary Expansion\"]", "f1 socre": 0.32989690721649484}
{"id": "d7adf528-91a9-4978-a25c-29e61562a3be", "pred": ["{{\"Reference\": [\"The Pitfalls of Defining Hallucination\", \"Hallucination is Inevitable: An Innate Limitation of Large Language Models\"],\n\"Citation\": [\"Hallucination is Inevitable: An Innate Limitation of Large Language Models\", \"The Pitfalls of Defining Hallucination\"]}}\n\nIn this paper, the authors show that hallucination is inevitable for large language models (LLMs"], "TTFT": 11.103811502456665, "answer": "{\"Reference\": [\"The Pitfalls of Defining Hallucination\"], \"Citation\": [\"An ASSESSMENT OF MODEL-ON-MODEL DECEPTION\"]}", "f1 socre": 0.29333333333333333}
{"id": "949f3450-9b30-4556-be5f-ca8afe9e072f", "pred": ["{\n\"Title\": \"LLamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource Language\",\n\"Authors\": [\"Cagri Toraman\"],\n\"Reference\": [\n{\"Title\": \"BELLE: A Large-Scale Chinese Instruction Tuning Dataset for Pretrained Language Models\"},\n{\"Title\": \"Bactrain-X: A Multilingual Instruction Tuning Dataset for Low-Resource L"], "TTFT": 12.38475251197815, "answer": "{\"Reference\": [], \"Citation\": [\"LlamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource Language\", \"InstructionCP: A fast approach to transfer Large Language Models into target language\"]}", "f1 socre": 0.43010752688172044}
{"id": "8878fb61-6996-4984-9774-5ce9fb506d37", "pred": ["{\n\"Title\": \"Extreme Compression of Large Language Models via Additive Quantization\",\n\"Authors\": [\"Vage Egiazarian\", \"* 12\", \"Andrei Panferov\", \"* 12\", \"Denis Kuznedelev\", \"* 23\", \"Elias Frantar\", \"* 4\", \"Artem Babenko\", \"* 2\"],\n\"Reference\": [\n{\"Title\": \"Mistral"], "TTFT": 11.69446086883545, "answer": "{\"Reference\": [\"Mistral 7B\"], \"Citation\": [\"Sparse Expansion and Neuronal Disentanglement\"]}", "f1 socre": 0.10526315789473684}
{"id": "17539923-dd06-4712-becd-68fb8d68614d", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"Context-Aware Learning to Rank with Self-Attention\" by Przemys\u0142aw Pobrotyn, Tomasz Bartczak, Miko\u0142aj Synowiec, Rados\u0142aw Bia\u0142obrzeski, and Jaros\u0142aw Bojar]\n\ncites\n\n[\"Learning a ranking model with delta features to study the effect of neighborhood on item preference in e"], "TTFT": 6.013138055801392, "answer": "[\"Influence of Neighborhood on the Preference of an Item in eCommerce Search\", \"Context-Aware Learning to Rank with Self-Attention\"]", "f1 socre": 0.31249999999999994}
{"id": "ad6d0a53-1f8a-46b5-9738-a34d0a3cafaf", "pred": ["Based on the provided instructions and the given papers, the longest citation chain is as follows:\n\n[\"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis\" by Mena, C. Gary et al. (2019), \"Churn prediction using deep convolutional neural networks and autoencoders\" by Wangperawong, A. et al. (2016)]\n\nExplanation:\n\nThe"], "TTFT": 3.672146797180176, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\"]", "f1 socre": 0.4000000000000001}
{"id": "b5b963fe-30ab-435f-9574-dcdfd96f3707", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Minimod: A Finite Difference solver for Seismic Modeling\" by Jie Meng, Andreas Atle, and Henri Calandra, 2020]\n[\"Massively scalable stencil algorithm for seismic modeling on the Cerebras WSE-2\" by Mathias Jacquelin, Mauricio Araya-Polo,"], "TTFT": 8.336519241333008, "answer": "[\"Minimod: A Finite Difference solver for Seismic Modeling\", \"Massively scalable stencil algorithm\"]", "f1 socre": 0.3368421052631579}
{"id": "7b104919-2fba-44d2-91dd-a56bd2f30e91", "pred": ["{{\"Reference\": [\"LLM A*: Human in the Loop Large Language Models Enabled A* Search for Robotics\"], \"Citation\": [\"FunSearch\", \"Various papers on search-based MAPF methods (e.g., CBS, ECBS, EECBS)\"]}}\n\nThe paper \"Evolution of Heuristics: Towards Efficient Automatic Algorithm Design Using Large Language Model\" references the paper \"LLM A*: Human in"], "TTFT": 12.563227891921997, "answer": "{\"Reference\": [\"LLM A*: Human in the Loop Large Language Models Enabled A* Search for Robotics\"], \"Citation\": [\"Ensembling Prioritized Hybrid Policies for Multi-agent Pathfinding\"]}", "f1 socre": 0.5}
{"id": "31ee8539-2e9e-40e2-adc3-082c872698f7", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\" by Niimi et al., 2024]\n\ncites\n\n[\"A PRE"], "TTFT": 11.793168783187866, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\", \"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\", \"An Efficient Multimodal Learning Framework to Comprehend Consumer Preferences Using BERT and Cross-Attention\"]", "f1 socre": 0.39682539682539686}
{"id": "9b982095-daad-4632-9573-b0d7037dd6a7", "pred": ["{{\"Reference\": [\"A Prompt for Investigating the Effectiveness of LLMs in Self-Critiquing for Planning: An Empirical Study\", \"Large Language Models Cannot Self-Correct Reasoning Intrinsically\"],\n\"Citation\": [\"LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks\"]}}"], "TTFT": 10.443064212799072, "answer": "{\"Reference\": [\"LARGE LANGUAGE MoDELS CANNOT SELF-CorRECT REASONING YET\", \"Can Large Language Models Really Improve by Self-critiquing Their Own Plans?\"], \"Citation\": []}", "f1 socre": 0.3943661971830986}
{"id": "ecded47a-3741-4458-ba61-c04271c33651", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"MonaCoBERT: Monotonic attention based ConvBERT for Knowledge Tracing\" by Unggi Lee, Yonghyun Park, Yujin Kim, Seongyune Choi, and Hyeoncheol Kim, 2021]\n\nThis paper cites:\n\n[\"Understanding the Instability of Transformer-based Mod"], "TTFT": 10.399053812026978, "answer": "[\"Very Deep Transformers for Neural Machine Translation\", \"Understanding the Difficulty of Training Transformers\", \"MonaCoBERT: Monotonic attention based ConvBERT for Knowledge Tracing\"]", "f1 socre": 0.29411764705882354}
{"id": "71d0fb39-d315-4e33-beec-c53271663f7f", "pred": ["{{\"Reference\": [\"LoRA-Switch: Boosting the Efficiency of Dynamic LLM Adapters via System-Algorithm Co-design\"], \"Citation\": []}}"], "TTFT": 13.256085872650146, "answer": "{\"Reference\": [\"ONLINE SPECULATIVE DECODING\"], \"Citation\": [\"LoRA-Switch: Boosting the Efficiency of Dynamic LLM Adapters via System-Algorithm Co-design\"]}", "f1 socre": 0.9019607843137255}
{"id": "5d482043-88d2-4246-8191-e603b7bf6ed4", "pred": ["{\n\"Title\": \"Scalable Pre-training of Large Autoregressive Image Models\",\n\"Authors\": [\"Alaaeldin El-Nouby\", \"Ali Farhat\", \"Mohamed Ragab\", \"Mostafa Dehghani\", \"Georg Heigold\", \"Chen Sun\", \"Mario Lu\u010di\u0107\", \"Cordelia Schmid\"],\n\"Reference\": [\n{\"Title\": \"Vivit"], "TTFT": 15.165497541427612, "answer": "{\"Reference\": [\"Data Filtering Networks\"], \"Citation\": [\"ARVideo: Autoregressive Pretraining for Self-Supervised Video Representation Learning\"]}", "f1 socre": 0.17073170731707318}
{"id": "014ae33b-22e7-4fb7-ae34-56f64160ce74", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n1. [\"DYNAP-SE2: a scalable multi-core dynamic neuromorphic asynchronous spiking neural network processor\" by Ole Richter, et al.]\n2. [\"Speck: A Smart event-based Vision Sensor with a low latency 327K Neuron Convolutional Neural Network Processing Pipeline\" by Ole Richter,"], "TTFT": 15.628410577774048, "answer": "[\"Speck: A Smart event-based Vision Sensor with a low latency 327K Neuron Convolutional Neuronal Network Processing Pipeline\", \"DYNAP-SE2: a scalable multi-core dynamic neuromorphic asynchronous spiking neural network processor\"]", "f1 socre": 0.7543859649122807}
{"id": "3e4ee654-dfd8-4ffd-a825-79e0997961c9", "pred": ["{{\"Reference\": [\"Navigating the Safety Landscape: Measuring LLM Cybersecurity Risks with VISAGE Metric\", \"Purple Llama CYBERSECEval: A Secure Coding Benchmark for Large Language Models\"],\n\"Citation\": [\"Navigating the Safety Landscape: Measuring LLM Cybersecurity Risks with VISAGE Metric\": \"studied the safety landscape of LLMs and discovered"], "TTFT": 12.062817811965942, "answer": "{\"Reference\": [\"Purple Llama CYBERSECEvAL: A Secure Coding Benchmark for Language Models\"], \"Citation\": [\"Navigating the Safety Landscape: Measuring Risks in Finetuning Large Language Models\"]}", "f1 socre": 0.5238095238095238}
{"id": "f359acf5-39a1-4490-ba4a-95d38de35fa7", "pred": ["{\n\"Title\": \"Soaring from 4K to 400K: Extending LLM's Context with Activation Beacon\",\n\"Authors\": [\"Peitian Zhang\", \"Zheng Liu\", \"Shitao Xiao\", \"Ninglu Shao\", \"Hongjin Qian\", \"Qiwei Ye\", \"Zhicheng Dou\"],\n\"Affiliations\": [\"Beijing Academy of Artificial Intelligence\","], "TTFT": 10.695560455322266, "answer": "{\"Reference\": [\"PoSE: EFFICIENT CONTEXT WINDOW EXTENSION OF LLMS VIA POSITIONAL SKIP-WISE TRAINING\"], \"Citation\": [\"Extending Llama-3's Context Ten-Fold Overnight\"]}", "f1 socre": 0.10752688172043011}
{"id": "2357f6df-fed9-4508-a80c-800fe6995caa", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"GTBENCH: Uncovering Game-Theoretic Characteristics of LLMs\" by Jinhao Duan, Renming Zhang, James Diffenderfer, Bhavya Kailkhura, Lichao Sun, Elias Stengel-Eskin, Mohit Bansal, Tianlong Chen, and Kaidi Xu]\n\n[\"Exploring"], "TTFT": 10.72509765625, "answer": "[\"![](https://cdn.mathpix.com/cropped/2024_06_04_791d7ec1c70f2f7bcd44g-01.jpg?height=135&width=174&top_left_y=320&top_left_x=260)\", \"Generative AI for Game Theory-based Mobile Networking\", \"Exploring Equilibrium Strategies in Network Games with Generative AI\"]", "f1 socre": 0.025477707006369425}
{"id": "aa9707c4-9506-4e51-899d-9213ad10a324", "pred": ["Based on the provided instructions, the longest citation chain from the given papers is as follows:\n\n[\"PromptMind Team at EHRSQL-2024: Improving Reliability of SQL Generation using Ensemble LLMs\", \"Angel-Optimized Text Embeddings\", \"Dolan and Brockett (2005): Building a question answering test collection\"]\n\nExplanation:\n\n1. The first paper, \"Prompt"], "TTFT": 11.649495124816895, "answer": "[\"Scaling Sentence Embeddings with Large Language Models\", \"ANGLE-OPTIMIZED TEXT EMBEDDINGS\", \"PromptMind Team at EHRSQL-2024: Improving Reliability of SQL Generation using Ensemble LLMs\"]", "f1 socre": 0.5617977528089888}
{"id": "edc6d95f-02ba-4835-bff6-853cb7d61f6b", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"The unreasonable effectiveness of optimal transport in economics\" by Galichon, 2021, \"Optimal transport in a nutshell\" by Galichon, 2021, \"Exploring the connection between optimal transport and economic applications\" by Galichon, 2016, \"Optimal Transport for Applied Mathematicians\" by Sant"], "TTFT": 13.675036668777466, "answer": "[\"Water is a radiation protection agent for ionised pyrrole\", \"The unreasonable effectiveness of optimal transport in economics\"]", "f1 socre": 0.23157894736842105}
{"id": "a780dc25-81d8-4e0f-b344-5086b8055e04", "pred": ["Based on the provided instructions, the longest citation chain can be constructed as follows:\n\n[\"Beyond Traditional Magnetic Resonance Processing with Artificial Intelligence\" by Amir Jahangiri and Vladislav Orekhov, 2024],\n[\"Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems\" by Tsung-Hsien Wen, Milica Ga\u0161i\u0107, Nikola Mrk\u0161"], "TTFT": 11.49069619178772, "answer": "[\"Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems\", \"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\", \"Deep Learning using Rectified Linear Units (ReLU)\", \"BEYOND TRADITIONAL MAGNETIC RESONANCE PROCESSING WITH ARTIFICIAL INTELLIGENCE *\"]", "f1 socre": 0.35294117647058826}
{"id": "1df3da1b-87c8-4692-9ec7-83d1b0e9caa3", "pred": ["{\n\"Title\": \"Break the Sequential Dependency of LLM Inference Using LOOKAHEAD DECODING\",\n\"Authors\": [\"Yichao Fu\", \"Peter Bailis\", \"Ion Stoica\", \"Hao Zhang\"],\n\"Reference\": [\n{\"Title\": \"Attention is all you need\"},\n{\"Title\": \"Gqa: Training generalized multi-query transformer models from multi-head checkpoints\"},\n"], "TTFT": 9.91625428199768, "answer": "{\"Reference\": [\"Mistral 7B\"], \"Citation\": [\"Kangaroo: Lossless Self-Speculative Decoding via Double Early Exiting\"]}", "f1 socre": 0.04938271604938271}
{"id": "f7116f92-a07c-4278-9333-b63ed5965907", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Beyond Traditional Magnetic Resonance Processing with Artificial Intelligence\" by Amir Jahangiri and Vladislav Orekhov, 2024],\n[\"Deep Learning using Rectified Linear Units (ReLU)\" by Abien Fred M. Agarap, 2018],\n[\"A Neural Network Architecture Combining"], "TTFT": 11.509602546691895, "answer": "[\"Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems\", \"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\", \"Deep Learning using Rectified Linear Units (ReLU)\", \"BEYOND TRADITIONAL MAGNETIC RESONANCE PROCESSING WITH ARTIFICIAL INTELLIGENCE *\"]", "f1 socre": 0.3090909090909091}
{"id": "c8d80aee-3b85-434f-a348-2b40bb7d12dd", "pred": ["Based on the provided instructions and papers, here is the longest citation chain:\n\n[\"Context-Aware Learning to Rank with Self-Attention\" by Przemys\u0142aw Pobrotyn, Tomasz Bartczak, Miko\u0142aj Synowiec, Rados\u0142aw Bia\u0142obrzeski, and Jaros\u0142aw Bojar],\n[\"Learning to Rank with Self-Attention: A Survey\" by Himan Abdollahpouri, Robin"], "TTFT": 16.089614152908325, "answer": "[\"Context-Aware Learning to Rank with Self-Attention\", \"On the Impact of Outlier Bias on User Clicks\", \"Treatment Effect Estimation for User Interest Exploration on Recommender Systems\"]", "f1 socre": 0.1651376146788991}
{"id": "13a4a371-6339-4c9d-82cf-fc9ab2bb017d", "pred": ["{\n\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"],\n\"Citation\": []\n}\n\nThe paper \"OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models\" does not mention \"TinyLlama: An Open-Source Small Language Model\" in its text, so there is no citation relationship between the two papers. However, \"OpenMoE\" references \"Tiny"], "TTFT": 13.604034900665283, "answer": "{\"Reference\": [\"TinyLlama: An Open-Source Small Language Model\"], \"Citation\": [\"Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training\"]}", "f1 socre": 0.4047619047619047}
{"id": "c4a4e642-fa00-4377-a756-40f2f7ea7a74", "pred": ["{{\"Reference\": [\"A Survey of Hallucination in Large Foundation Models by Vipula Rawte, Amit Sheth, and Amitava Das\"], \"Citation\": [\"CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation by Quan Tu, Shilong Fan, Zihang Tian, Rui Yan\"]}}\n\nThis paper on CharacterEval references the survey paper on hallucination in large foundation models and"], "TTFT": 11.539960622787476, "answer": "{\"Reference\": [\"A Survey of Hallucination in \\\"Large\\\" Foundation Models\"], \"Citation\": [\"From Role-Play to Drama-Interaction: An LLM Solution\"]}", "f1 socre": 0.2745098039215686}
{"id": "835a4005-8c32-4083-b52d-4e014d1d35f5", "pred": ["Based on the provided instructions and papers, here is the longest citation chain:\n\n[\"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\" by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed, \"SUPERB: Speech processing Universal"], "TTFT": 11.292267322540283, "answer": "[\"CONTRASTIVE SEMI-SUPERVISED LEARNING FOR ASR\", \"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\", \"SUPERB: Speech processing Universal PERformance Benchmark\"]", "f1 socre": 0.38461538461538464}
{"id": "cd618a43-7176-432d-b21e-432e5c2ca4a7", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\" by Abien Fred M. Agarap, 2018, ICMLC 2018]\n\n[\"A Novel Approach Combining Recurrent Neural Network and"], "TTFT": 6.193939208984375, "answer": "[\"Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems\", \"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\"]", "f1 socre": 0.34374999999999994}
{"id": "e904afd3-5cb9-4bf7-bbee-9c8defac858c", "pred": ["Based on the provided instructions and papers, the longest citation chain can be constructed as follows:\n\n[\"On the performativity of SDG classifications in large bibliometric databases\" by Ottaviani and Stahlschmidt (2024), \"Language models are unsupervised multitask learners\" by Radford et al. (2019), \"DistilBERT: Distilling BERT: Six-Order Distillation for a Fast and Che"], "TTFT": 7.292623519897461, "answer": "[\"Contrastive Decoding: Open-ended Text Generation as Optimization\", \"An Empirical Study On Contrastive Search And Contrastive Decoding For Open-ended Text Generation\", \"On the performativity of SDG classifications in large bibliometric databases\"]", "f1 socre": 0.380952380952381}
{"id": "683f4cfd-1cfe-475d-927a-08a71d4f69e9", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\" by Niimi et al., 2024]\n\n[\"Churn Prediction with"], "TTFT": 11.674830198287964, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\", \"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\", \"An Efficient Multimodal Learning Framework to Comprehend Consumer Preferences Using BERT and Cross-Attention\"]", "f1 socre": 0.45312500000000006}
{"id": "0223faf0-f46f-459d-b1cd-45c4dab78fc2", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"A tutorial on the range variant of asymmetric numeral systems\" by James Townsend, \"Asymmetric numeral systems\" by Jarek Duda, \"Asymmetric numeral systems: entropy coding combining speed of Huffman coding with compression rate of arithmetic coding\" by Jarek Duda, Khalid Tahboub, Neeraj J. Gad"], "TTFT": 14.098109483718872, "answer": "[\"Lightweight compression with encryption based on Asymmetric Numeral Systems\", \"A tutorial on the range variant of asymmetric numeral systems\", \"ATC: an Advanced Tucker Compression library for multidimensional data\"]", "f1 socre": 0.3652173913043478}
{"id": "b7405d8e-13f0-4750-a25d-df3a67ffd39a", "pred": ["{{\"Reference\": [\"Benchmarking Large Language Models in Retrieval-Augmented Generation\", \"Retrieval-Augmented Generation in Question Answering: A Survey\"],\n\"Citation\": [\"Seven Failure Points When Engineering a Retrieval Augmented Generation System\"]}}\n\nThis paper presents the lessons learned and failure points from three case studies of implementing Retrieval-Augmented Generation (RAG) systems. It also discusses"], "TTFT": 10.79617714881897, "answer": "{\"Reference\": [\"Benchmarking Large Language Models in Retrieval-Augmented Generation\"], \"Citation\": [\"When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively\"]}", "f1 socre": 0.4186046511627907}
{"id": "150529cd-8767-477e-aa2b-28bae4970fdd", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System\" by Santos et al., \"Massively scalable stencil algorithm for the 3D wave equation on the Cerebras Wafer-Scale Engine\" by Jacquelin and Araya-Polo]\n\nThe reason for this citation chain is that the paper by Sant"], "TTFT": 15.363740921020508, "answer": "[\"Massively scalable stencil algorithm\", \"Disruptive Changes in Field Equation Modeling A Simple Interface for Wafer Scale Engines\", \"Breaking the Molecular Dynamics Timescale Barrier Using a Wafer-Scale System\"]", "f1 socre": 0.39285714285714285}
{"id": "a8fa7aa1-2198-419b-a0de-6ba4d1c1f160", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Dance Any Beat: Blending Beats with Visuals in Dance Video Generation\" by Xuanchen Wang, Heng Wang, Weidong Cai, and Dongnan Liu],\n[\"InMoDeGAN: Interpretable Motion Decomposing Generative Adversarial Network for Video Generation\" by Yaohui Wang, Francois Bremond,"], "TTFT": 14.07155442237854, "answer": "[\"InMoDeGAN: Interpretable Motion Decomposition Generative Adversarial Network for Video Generation\", \"LATENT IMAGE ANIMATOR: <br> LEARNING TO ANIMATE IMAGES VIA LATENT SPACE NAVIGATION\", \"Dance Any Beat: Blending Beats with Visuals in Dance Video Generation\"]", "f1 socre": 0.43548387096774194}
{"id": "49680a79-a974-49b0-b7e3-85ff515bc4f2", "pred": ["{{\"Reference\": [\"Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding\"], \"Citation\": [\"Smurfs: Leveraging Multiple Proficiency Agents for Effective Tool Utilization in LLMs\" ]}}\n\nThe paper \"EasyTool: Enhancing LLM-based Agents with Concise Tool Instruction\" references the paper \"Don't Fine-Tune, Dec"], "TTFT": 10.98206639289856, "answer": "{\"Reference\": [\"Don't Fine-Tune, Decode: Syntax Error-Free Tool Use via Constrained Decoding\"], \"Citation\": [\"Smurfs: Leveraging Multiple Proficiency Agents with Context-Efficiency for Tool Planning\"]}", "f1 socre": 0.6521739130434783}
{"id": "9f29f552-c817-4c2a-8910-b5e8ae7c014f", "pred": ["[\"Fine-Tuning LLaMA for Multi-Stage Text Retrieval\" by Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin, 2023, cited by \"Improving Text Embeddings with Large Language Models\" by Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, Furu Wei, and Jimmy Lin,"], "TTFT": 13.682081937789917, "answer": "[\"RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large Language Models\", \"Fine-Tuning LLaMA for Multi-Stage Text Retrieval\", \"Improving Text Embeddings with Large Language Models\"]", "f1 socre": 0.3225806451612903}
{"id": "30a2535f-7e73-46ad-a402-ee80ac6e4f92", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"Quantum Annealers Chain Strengths: A Simple Heuristic to Set Them All\" by Valentin Gilbert and St\u00e9phane Louise, 2024, arXiv: 2404.05443 [39]],\n[\"Benchmarking Quantum Annealers with Near-Optimal Minor Embeddings\" by Valentin Gilbert"], "TTFT": 16.3840811252594, "answer": "[\"Scaling Advantage in Approximate Optimization with Quantum Annealing\", \"4-clique Network Minor Embedding for Quantum Annealers\", \"Quantum Annealers Chain Strengths: A Simple Heuristic to Set Them All\", \"Benchmarking Quantum Annealers with Near-Optimal Minor-Embedded Instances\"]", "f1 socre": 0.40579710144927533}
{"id": "68e88dcc-6dec-4bb4-b51b-1fe8d6cd856d", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"LGDE: Local Graph-based Dictionary Expansion\" by Schindler et al., 2023]\n[\"Query2doc: Query Expansion with Large Language Models\" by Wang, Yang, and Wei, 2023]\n[\"From free text to clusters: A graph-based approach to text mining and information retrieval\" by Raghav"], "TTFT": 10.241957187652588, "answer": "[\"Query2doc: Query Expansion with Large Language Models\", \"Query Expansion by Prompting Large Language Models\", \"LGDE: Local Graph-based Dictionary Expansion\"]", "f1 socre": 0.32989690721649484}
{"id": "d7adf528-91a9-4978-a25c-29e61562a3be", "pred": ["{{\"Reference\": [\"The Pitfalls of Defining Hallucination\", \"Hallucination is Inevitable: An Innate Limitation of Large Language Models\"],\n\"Citation\": [\"Hallucination is Inevitable: An Innate Limitation of Large Language Models\", \"The Pitfalls of Defining Hallucination\"]}}\n\nIn this paper, the authors show that hallucination is inevitable for large language models (LLMs"], "TTFT": 11.103811502456665, "answer": "{\"Reference\": [\"The Pitfalls of Defining Hallucination\"], \"Citation\": [\"An ASSESSMENT OF MODEL-ON-MODEL DECEPTION\"]}", "f1 socre": 0.29333333333333333}
{"id": "949f3450-9b30-4556-be5f-ca8afe9e072f", "pred": ["{\n\"Title\": \"LLamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource Language\",\n\"Authors\": [\"Cagri Toraman\"],\n\"Reference\": [\n{\"Title\": \"BELLE: A Large-Scale Chinese Instruction Tuning Dataset for Pretrained Language Models\"},\n{\"Title\": \"Bactrain-X: A Multilingual Instruction Tuning Dataset for Low-Resource L"], "TTFT": 12.38475251197815, "answer": "{\"Reference\": [], \"Citation\": [\"LlamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource Language\", \"InstructionCP: A fast approach to transfer Large Language Models into target language\"]}", "f1 socre": 0.43010752688172044}
{"id": "8878fb61-6996-4984-9774-5ce9fb506d37", "pred": ["{\n\"Title\": \"Extreme Compression of Large Language Models via Additive Quantization\",\n\"Authors\": [\"Vage Egiazarian\", \"* 12\", \"Andrei Panferov\", \"* 12\", \"Denis Kuznedelev\", \"* 23\", \"Elias Frantar\", \"* 4\", \"Artem Babenko\", \"* 2\"],\n\"Reference\": [\n{\"Title\": \"Mistral"], "TTFT": 11.69446086883545, "answer": "{\"Reference\": [\"Mistral 7B\"], \"Citation\": [\"Sparse Expansion and Neuronal Disentanglement\"]}", "f1 socre": 0.10526315789473684}
{"id": "17539923-dd06-4712-becd-68fb8d68614d", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"Context-Aware Learning to Rank with Self-Attention\" by Przemys\u0142aw Pobrotyn, Tomasz Bartczak, Miko\u0142aj Synowiec, Rados\u0142aw Bia\u0142obrzeski, and Jaros\u0142aw Bojar]\n\ncites\n\n[\"Learning a ranking model with delta features to study the effect of neighborhood on item preference in e"], "TTFT": 6.013138055801392, "answer": "[\"Influence of Neighborhood on the Preference of an Item in eCommerce Search\", \"Context-Aware Learning to Rank with Self-Attention\"]", "f1 socre": 0.31249999999999994}
{"id": "ad6d0a53-1f8a-46b5-9738-a34d0a3cafaf", "pred": ["Based on the provided instructions and the given papers, the longest citation chain is as follows:\n\n[\"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis\" by Mena, C. Gary et al. (2019), \"Churn prediction using deep convolutional neural networks and autoencoders\" by Wangperawong, A. et al. (2016)]\n\nExplanation:\n\nThe"], "TTFT": 3.672146797180176, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\"]", "f1 socre": 0.4000000000000001}
{"id": "b5b963fe-30ab-435f-9574-dcdfd96f3707", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Minimod: A Finite Difference solver for Seismic Modeling\" by Jie Meng, Andreas Atle, and Henri Calandra, 2020]\n[\"Massively scalable stencil algorithm for seismic modeling on the Cerebras WSE-2\" by Mathias Jacquelin, Mauricio Araya-Polo,"], "TTFT": 8.336519241333008, "answer": "[\"Minimod: A Finite Difference solver for Seismic Modeling\", \"Massively scalable stencil algorithm\"]", "f1 socre": 0.3368421052631579}
{"id": "7b104919-2fba-44d2-91dd-a56bd2f30e91", "pred": ["{{\"Reference\": [\"LLM A*: Human in the Loop Large Language Models Enabled A* Search for Robotics\"], \"Citation\": [\"FunSearch\", \"Various papers on search-based MAPF methods (e.g., CBS, ECBS, EECBS)\"]}}\n\nThe paper \"Evolution of Heuristics: Towards Efficient Automatic Algorithm Design Using Large Language Model\" references the paper \"LLM A*: Human in"], "TTFT": 12.563227891921997, "answer": "{\"Reference\": [\"LLM A*: Human in the Loop Large Language Models Enabled A* Search for Robotics\"], \"Citation\": [\"Ensembling Prioritized Hybrid Policies for Multi-agent Pathfinding\"]}", "f1 socre": 0.5}
{"id": "31ee8539-2e9e-40e2-adc3-082c872698f7", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\" by Niimi et al., 2024]\n\ncites\n\n[\"A PRE"], "TTFT": 11.793168783187866, "answer": "[\"Churn analysis using deep convolutional neural networks and autoencoders\", \"Churn Prediction with Sequential Data and Deep Neural Networks A Comparative Analysis*\", \"MUlTimodAL DeEP LEARNING OF Word-OF-MOUTH TEXT AND DEMOGRAPHICS TO PREDICT CUSTOMER RATING: HANDLING CONSUMER HETEROGENEITY IN MARKETING\", \"An Efficient Multimodal Learning Framework to Comprehend Consumer Preferences Using BERT and Cross-Attention\"]", "f1 socre": 0.39682539682539686}
{"id": "9b982095-daad-4632-9573-b0d7037dd6a7", "pred": ["{{\"Reference\": [\"A Prompt for Investigating the Effectiveness of LLMs in Self-Critiquing for Planning: An Empirical Study\", \"Large Language Models Cannot Self-Correct Reasoning Intrinsically\"],\n\"Citation\": [\"LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks\"]}}"], "TTFT": 10.443064212799072, "answer": "{\"Reference\": [\"LARGE LANGUAGE MoDELS CANNOT SELF-CorRECT REASONING YET\", \"Can Large Language Models Really Improve by Self-critiquing Their Own Plans?\"], \"Citation\": []}", "f1 socre": 0.3943661971830986}
{"id": "ecded47a-3741-4458-ba61-c04271c33651", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"MonaCoBERT: Monotonic attention based ConvBERT for Knowledge Tracing\" by Unggi Lee, Yonghyun Park, Yujin Kim, Seongyune Choi, and Hyeoncheol Kim, 2021]\n\nThis paper cites:\n\n[\"Understanding the Instability of Transformer-based Mod"], "TTFT": 10.399053812026978, "answer": "[\"Very Deep Transformers for Neural Machine Translation\", \"Understanding the Difficulty of Training Transformers\", \"MonaCoBERT: Monotonic attention based ConvBERT for Knowledge Tracing\"]", "f1 socre": 0.29411764705882354}
{"id": "71d0fb39-d315-4e33-beec-c53271663f7f", "pred": ["{{\"Reference\": [\"LoRA-Switch: Boosting the Efficiency of Dynamic LLM Adapters via System-Algorithm Co-design\"], \"Citation\": []}}"], "TTFT": 13.256085872650146, "answer": "{\"Reference\": [\"ONLINE SPECULATIVE DECODING\"], \"Citation\": [\"LoRA-Switch: Boosting the Efficiency of Dynamic LLM Adapters via System-Algorithm Co-design\"]}", "f1 socre": 0.9019607843137255}
{"id": "5d482043-88d2-4246-8191-e603b7bf6ed4", "pred": ["{\n\"Title\": \"Scalable Pre-training of Large Autoregressive Image Models\",\n\"Authors\": [\"Alaaeldin El-Nouby\", \"Ali Farhat\", \"Mohamed Ragab\", \"Mostafa Dehghani\", \"Georg Heigold\", \"Chen Sun\", \"Mario Lu\u010di\u0107\", \"Cordelia Schmid\"],\n\"Reference\": [\n{\"Title\": \"Vivit"], "TTFT": 15.165497541427612, "answer": "{\"Reference\": [\"Data Filtering Networks\"], \"Citation\": [\"ARVideo: Autoregressive Pretraining for Self-Supervised Video Representation Learning\"]}", "f1 socre": 0.17073170731707318}
{"id": "014ae33b-22e7-4fb7-ae34-56f64160ce74", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n1. [\"DYNAP-SE2: a scalable multi-core dynamic neuromorphic asynchronous spiking neural network processor\" by Ole Richter, et al.]\n2. [\"Speck: A Smart event-based Vision Sensor with a low latency 327K Neuron Convolutional Neural Network Processing Pipeline\" by Ole Richter,"], "TTFT": 15.628410577774048, "answer": "[\"Speck: A Smart event-based Vision Sensor with a low latency 327K Neuron Convolutional Neuronal Network Processing Pipeline\", \"DYNAP-SE2: a scalable multi-core dynamic neuromorphic asynchronous spiking neural network processor\"]", "f1 socre": 0.7543859649122807}
{"id": "3e4ee654-dfd8-4ffd-a825-79e0997961c9", "pred": ["{{\"Reference\": [\"Navigating the Safety Landscape: Measuring LLM Cybersecurity Risks with VISAGE Metric\", \"Purple Llama CYBERSECEval: A Secure Coding Benchmark for Large Language Models\"],\n\"Citation\": [\"Navigating the Safety Landscape: Measuring LLM Cybersecurity Risks with VISAGE Metric\": \"studied the safety landscape of LLMs and discovered"], "TTFT": 12.062817811965942, "answer": "{\"Reference\": [\"Purple Llama CYBERSECEvAL: A Secure Coding Benchmark for Language Models\"], \"Citation\": [\"Navigating the Safety Landscape: Measuring Risks in Finetuning Large Language Models\"]}", "f1 socre": 0.5238095238095238}
{"id": "f359acf5-39a1-4490-ba4a-95d38de35fa7", "pred": ["{\n\"Title\": \"Soaring from 4K to 400K: Extending LLM's Context with Activation Beacon\",\n\"Authors\": [\"Peitian Zhang\", \"Zheng Liu\", \"Shitao Xiao\", \"Ninglu Shao\", \"Hongjin Qian\", \"Qiwei Ye\", \"Zhicheng Dou\"],\n\"Affiliations\": [\"Beijing Academy of Artificial Intelligence\","], "TTFT": 10.695560455322266, "answer": "{\"Reference\": [\"PoSE: EFFICIENT CONTEXT WINDOW EXTENSION OF LLMS VIA POSITIONAL SKIP-WISE TRAINING\"], \"Citation\": [\"Extending Llama-3's Context Ten-Fold Overnight\"]}", "f1 socre": 0.10752688172043011}
{"id": "2357f6df-fed9-4508-a80c-800fe6995caa", "pred": ["Based on the provided papers, the longest citation chain is as follows:\n\n[\"GTBENCH: Uncovering Game-Theoretic Characteristics of LLMs\" by Jinhao Duan, Renming Zhang, James Diffenderfer, Bhavya Kailkhura, Lichao Sun, Elias Stengel-Eskin, Mohit Bansal, Tianlong Chen, and Kaidi Xu]\n\n[\"Exploring"], "TTFT": 10.72509765625, "answer": "[\"![](https://cdn.mathpix.com/cropped/2024_06_04_791d7ec1c70f2f7bcd44g-01.jpg?height=135&width=174&top_left_y=320&top_left_x=260)\", \"Generative AI for Game Theory-based Mobile Networking\", \"Exploring Equilibrium Strategies in Network Games with Generative AI\"]", "f1 socre": 0.025477707006369425}
{"id": "aa9707c4-9506-4e51-899d-9213ad10a324", "pred": ["Based on the provided instructions, the longest citation chain from the given papers is as follows:\n\n[\"PromptMind Team at EHRSQL-2024: Improving Reliability of SQL Generation using Ensemble LLMs\", \"Angel-Optimized Text Embeddings\", \"Dolan and Brockett (2005): Building a question answering test collection\"]\n\nExplanation:\n\n1. The first paper, \"Prompt"], "TTFT": 11.649495124816895, "answer": "[\"Scaling Sentence Embeddings with Large Language Models\", \"ANGLE-OPTIMIZED TEXT EMBEDDINGS\", \"PromptMind Team at EHRSQL-2024: Improving Reliability of SQL Generation using Ensemble LLMs\"]", "f1 socre": 0.5617977528089888}
{"id": "edc6d95f-02ba-4835-bff6-853cb7d61f6b", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"The unreasonable effectiveness of optimal transport in economics\" by Galichon, 2021, \"Optimal transport in a nutshell\" by Galichon, 2021, \"Exploring the connection between optimal transport and economic applications\" by Galichon, 2016, \"Optimal Transport for Applied Mathematicians\" by Sant"], "TTFT": 13.675036668777466, "answer": "[\"Water is a radiation protection agent for ionised pyrrole\", \"The unreasonable effectiveness of optimal transport in economics\"]", "f1 socre": 0.23157894736842105}
{"id": "a780dc25-81d8-4e0f-b344-5086b8055e04", "pred": ["Based on the provided instructions, the longest citation chain can be constructed as follows:\n\n[\"Beyond Traditional Magnetic Resonance Processing with Artificial Intelligence\" by Amir Jahangiri and Vladislav Orekhov, 2024],\n[\"Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems\" by Tsung-Hsien Wen, Milica Ga\u0161i\u0107, Nikola Mrk\u0161"], "TTFT": 11.49069619178772, "answer": "[\"Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems\", \"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\", \"Deep Learning using Rectified Linear Units (ReLU)\", \"BEYOND TRADITIONAL MAGNETIC RESONANCE PROCESSING WITH ARTIFICIAL INTELLIGENCE *\"]", "f1 socre": 0.35294117647058826}
{"id": "1df3da1b-87c8-4692-9ec7-83d1b0e9caa3", "pred": ["{\n\"Title\": \"Break the Sequential Dependency of LLM Inference Using LOOKAHEAD DECODING\",\n\"Authors\": [\"Yichao Fu\", \"Peter Bailis\", \"Ion Stoica\", \"Hao Zhang\"],\n\"Reference\": [\n{\"Title\": \"Attention is all you need\"},\n{\"Title\": \"Gqa: Training generalized multi-query transformer models from multi-head checkpoints\"},\n"], "TTFT": 9.91625428199768, "answer": "{\"Reference\": [\"Mistral 7B\"], \"Citation\": [\"Kangaroo: Lossless Self-Speculative Decoding via Double Early Exiting\"]}", "f1 socre": 0.04938271604938271}
{"id": "f7116f92-a07c-4278-9333-b63ed5965907", "pred": ["Based on the provided instructions and papers, the longest citation chain is as follows:\n\n[\"Beyond Traditional Magnetic Resonance Processing with Artificial Intelligence\" by Amir Jahangiri and Vladislav Orekhov, 2024],\n[\"Deep Learning using Rectified Linear Units (ReLU)\" by Abien Fred M. Agarap, 2018],\n[\"A Neural Network Architecture Combining"], "TTFT": 11.509602546691895, "answer": "[\"Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems\", \"A Neural Network Architecture Combining Gated Recurrent Unit (GRU) and Support Vector Machine (SVM) for Intrusion Detection in Network Traffic Data\", \"Deep Learning using Rectified Linear Units (ReLU)\", \"BEYOND TRADITIONAL MAGNETIC RESONANCE PROCESSING WITH ARTIFICIAL INTELLIGENCE *\"]", "f1 socre": 0.3090909090909091}
{"id": "c8d80aee-3b85-434f-a348-2b40bb7d12dd", "pred": ["Based on the provided instructions and papers, here is the longest citation chain:\n\n[\"Context-Aware Learning to Rank with Self-Attention\" by Przemys\u0142aw Pobrotyn, Tomasz Bartczak, Miko\u0142aj Synowiec, Rados\u0142aw Bia\u0142obrzeski, and Jaros\u0142aw Bojar],\n[\"Learning to Rank with Self-Attention: A Survey\" by Himan Abdollahpouri, Robin"], "TTFT": 16.089614152908325, "answer": "[\"Context-Aware Learning to Rank with Self-Attention\", \"On the Impact of Outlier Bias on User Clicks\", \"Treatment Effect Estimation for User Interest Exploration on Recommender Systems\"]", "f1 socre": 0.1651376146788991}
{"id": "651ab044-51e3-44e4-a3f2-0e672bbecba8", "pred": ["{{\"Reference\": [\"Evaluating Correctness and Usefulness of Instruction-Following Language Models\", \"Large Language Models as Agents: Applications and Challenges\"],\n \"Citation\": [\"GPT-4 as an Agronomist Assistant? Answering Agriculture Exams Using Large Language Models\"]}}\n\nThis paper compares the use of Retrieval-Augmented Generation (RAG) and fine-tuning for generating high"], "TTFT": 16.650513172149658, "answer": "{\"Reference\": [\"GPT-4 AS AN AGRONOMIST ASSISTANT? ANSWERING AGRICULTURE QUESTIONS USING LARGE LANGUAGE MODELS\"], \"Citation\": [\"Large Language Models (LLMs): Deployment, Tokenomics and Sustainability\"]}", "f1 socre": 0.5116279069767441}
